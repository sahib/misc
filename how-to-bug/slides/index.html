<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml"><head><title>How to bug</title><meta charset="UTF-8"></meta><meta name="generator" content="Hovercraft! 1.0 http://regebro.github.com/hovercraft"></meta><link rel="stylesheet" href="css/hovercraft.css" media="all"></link><link rel="stylesheet" href="css/highlight.css" media="all"></link><link rel="stylesheet" href="hovercraft.css" media="screen,projection"></link><script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        showProcessingMessages: false,
        messageStyle: "none",
        TeX : { extensions : ['color.js'] }
      });
    </script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script></head><body class="impress-not-supported"><div id="impress-help"></div><div id="impress" data-transition-duration="1500"><div class="step step-level-1" step="0" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="0" data-y="0" data-z="0"><p>How (not) to Bug: Ein Kurs f&#xFC;r Angeschrittene und Fortg&#xE4;nger</p><h1 id="expectation-management">Expectation management</h1><pre class="highlight code">* This is more of a table of contents, than in-depth knowledge.
* Therefore touching a lot of topics. Mostly Go, but also many general techniques.
* TODO: L&#xFC;ckenf&#xFC;ller zwischen How-to-TDD und How-to-unittests.
* You accept software is never perfect and never will be.
* You accept you have to accept and manage mistakes.
* You don't except this workshop to be complete in any way.
* You don't expect that we implement all of the things mentioned.
* You know not to belive everything the internet or Lemurs says.</pre><div class="notes"><p>My expectations:</p><ul><li>You ask immediately when you did not understand something.</li><li>You will have some exercises in between.</li><li>You will not need to understand everything.
In the worst case you get better in buzzword bingo.</li></ul></div></div><div class="step step-level-1" step="1" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="1600" data-y="0" data-z="0"><h1 id="first-step-accept-defeat">First step: Accept defeat</h1><ul><li class="substep">How good you think are we in testing our software?</li><li class="substep">How the f**k do we measure that?</li><li class="substep"><span class="math ">\(Q_{pm} = \frac{loc(test\_code)}{loc(app\_code)}\)</span></li><li class="substep">Backend: <span class="math ">\(0.27\)</span></li><li class="substep">Firmware: <span class="math ">\(0.21\)</span> (feels bad man)</li><li class="substep">SQLite: <span class="math ">\(640\)</span> (what the fuck.)</li><li class="substep">Are we particularly bad? Not really.</li><li class="substep">Is SQLite way better at this? Oh yes.</li><li class="substep">Bonus: InfluxDB: <span class="math ">\(0.3\)</span></li></ul><div class="notes"><p>If we approach the topic humble, chances are better we learn something.</p><p>Coverage is also a widely known way to measure how tested code is.
We come to that in a minute.</p><p><a href="https://www.sqlite.org/testing.html">https://www.sqlite.org/testing.html</a></p><p>if you wonder why the firmware has not so much: ui, most other services are small-ish)</p></div></div><div class="step step-level-1" step="2" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="3200" data-y="0" data-z="0"><h1 id="meet-your-doom-sqlite">Meet your doom: SQLite</h1><p>Money quotes:</p><ul><li><tt>veryquick</tt> consists of 285k distinct tests.</li><li>Full run prior to releases consists of several billion (!) test runs.</li></ul><p>Different test types:</p><ul><li>Four independently developed test harnesses</li><li>100% branch test coverage in an as-deployed configuration</li><li>Millions and millions of test cases</li><li>Out-of-memory tests</li><li>I/O error tests</li><li>Crash and power loss tests</li><li>Fuzz tests</li><li>Boundary value tests</li><li>Disabled optimization tests</li><li>Regression tests</li><li>Malformed database tests</li><li>Extensive use of assert() and run-time checks</li><li>Valgrind analysis</li><li>Undefined behavior checks</li><li>Checklists</li></ul></div><div class="step step-level-1" step="3" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="4800" data-y="0" data-z="0"><h1 id="how-much-quality-do-we-need">How much quality do we need?</h1><p>TODO:</p><ul><li>Bastelbude</li><li>Most-deployed Database in the world.</li></ul><p>More tests &lt;-&gt; Slower development
More money &lt;-&gt; more tests
Simpler software (SQLite is simple compared to Postgres) &lt;-&gt; Better ratio (we are complex compared to Ottobock)</p><p>Actual question is: How much quality can be buy cheap?</p></div><div class="step step-level-1" step="4" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="6400" data-y="0" data-z="0"><h1 id="the-lifecycle-of-mistakes">The lifecycle of mistakes</h1><ol><li>Testing: Just don't commit bugs</li><li>Prevention: Defensive Programming</li><li>Debugging: Fuck, I committed bugs.</li><li>Profiling: Oh wow, now it's slow.</li></ol><div class="notes"><p>Table of contents</p></div></div><div class="step step-level-1" step="5" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="8000" data-y="0" data-z="0"><h1 id="testing-types">1. Testing: Types</h1><ul><li>Unittests</li><li>Integration tests</li><li>End-to-End tests</li><li>Smoke tests</li><li>Benchmarks/Load testing</li><li>Pen testing</li></ul><p>TODO: Diagram with effort vs coverage</p><p>TODO: A good mix.</p><div class="notes"><p>That's not a strict law, sometimes unit and integration test
flow into each other.</p><p>Also, the list is not complete.</p></div></div><div class="step step-level-1" step="6" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="9600" data-y="0" data-z="0"><h1 id="testing-rules">1. Testing: Rules</h1><ul><li>Should be automated (if possible at all -&gt; Matlab, UI, hardware)</li><li>Happy path is not enough, but the most important one.</li><li>Unit tests should have no dependencies</li><li>Don't test things that are not in your software (json.Marshal)</li><li>Tests should be stateless and may run in parallel (<tt>stretchr/testify</tt> sucks)</li></ul><p>TODO: More?</p><h1 id="testing-unit">1. Testing: Unit</h1><p>Example with go test
Everyone saw a test already, so let's focus on how it's done in Go</p><p>t.FailNow()
t.Run()
t.Parallel()</p><p>stretchr/require</p><h1 id="testing-terms">1. Testing: Terms</h1><p>Black/White/Grey box</p><p>Blackbox vs whitebox in Go -&gt; different packages.</p><p>Mocks, fakes, dummies</p><h1 id="testing-table-driven-tests">1. Testing: Table driven tests</h1><p>Parametrized tests in other languages / frameworks.
Table because in Go you implemented them by writing a table.</p><h1 id="testing-types-of-coverage">1. Testing: Types of Coverage</h1><p>You see often badges like "100% test coverage" in the internet.
Sounds great, does it? -&gt; Cargo Cult (Begriff erkl&#xE4;ren)</p><p>But what the heck does that even mean?</p><p>go test -cover -&gt; statement coverage</p></div><div class="step step-level-1" step="7" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="11200" data-y="0" data-z="0"><h1 id="testing-statement-coverage">1. Testing: Statement coverage</h1><p>-&gt; Many open source projects claim 100% coverage.
-&gt; That's what they mean.
-&gt; Please don't do this.</p><pre class="highlight code bash">func f<span class="o">(</span>max int<span class="o">)</span> int <span class="o">{</span>
    result :<span class="o">=</span> <span class="m">1</span>
    <span class="k">for</span> idx :<span class="o">=</span> <span class="m">0</span><span class="p">;</span> idx &lt; max<span class="p">;</span> idx++ <span class="o">{</span>
        <span class="k">if</span> result &lt; <span class="m">1000</span> <span class="o">{</span>
            result *<span class="o">=</span> idx
        <span class="o">}</span>

        <span class="nv">result</span> <span class="o">+=</span> idx
    <span class="o">}</span>

    <span class="k">return</span> result
<span class="o">}</span></pre></div><div class="step step-level-1" step="8" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="12800" data-y="0" data-z="0"><h1 id="testing-branch-coverage">1. Testing: Branch coverage</h1><p>-&gt; SQLite has fucking 100%</p><pre class="highlight code bash">TODO</pre></div><div class="step step-level-1" step="9" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="14400" data-y="0" data-z="0"><h1 id="testing-condition-coverage">1. Testing: Condition coverage</h1><pre class="highlight code bash">TODO</pre></div><div class="step step-level-1" step="10" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="16000" data-y="0" data-z="0"><h1 id="testing-fuzzing">1. Testing: Fuzzing</h1><pre class="highlight code bash">TODO: Use Go <span class="m">1</span>.18 fuzzing</pre></div><div class="step step-level-1" step="11" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="17600" data-y="0" data-z="0"><h1 id="prevention">2. Prevention</h1><p>Statistics: Average number of bugs per line.
Still a fact: With enough lines of code, there will be bugs, no matter
the experience level.</p><h1 id="prevention-out-of-scope">2. Prevention: Out of scope</h1><ul><li>Software design choices to lower number of bugs (good design result in lower )</li><li>Software processes that improve communication and therefore lower mistakes.
Communication: Many bugs happen when two software systems talk to each other.
But not the right language.</li></ul><h1 id="prevention-the-language">2. Prevention: The language</h1><p>choose a language with strict type system:</p><ul><li>Ada</li><li>Rust</li><li>Go, Haskell</li><li>Elm</li><li>Typescript</li></ul><p>Nopes:</p><ul><li>C</li><li>Python</li><li>PHP</li><li>Bash (well, for bigger software)</li></ul><h1 id="prevention-use-the-tools-luke">2. Prevention: Use the tools, Luke!</h1><p>static analyzers</p><h1 id="prevention-complexity">2. Prevention: Complexity</h1><p>special case: software complexity can be measured and acted up on (McCabe, cyclomatic complexity)</p><p>feature creep (case of log4j, Software complexity must be measured as the sum of all dependencies)</p><h1 id="prevention-regressions">2. Prevention: Regressions</h1><p>Bug fixes should be considered
(do we do this? often, but not always)</p><h1 id="prevention-documentation">2. Prevention: Documentation</h1><p>Literate programming (jupyter kinda does this)
(bit too much for us)</p><p>Write go examples</p><p>Write good docstrings where necessary,
don't just write doc strings to make the linter shut up</p><p>Documentation should stay close
(that's also why I don't like Confluence for code docs.
Docs won't change when it's not close to the code)</p><h1 id="prevention-ci-cd-pipelines">2. Prevention: CI/CD Pipelines</h1><p>CI/CD</p><h1 id="prevention-code-reviews">2. Prevention: Code Reviews</h1><p>Good commit messages
Assign only when really ready.</p><h1 id="prevention-introspection">2. Prevention: Introspection</h1><ul><li>Design your software inspectable. Built command line tools that help you check what's going on</li></ul><h1 id="debugging">3. Debugging</h1><p>&#x201C;If debugging is the process of removing software bugs, then programming must be the process of putting them in</p><ul><li>Nope: Software is complex and sometimes things break because of environment (disk full, not enough mem, other services have bugs and cascade)</li><li>...or just maybe you didn't test for the right thing: Most of the times the requirements were correctly implemented.
Well, the requirements were maybe wrong.</li><li>Also, software engineering is a team sport. Most bugs happen in communication.</li><li>Use proper logs (for fuck's sake)</li><li>Kill a Go process with SIGABRT to get its stack trace (pkill -ABRT "name")</li><li>Debuggers are nice, but if you need one you should re-consider your life decisions.
and easily live-debug the faulty behavior. Don't rely on individual knowledge, code it as script.</li><li>Don't make complex software:<blockquote><p>Debugging is twice as hard as writing the code in the first place. Therefore, if you write the code as cleverly as possible, you are, by definition, not smart enough to debug it.</p><p>(Brian Kernighan)</p></blockquote></li></ul><h1 id="profiling">4. Profiling</h1><p>We came from "It doesn't matter how fast you return  "</p><p>But it does matter how fast you return a correct result.</p><p>Everything correct, but the result never arrive? Ok, great.</p><h1 id="takeaways">5. Takeaways</h1><p>-&gt; Implement regression testing.
-&gt; Better reviews</p></div></div><script type="text/javascript" src="js/impress.js"></script><script type="text/javascript" src="js/gotoSlide.js"></script><script type="text/javascript" src="js/hovercraft.js"></script></body></html>