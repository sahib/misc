<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml"><head><title>Performance Performance: I/O</title><meta charset="UTF-8"></meta><meta name="generator" content="Hovercraft! 1.0 http://regebro.github.com/hovercraft"></meta><link rel="stylesheet" href="css/hovercraft.css" media="all"></link><link rel="stylesheet" href="css/highlight.css" media="all"></link><link rel="stylesheet" href="hovercraft.css" media="screen,projection"></link><link rel="stylesheet" href="hovercraft.css" media="screen,projection"></link><script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        showProcessingMessages: false,
        messageStyle: "none",
        TeX : { extensions : ['color.js'] }
      });
    </script></head><body class="impress-not-supported"><div id="impress-help"></div><div id="impress" data-transition-duration="1500 1500"><div class="step step-level-1" step="0" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="0" data-y="0" data-z="0"><h1 id="what-this-workshop-is">What this workshop is</h1><ul><li>An intro into Performance thinking</li><li>An attempt at understanding the machine we program on</li><li>Some lesser know tricks</li><li>A workshop. You gonna have to work for it.</li></ul></div><div class="step step-level-1" step="1" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="1600" data-y="0" data-z="0"><h1 id="what-this-workshop-is-not">What this workshop is not</h1><ul><li>An exhausting list of tricks</li><li>A lecture on algorithm and data structures - pick a book.</li><li>Something you just listen and it clicks</li><li>Application specific performance tips (Networking, SQL, ...)</li></ul></div><div class="step step-level-1" step="2" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="3200" data-y="0" data-z="0"><h1 id="you-need-to-experiment-yourself">You need to experiment yourself</h1><p>We will write our own little databases in this workshop.</p><ul><li>You can group up or do it on our own if you must.</li><li>You can also use your favourite programming language.</li><li>You can always ask me outside the workshop when writing it.</li></ul><p>TODO: Move the explanation of LSM dbs to intro and re-order io and cpu?</p></div><div class="step step-level-1" step="3" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="4800" data-y="0" data-z="0"><h1 id="what-is-performance-optimization">What is performance optimization?</h1><p>TODO: Wikipedia.</p><p>Optimizing different metrics that</p></div><div class="step step-level-1" step="4" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="6400" data-y="0" data-z="0"><h1 id="when-to-apply-it">When to apply it?</h1><ul><li>Probably more often than you do now.</li><li>Whenever your performance requirements are not fulfilled.</li></ul><p>Wait, there are requirements?</p></div><div class="step step-level-1" step="5" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="8000" data-y="0" data-z="0"><h1 id="how-to-figure-out-performance-criteria">How to figure out performance criteria?</h1><p>Well, that's your job mostly. Your PM won't tell you most of the time.
But they will help you to get the requirements.</p><p>Ask those questions:</p><ul><li>On what kind of system the software will run on?</li><li>How many users will there be in parallel?</li><li>What kind of latency is the user willing to accept? (games, websites, ATMs)</li><li>How much scaling is expected in the next time?</li><li>Will my technology choice be a bottleneck? (Electron)</li><li>Do edge cases need to perform well?</li><li>Are the optimizations worth the risk/effort?</li><li>...</li></ul><p>Do some basic calculations based on these and add X to your goals.</p><div class="notes"><p>Do not ask: How fast could this be?
(that's a fine question for personal learning though,
but not when you get paid for delivering value to a company ;-))</p><p>After this workshop you should be able to onvert the answers to those questions
to measurable numbers.</p><p>Edgecases are a good point: Sometimes performance is only bad in certain cases.
Ask your PM if those are important for your business.
If it's a open source library, probably fix those edge cases too.</p></div></div><div class="step step-level-1" step="6" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="9600" data-y="0" data-z="0"><h1 id="when-not-to-apply-it">When not to apply it?</h1><div class="line-block">"Programmers waste enormous amounts of time thinking about, or worrying<br></br>about, the speed of noncritical parts of their programs, and these attempts at<br></br>efficiency actually have a strong negative impact when debugging and<br></br>maintenance are considered. We should forget about small efficiencies, say<br></br>about 97% of the time: premature optimization is the root of all evil. Yet we<br></br>should not pass up our opportunities in that critical 3%."<br></br></div><p>-- Donald Knuth</p><div class="notes"><p>If you don't have a problem you really should not do anything.
It is difficult to define what a "problem" is.
Electron apparently defined that it's not a problem if low-memory devices
can't use their framework.</p></div></div><div class="step step-level-1" step="7" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="11200" data-y="0" data-z="0"><h1 id="how-do-i-know-if-it-s-premature">How do I know if it's premature?</h1><img src="images/premature_optimization_flowchart.png"></img><p>Remember: It does not matter you fast you compute a wrong result.</p><div class="notes"><p>The main point is: Take your time to do things the right away. Don't drop the pen
when it worked for the first time and didn't feel slow, really take some to measure.</p><p>However, don't just blindly optimize things before you measured or optimize the small
things after measuring.</p><p>Optimizations come at a price. It's usually more and harder code to maintain (and if not,
why didn't you do it in the first place?) or they have some other disadavntages (an index
in a database for example slows drown writes and needs space!). Is it worth the risk?</p></div></div><div class="step step-level-1" step="8" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="12800" data-y="0" data-z="0"><h1 id="how-do-i-measure">How do I measure?</h1><p>Via automated benchmarks.</p><div class="notes"><p>The how will be shown</p></div></div><div class="step step-level-1" step="9" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="14400" data-y="0" data-z="0"><h1 id="how-do-i-know-how-to-optimize">How do I know how to optimize?</h1><p>No short answer and no shortcuts to this.
It will be a long journey and this is workshop will be only a step on the journey.</p><p>Very many different languages, OS (Python, Go) and many different applications
(SQL - 90%: just add an index) that cannot all be covered.</p><p>But there are some common basics and more importantly a commone thinking behind all of it.
And that is: <strong>You have to understand what your program is doing to optimize it.</strong></p></div><div class="step step-level-1" step="10" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="16000" data-y="0" data-z="0"><h1 id="in-a-nutshell-go-from-big-to-small">In a nutshell: Go from big to small</h1><p>Algorithm for optimizing a <strong>correct</strong> program:</p><ol><li>Do the obvious things right away. ("obvious" depends a lot on experience)</li><li>Check if your requirements are met. If you don't have concrete performance requirements, make some.</li><li>Benchmark to find the biggest bottlenecks regarding performance (we are incredible bad at guessing! Never skip this step)</li><li>Optimize biggest offender found and repeat from step 1.</li></ol><p>Never mix up this order.</p></div><div class="step step-level-1" step="11" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="17600" data-y="0" data-z="0"><h1 id="what-is-this-program-doing">What is this program doing?</h1><pre class="highlight code python"><span class="kn">import</span> <span class="nn">sys</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">stdin</span><span class="o">.</span><span class="n">readline</span><span class="p">()</span><span class="o">.</span><span class="n">strip</span><span class="p">())</span></pre><div class="notes"><p>Interpreted -&gt; compiled to byte code.
sys.stdin.readline are two dict lookups.
memory allocations
file I/O from stdin to stdout
calling a c function (strip)
unicode conversion!</p></div></div><div class="step step-level-1" step="12" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="19200" data-y="0" data-z="0"><h1 id="inside-python">Inside Python</h1><p>All functions eventuall call functions implemented in C:</p><pre class="highlight code python"><span class="n">static</span> <span class="n">PyObject</span> <span class="o">*</span>
<span class="n">strip</span><span class="p">(</span><span class="n">PyObject</span> <span class="o">*</span><span class="bp">self</span><span class="p">,</span> <span class="n">PyObject</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span>
<span class="p">{</span>
    <span class="n">char</span> <span class="o">*</span><span class="n">s</span> <span class="o">=</span> <span class="n">NULL</span><span class="p">;</span>
    <span class="k">if</span> <span class="p">(</span><span class="err">!</span><span class="n">PyArg_ParseTuple</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="s2">"s"</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">s</span><span class="p">))</span> <span class="p">{</span>
        <span class="k">return</span> <span class="n">NULL</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="n">PyUnicode_FromString</span><span class="p">(</span><span class="n">s</span><span class="p">);</span>
<span class="p">}</span></pre><div class="notes"><p>And that happens for every function call in Python. Very often.
All those objects are allocated on the heap. Python is easy, but the price you pay for it
is high. This might give you a first feeling on how much stuff happens in a simple program.</p><p>Printing to stdout and drawing something on the screen is insanely complex too and beyond
this workshop.</p></div></div><div class="step step-level-1" step="13" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="20800" data-y="0" data-z="0"><h1 id="a-word-on-interpreted-languages">A word on interpreted languages</h1><p>TODO: needed?</p><ul><li>Many things in this workshop do not apply to you 1:1.</li><li>If you follow this workshop, a compiled language helps.</li><li>TODO</li></ul><p>Maybe some day you have to extend your language with a C module?</p></div><div class="step step-level-1" step="14" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="22400" data-y="0" data-z="0"><h1 id="theory-complexity">Theory: Complexity</h1><ul><li>Data structures and algorithms can be divided in performance classes.</li><li>General types are space and time complexity.</li><li>Often also divided in worst case, best case, average case and specific operations.</li><li>Complexity classes are given in Big-O notation.</li></ul></div><div class="step step-level-1" step="15" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="24000" data-y="0" data-z="0"><h1 id="theory-big-o-notation">Theory: Big-O Notation</h1><img src="images/bigo.svg"></img><p>www.bigocheatsheet.com</p><div class="notes"><p>O(1) -&gt; constant
O(n) -&gt; linear
O(log n) -&gt; logarithmic
O(n * log n) -&gt; sorting
O(n ** x) -&gt; polynomial
O(x ** n) -&gt; exponential
O(n!) -&gt; fucktorial (oops, typo)</p><p>Data structures and algorithms:</p><p>-&gt; Some have better space / time complexity.
-&gt; Most have tradeoffs, only few are universally useful like arrays / hash tables
-&gt; Some are probalibisitic: i.e. they save you work or space at the expense of accuracy (bloom filters)
-&gt; Difference between O(log n) and O(1) is not important most of the time. (database developers might disagree here though)</p></div></div><div class="step step-level-1" step="16" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="25600" data-y="0" data-z="0"><h1 id="complexity-examples">Complexity examples</h1><ul><li>Time complexity of bubble sort?</li><li>Time complexity of binary search (worst + best)?</li><li>Space complexity of merge sort vs quick sort?</li><li>Removing an element from an array vs from a linked list?</li><li>Best case / Worst case time complexity of get and set of a hash table?</li><li>Space complexity of a hash map?</li></ul><div class="notes"><p>n**2
log2 n
n vs 1
n vs 1
1 and 1 (but much more expensive than an array index)
n</p><p>Makes you wonder why you don't use hash maps all the time?
Indeed they are a wonderful invention, but:</p><ul><li>get is still much more expensive than an array index.</li><li>collisions can happen, making things inefficient.</li><li>range queries and sorting are impossible.</li><li>self balancing trees have O(log n) for get/set but are stable.</li></ul></div></div><div class="step step-level-1" step="17" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="27200" data-y="0" data-z="0"><h1 id="data-structures-in-this-workshop">Data structures in this workshop</h1><p>This was it all. Go pick a book or course.</p><div class="notes"><p>Data structures and algorithms is something you gonna have to learn yourself.
Would totally go over the scope of this workshop and does not work as frontal lecture.</p><p>Do not ignore primitive algorithms like bubble sort.
Remember: Fancy algorithms are slow when n is small, and n is usually small.</p></div></div><div class="step step-level-1" step="18" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="28800" data-y="0" data-z="0"><h1 id="benchmarking">Benchmarking</h1><p>Tests that measure performance requirements.</p><ul><li>Heavily tied to hardware.</li><li>Requires</li></ul><p>What are possible performance metrics?</p><div class="notes"><p>Collect possible performance metrics (unit in parans):</p><ul><li>Execution time (time, cpu cycles)</li><li>Latency (time)</li><li>Throughput (IO, bytes/sec)</li><li>Memory (allocations, peak, total bytes)</li></ul></div></div><div class="step step-level-1" step="19" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="30400" data-y="0" data-z="0"><h1 id="humans-are-bad-at-magnitudes">Humans are bad at magnitudes</h1><p><a href="https://colin-scott.github.io/personal_website/research/interactive_latency.html">https://colin-scott.github.io/personal_website/research/interactive_latency.html</a></p><p>In general:</p><ul><li>CPU &lt; Memory &lt; Files &lt; Network.</li></ul><p>Optimize in that order.</p></div><div class="step step-level-1" step="20" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="32000" data-y="0" data-z="0"><h1 id="profiling">Profiling</h1><p>Profiling is like benchmarking, but just once.</p><div class="notes"><p>Profiling is usually used for finding a bottleneck,
but you benchmark a program as part of it.</p><p>So most of the time the terms can be used interchangeably.</p></div></div><div class="step step-level-1" step="21" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="33600" data-y="0" data-z="0"><h1 id="benchmarking-and-statistics">Benchmarking and Statistics</h1><pre class="highlight code bash">$ hyperfine</pre><div class="notes"><ul><li>Run several times.</li><li>If the variance is not big, take the maximum.</li><li>If the variance is rather large, use min...max.</li></ul></div></div><div class="step step-level-1" step="22" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="35200" data-y="0" data-z="0"><h1 id="benchmarking-and-ci-cd">Benchmarking and CI/CD</h1><p><a href="https://github.com/dandavison/chronologer">https://github.com/dandavison/chronologer</a></p><div class="notes"><p>In an ideal world, performance requirements are tested just like
normal functional requirements.</p><p>Challenges:</p><ul><li>Different machines that benchmarks run on.</li><li>Only comparison between releases makes sense.</li></ul><p>Makes sense only for big projects. Many projects have
their own set of scripts to do this. I'm not aware of a standard solution.</p></div></div><div class="step step-level-1" step="23" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="36800" data-y="0" data-z="0"><h1 id="workshop-project">Workshop Project</h1><p>&#x201C;What I cannot create, I do not understand&#x201D;.</p><p>-- Richard Feynman</p><div class="notes"><p>Words don't cut it. To understand something you have to lay your hands on something
and start exploring. Workshop is about tacit knowledge, you have to connect the little dots
on my slides by working on this small slide project. I can only show you things, not understand and
learn it for you.</p><p>tacit = unausgeprochen</p></div></div><div class="step step-level-1" step="24" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="38400" data-y="0" data-z="0"><h1 id="kv-store-memory-only">KV Store: Memory only</h1><pre class="highlight code go"><span class="kd">type</span><span class="w"> </span><span class="nx">KV</span><span class="w"> </span><span class="kd">map</span><span class="p">[</span><span class="kt">string</span><span class="p">][]</span><span class="kt">byte</span><span class="w">

</span><span class="kd">func</span><span class="w"> </span><span class="p">(</span><span class="nx">kv</span><span class="w"> </span><span class="o">*</span><span class="nx">KV</span><span class="p">)</span><span class="w"> </span><span class="nx">sync</span><span class="p">()</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="kd">var</span><span class="w"> </span><span class="nx">b</span><span class="w"> </span><span class="nx">bytes</span><span class="p">.</span><span class="nx">Buffer</span><span class="p">{}</span><span class="w">
    </span><span class="k">for</span><span class="w"> </span><span class="nx">k</span><span class="p">,</span><span class="w"> </span><span class="nx">v</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="k">range</span><span class="w"> </span><span class="nx">kv</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="nx">b</span><span class="p">.</span><span class="nx">WriteString</span><span class="p">(</span><span class="nx">fmt</span><span class="p">.</span><span class="nx">Sprintf</span><span class="p">(</span><span class="s">"%s=%s\n"</span><span class="p">,</span><span class="w"> </span><span class="nx">k</span><span class="p">,</span><span class="w"> </span><span class="nx">v</span><span class="p">))</span><span class="w">
    </span><span class="p">}</span><span class="w">

    </span><span class="k">return</span><span class="w"> </span><span class="nx">ioutil</span><span class="p">.</span><span class="nx">WriteFile</span><span class="p">(</span><span class="s">"/blah"</span><span class="p">,</span><span class="w"> </span><span class="mo">0644</span><span class="p">,</span><span class="w"> </span><span class="nx">b</span><span class="p">.</span><span class="nx">Bytes</span><span class="p">())</span><span class="w">
</span><span class="p">}</span></pre><div class="notes"><p>You could use a bigh in-memory hash table and sync that to disk sometimes.</p><p>When do you call sync()? After every write? Inefficient.
Less often? Then you will suffer data loss on power loss or crash.</p><p>Sounds impractical, but surprise: Redis actually works this way.
They do not use a hash map internally though, but a tree structure as index.
Oh, and they perform most work in a single thread. Still fast.</p></div></div><div class="step step-level-1" step="25" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="40000" data-y="0" data-z="0"><h1 id="kv-store-append-only">KV Store: Append only</h1><pre class="highlight code bash">set<span class="o">()</span> <span class="o">{</span>
    <span class="nb">printf</span> <span class="s2">"%s=%s\n"</span> <span class="s2">"</span><span class="nv">$1</span><span class="s2">"</span> <span class="s2">"</span><span class="nv">$2</span><span class="s2">"</span> &gt;&gt; ./db
<span class="o">}</span>

get<span class="o">()</span> <span class="o">{</span>
    grep <span class="s2">"^</span><span class="nv">$1</span><span class="s2">="</span> ./db <span class="p">|</span> tail -1 <span class="p">|</span> cut -d<span class="o">=</span> -f2-
<span class="o">}</span></pre><div class="notes"><p>Simple append only write, get reads only the last value.
Every update of an existing key writes it again.</p><p>Terribly slow because get needs to scan the whole db, but
very easy to implement and set is pretty fast. If you hardly
ever call get then this might be a viable solution.</p></div></div><div class="step step-level-1" step="26" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="41600" data-y="0" data-z="0"><h1 id="kv-store-indexed">KV Store: Indexed</h1><pre class="highlight code go"><span class="kd">type</span><span class="w"> </span><span class="nx">KV</span><span class="w"> </span><span class="kd">map</span><span class="p">[</span><span class="kt">string</span><span class="p">]</span><span class="kt">int64</span><span class="w">

</span><span class="kd">func</span><span class="w"> </span><span class="p">(</span><span class="nx">kv</span><span class="w"> </span><span class="o">*</span><span class="nx">KV</span><span class="p">)</span><span class="w"> </span><span class="nx">Get</span><span class="p">(</span><span class="nx">key</span><span class="w"> </span><span class="kt">string</span><span class="p">)</span><span class="w"> </span><span class="p">[]</span><span class="kt">byte</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="c1">// 1. Get &amp; seek to offset</span><span class="w">
    </span><span class="c1">// 2. Read value from db file.</span><span class="w">
</span><span class="p">}</span><span class="w">

</span><span class="kd">func</span><span class="w"> </span><span class="p">(</span><span class="nx">kv</span><span class="w"> </span><span class="o">*</span><span class="nx">KV</span><span class="p">)</span><span class="w"> </span><span class="nx">Set</span><span class="p">(</span><span class="nx">key</span><span class="w"> </span><span class="kt">string</span><span class="p">,</span><span class="w"> </span><span class="nx">val</span><span class="w"> </span><span class="p">[]</span><span class="kt">byte</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="c1">// 1. Check size of db file.</span><span class="w">
    </span><span class="c1">// 2. Append value to file with offset equal to db size</span><span class="w">
    </span><span class="c1">// 3. Update kv index with new offset.</span><span class="w">
</span><span class="p">}</span></pre><div class="notes"><p>This is actually already quite nice!</p><p>This approach is called "log structured", because values are handled
like a stream of logs, just timestamped (or offset stamped) data.</p><p>We can handle any number of values as long as we do not run out of memory.
If we throw in a little caching, we could probably get decent performance.
This would also be a decent usage for something called mmap which we will
look into later in this series.</p><p>When loading the db file, we can reconstruct the index map easily.</p><p>Problems:</p><ul><li>There will be many duplicates if we update the same keys over and over.</li><li>The database file will grow without bound. Might turn out problematic.</li><li>There may only be one writer at a point (race condition between size of db
and actual write).</li></ul></div></div><div class="step step-level-1" step="27" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="43200" data-y="0" data-z="0"><h1 id="kv-store-segments">KV Store: Segments</h1><p>Solution:</p><ol><li>If the db file gets too big (&gt; 32M), start a new one.</li><li>Old one gets compacted in background (i.e. duplicates get removed)</li><li>Index structure remembers what file we need to read.</li></ol><p>TODO: Find good diagram.</p><div class="notes"><p>The compaction step can be easily done in the background.</p><p>Open issues:</p><ul><li>We still need to have all keys in memory.</li><li>Range queries are kinda impossible.</li><li>We can't delete stuff.</li></ul></div></div><div class="step step-level-1" step="28" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="44800" data-y="0" data-z="0"><h1 id="kv-store-deletion">KV Store: Deletion</h1><img src="images/tombstones.png"></img><div class="notes"><p>When we want to delete something, we just write a special value
that denotes that this key was deleted. If a tombstone is the last
value then the key is gone. Compaction can use it to clean up old
traces of that value.</p><p>At this point we already build a key value store that is used out there: Bitcask.</p></div></div><div class="step step-level-1" step="29" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="46400" data-y="0" data-z="0"><h1 id="kv-store-range-queries">KV Store: Range queries</h1><p>TODO: good diagram</p><p>Change approach quite a bit:</p><ol><li>Keep a batch of key-value pairs in memory, but sorted by key.</li><li>If batch gets too big, then swap to disk.</li><li>Keep every 100th key in the offset index.</li><li>If key not in index, go to file and scan the range.</li></ol><div class="notes"><p>This technique is called a Log-Structured-Merge tree (LSM).</p><p>"tree" because usually a tree is used instead of a hash table for easy handling,
but this is not strictly necessary and the main point of the concept.</p><p>Since the index can be "sparse" (not all keys need to be stored), we have very
fine grained control over memory usage. Worst thing is a bit of extra scanning
in the file.</p><p>Open problems:</p><ul><li>Get on non-existing keys.</li><li>Crash safety</li></ul></div></div><div class="step step-level-1" step="30" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="48000" data-y="0" data-z="0"><h1 id="kv-store-wal">KV Store: WAL</h1><p>What if a crash occurs before things get written to disk?</p><p>We have to use a WAL like above! On a crash we can reconstruct everything from it.
Postgres and many other databases make use of this technique too.</p></div><div class="step step-level-1" step="31" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="49600" data-y="0" data-z="0"><h1 id="kv-store-fin">KV Store: Fin</h1><div class="notes"><p>I left quite some details out, but that's something you should be able to figure out.</p></div></div><div class="step step-level-1 chapter-class" step="32" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="51200" data-y="0" data-z="0"><h1 id="cpu">CPU</h1><p>Quiz:</p><ul><li>If two programs A and B execute the same number of instructions will they have roughly the same runtime?</li><li>If two CPUs have the same frequency, can we make assumptions based on their speed?</li></ul><div class="notes"><p>Answer no.</p><p>Every instruction can take a different amount of cpu cycles.
Every instruction can do a lot of different work (SIMD vs normal)</p></div></div><div class="step step-level-1" step="33" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="52800" data-y="0" data-z="0"><h1 id="compilers">Compilers</h1><img src="images/llvm.png" width="100%"></img><div class="notes"><p>Steps to compile something:</p><ul><li>Lexer/Tokenizer (break code in tokens)</li><li>Parser (build AST from code)</li><li>High Level IR (build generic language from it)</li><li>Low level IR (optimize and make it suitable for machines)</li><li>Convert to actual target machine code</li></ul></div></div><div class="step step-level-1" step="34" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="54400" data-y="0" data-z="0"><h1 id="fun-fact-supercompilers">Fun fact: Supercompilers</h1><img src="images/supercompiler.png"></img><div class="notes"><blockquote><ul><li>Compilers do not usually produce the best code and rely heavily on pattern matching, heuristics
and just being smart. They can miss room for optimizations although this is rather rare in practice.
(except Go, which is just a developing compiler)</li><li>Super compilers brute force compilation (sometimes with benchmarks) until they found the best performing
piece of code.</li><li>Not used in practice, since freaking slow but helpful for developing new compiler optimizations.</li></ul></blockquote><p>STOKE: <a href="https://github.com/StanfordPL/stoke">https://github.com/StanfordPL/stoke</a></p></div></div><div class="step step-level-1" step="35" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="56000" data-y="0" data-z="0"><h1 id="how-is-code-executed">How is code executed?</h1><ul><li>Assembly: 1:1 human readable interpretation of machine code.</li><li>Machine code: machine readable instructions (each instruction has an id)</li><li>Assembler: Program that converts assembly to machine code.</li></ul><div class="notes"><ul><li>This slides could be also a talk about "Why interpreted languages suck"<blockquote><p>Most optimizations will not work with python.
As a language it's really disconnected from the HW - every single statement will cause 100s or 1000s of assembly instructions.
Also there are no almost no guarantees how big e.g. arrays or other data structures will be and how they are layout in memory.
You have to rely on your interpreter (and I count Java's JIT as one!) to be fast on modern hardware - most are not and that's why
there's so much C libraries in python, making the whole packaging system a bloody mess.</p></blockquote></li></ul></div></div><div class="step step-level-1" step="36" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="57600" data-y="0" data-z="0"><h1 id="other-terminology">Other terminology</h1><ul><li>Instruction Set Architecture (x86, arm)</li><li>RISC / CISC</li><li>Microarchitecture / Microcode (<tt>Pentium</tt>, <tt>Coffee Lake</tt>...)</li><li>Instruction Set Extensions / SIMD (MMX, AES, SSE...)</li></ul><div class="notes"><p>Example of a CISC instruction set: x86
Today, most complex operations get translated to RISC code though by the CPU.
CISC turned out to be slower, surprisingly.</p><p>RISC: ARM. Usually cheaper to build and also faster.</p><p>Microarchitecture: Implementation of a certain ISA.</p><p>ISE are not directly available in Go, only if the compiler decides to.</p></div></div><div class="step step-level-1" step="37" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="59200" data-y="0" data-z="0"><h1 id="how-is-machine-code-stored-elf">How is machine code stored? ELF!</h1><p>ELF (Executable and linkable format)</p><pre class="highlight code bash">$ readelf --sections /usr/bin/ls
<span class="o">[</span>...<span class="o">]</span>
<span class="o">[</span><span class="m">12</span><span class="o">]</span> .text             PROGBITS         <span class="m">0000000000008020</span>  <span class="m">00008020</span>
<span class="o">[</span>...<span class="o">]</span>
<span class="o">[</span><span class="m">22</span><span class="o">]</span> .data             PROGBITS         <span class="m">0000000000059000</span>  <span class="m">00058000</span>
$ objdump --disassemble /usr/bin/ls</pre><div class="notes"><p>Beside storing the actual instructions ELF solves:</p><ul><li>Storing debugging info</li><li>Making it possible to link with existing other libraries.</li><li>Includes a text (code) and data section (pre-initialized variables)</li><li>Different OS use different formats, but ELF is probably the most relevant for you
and also the most widely known. Windows has a different one.</li></ul></div></div><div class="step step-level-1" step="38" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="60800" data-y="0" data-z="0"><h1 id="go-assembler-1">Go Assembler #1</h1><pre class="highlight code go"><span class="ln"> 1 </span><span class="w"> </span><span class="kn">package</span><span class="w"> </span><span class="nx">main</span><span class="w">
</span><span class="ln"> 2 </span><span class="w">
</span><span class="ln"> 3 </span><span class="w"> </span><span class="c1">//go:noinline</span><span class="w">
</span><span class="ln"> 4 </span><span class="w"> </span><span class="kd">func</span><span class="w"> </span><span class="nx">add</span><span class="p">(</span><span class="nx">a</span><span class="p">,</span><span class="w"> </span><span class="nx">b</span><span class="w"> </span><span class="kt">int</span><span class="p">)</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="p">{</span><span class="w">
</span><span class="ln"> 5 </span><span class="w">     </span><span class="k">return</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nx">b</span><span class="w">
</span><span class="ln"> 6 </span><span class="w"> </span><span class="p">}</span><span class="w">
</span><span class="ln"> 7 </span><span class="w">
</span><span class="ln"> 8 </span><span class="w"> </span><span class="kd">func</span><span class="w"> </span><span class="nx">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span><span class="w">
</span><span class="ln"> 9 </span><span class="w">     </span><span class="nx">add</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">)</span><span class="w">
</span><span class="ln">10 </span><span class="w"> </span><span class="p">}</span></pre></div><div class="step step-level-1" step="39" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="62400" data-y="0" data-z="0"><h1 id="go-assembler-2">Go Assembler #2</h1><p>Go assembly = assembler for a fantasy CPU</p><pre class="highlight code bash">main.add STEXT nosplit <span class="nv">size</span><span class="o">=</span><span class="m">4</span> <span class="nv">args</span><span class="o">=</span>0x10 <span class="nv">locals</span><span class="o">=</span>0x0 <span class="nv">funcid</span><span class="o">=</span>0x0 <span class="nv">align</span><span class="o">=</span>0x0
      <span class="o">(</span>test.go:4<span class="o">)</span>     TEXT    main.add<span class="o">(</span>SB<span class="o">)</span>, NOSPLIT<span class="p">|</span>ABIInternal, <span class="nv">$0</span>-16
      <span class="o">(</span>test.go:4<span class="o">)</span>     FUNCDATA        <span class="nv">$0</span>, gclocals&#xB7;g2BeySu+wFnoycgXfElmcg<span class="o">==(</span>SB<span class="o">)</span>
      <span class="o">(</span>test.go:4<span class="o">)</span>     FUNCDATA        <span class="nv">$1</span>, gclocals&#xB7;g2BeySu+wFnoycgXfElmcg<span class="o">==(</span>SB<span class="o">)</span>
      <span class="o">(</span>test.go:4<span class="o">)</span>     FUNCDATA        <span class="nv">$5</span>, main.add.arginfo1<span class="o">(</span>SB<span class="o">)</span>
      <span class="o">(</span>test.go:4<span class="o">)</span>     FUNCDATA        <span class="nv">$6</span>, main.add.argliveinfo<span class="o">(</span>SB<span class="o">)</span>
      <span class="o">(</span>test.go:4<span class="o">)</span>     PCDATA  <span class="nv">$3</span>, <span class="nv">$1</span>
      <span class="o">(</span>test.go:5<span class="o">)</span>     ADDQ    BX, AX
      <span class="o">(</span>test.go:5<span class="o">)</span>     RET
<span class="o">(</span>...<span class="o">)</span></pre><div class="notes"><p>Important: Explain registers!</p><p>Can we just say: To make things faster you have to reduce the number of instructions?</p><p>Sadly no. Modern CPUs are MUCH complexer than machines that sequentially execute instructions.
They take all kind of shortcuts to execute things faster - most of the time.
See also: Megaherz myth (-&gt; higher clock = more cycles per time)</p><p>Effects that may play a role</p><ul><li>Not every instruction takes the same amount of cycles (MOV 1 cycle,</li><li>Pipelining</li><li>Superscalar Execution</li><li>Branch prediction / Cache prefetching</li><li>Out-of-order execution</li><li>Cache misses (fetching from main memory mean</li></ul><p>List of typical cycles per instructions ("latency"): <a href="https://www.agner.org/optimize/instruction_tables.pdf">https://www.agner.org/optimize/instruction_tables.pdf</a></p></div></div><div class="step step-level-1" step="40" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="64000" data-y="0" data-z="0"><h1 id="detour-calling-conventions">Detour: Calling conventions</h1><pre class="highlight code asm"><span class="nl">FuncAddGo:</span><span class="w">
   </span><span class="nf">MOVQ</span><span class="w"> </span><span class="mi">0x8</span><span class="p">(</span><span class="no">SP</span><span class="p">),</span><span class="w"> </span><span class="no">AX</span><span class="w">  </span><span class="c1">; get arg x
</span><span class="w">   </span><span class="nf">MOVQ</span><span class="w"> </span><span class="mi">0x10</span><span class="p">(</span><span class="no">SP</span><span class="p">),</span><span class="w"> </span><span class="no">CX</span><span class="w"> </span><span class="c1">; get arg y
</span><span class="w">   </span><span class="nf">ADDQ</span><span class="w"> </span><span class="no">CX</span><span class="p">,</span><span class="w"> </span><span class="no">AX</span><span class="w">       </span><span class="c1">; %ax &lt;- x + y
</span><span class="w">   </span><span class="nf">MOVQ</span><span class="w"> </span><span class="no">AX</span><span class="p">,</span><span class="w"> </span><span class="mi">0x20</span><span class="p">(</span><span class="no">SP</span><span class="p">)</span><span class="w"> </span><span class="c1">; return x+y-z
</span><span class="w">   </span><span class="nf">RET</span></pre><pre class="highlight code asm"><span class="nl">FuncAddC:</span><span class="w">
    </span><span class="nf">LEAL</span><span class="w">  </span><span class="p">(</span><span class="nv">%rdi</span><span class="p">,</span><span class="nv">%rsi</span><span class="p">),</span><span class="w"> </span><span class="nv">%eax</span><span class="w">
    </span><span class="nf">ADDL</span><span class="w">  </span><span class="nv">%edx</span><span class="p">,</span><span class="w"> </span><span class="nv">%eax</span><span class="w">
    </span><span class="nf">RETQ</span></pre><div class="notes"><p>Go and C have different calling conventions.
C passes params and return values over registers
Go uses memory addresses (on the stack)</p><p>This makes it impossible to call a C function directly from Go.
Some languages like Zig share the same calling convetions and make
it therefore possible to directly call C code. For go we need a weird
abstraction layer called cgo.</p></div></div><div class="step step-level-1" step="41" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="65600" data-y="0" data-z="0"><h1 id="inlining-functions">Inlining functions</h1><p>Inlining functions can speed up things at the cost of increased ELF size.</p><p>Advantage: Parameters do not need to get copied, but CPU can re-use whatever
is in the registers alreadys. Also return values do not need to be copied.</p><p>Only done for small functions and only in hot paths.</p></div><div class="step step-level-1" step="42" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="67200" data-y="0" data-z="0"><h1 id="pipelining">Pipelining</h1><p><a href="https://de.wikipedia.org/wiki/Pipeline_(Prozessor">https://de.wikipedia.org/wiki/Pipeline_(Prozessor</a>)</p><p>LOAD: Load the instruction from memory, increment instruction counter.
DECODE: Data for the command is loaded.
EXEC: Instruction is executed.
WRITEBACK: Result is written back to a register.</p><ul><li>Every instruction needs to do this</li><li>Modern CPUs can work on many instructions at the same time</li><li>They can be also re-ordered by the CPU!</li><li>This can lead to issues when an instruction depends on results of another instructions! (branches!)</li><li>It can even happen that we do unncessary work! See SPECTRE and MELTDOWN security issues!</li></ul></div><div class="step step-level-1" step="43" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="68800" data-y="0" data-z="0"><h1 id="branch-prediction">Branch prediction</h1><p>... you can give hints to your CPU!</p><pre class="highlight code c"><span class="k">if</span><span class="p">(</span><span class="n">likely</span><span class="p">(</span><span class="n">a</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">1</span><span class="p">))</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="c1">// ...
</span><span class="p">}</span><span class="w">

</span><span class="k">if</span><span class="p">(</span><span class="n">unlikely</span><span class="p">(</span><span class="n">err</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">0</span><span class="p">))</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="c1">// ...
</span><span class="p">}</span></pre><p>No likely() in Go, compiler tries to insert those hints automayically.
Not much of an important optimization nowadays though as CPUs get a lot better:</p><p><a href="https://de.wikipedia.org/wiki/Sprungvorhersage">https://de.wikipedia.org/wiki/Sprungvorhersage</a></p><p>(but can be relevant for very hot paths on cheap ARM cpus)</p></div><div class="step step-level-1" step="44" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="70400" data-y="0" data-z="0"><h1 id="branch-prediction-in-real-life">Branch prediction in real life</h1><pre class="highlight code go"><span class="k">for</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="nx">i</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="nx">i</span><span class="w"> </span><span class="p">&lt;</span><span class="w"> </span><span class="nx">N</span><span class="p">;</span><span class="w"> </span><span class="nx">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="nx">unsorted</span><span class="p">[</span><span class="nx">i</span><span class="p">]</span><span class="w"> </span><span class="p">&lt;</span><span class="w"> </span><span class="nx">X</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="nx">sum</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="nx">unsorted</span><span class="p">[</span><span class="nx">i</span><span class="p">];</span><span class="w">
    </span><span class="p">}</span><span class="w">
</span><span class="p">}</span></pre><pre class="highlight code go"><span class="k">for</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="nx">i</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="nx">i</span><span class="w"> </span><span class="p">&lt;</span><span class="w"> </span><span class="nx">N</span><span class="p">;</span><span class="w"> </span><span class="nx">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="nx">sorted</span><span class="p">[</span><span class="nx">i</span><span class="p">]</span><span class="w"> </span><span class="p">&lt;</span><span class="w"> </span><span class="nx">X</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="nx">sum</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="nx">sorted</span><span class="p">[</span><span class="nx">i</span><span class="p">];</span><span class="w">
    </span><span class="p">}</span><span class="w">
</span><span class="p">}</span></pre><div class="notes"><p>Effect is unnotice-able if optimizations are enabled.
Why? Compilers can make the inner branch a branchless statement.</p></div></div><div class="step step-level-1" step="45" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="72000" data-y="0" data-z="0"><h1 id="go-1-20-profile-guided-optimization">Go 1.20: Profile Guided Optimization</h1><p>Idea:</p><ul><li>Let program run in analysis mode.</li><li>Capture data about what branches were hit how often.</li><li>Use this data on the next compile to decide which branch is likely!</li></ul><img src="images/pgo.png"></img><div class="notes"><p>Also decides on where to inline functions.</p><p><a href="https://tip.golang.org/doc/pgo">https://tip.golang.org/doc/pgo</a></p><p>Old news for languages like C.</p></div></div><div class="step step-level-1" step="46" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="73600" data-y="0" data-z="0"><h1 id="branchless-programming">Branchless programming</h1><pre class="highlight code c"><span class="kt">int32_t</span><span class="w"> </span><span class="nf">max</span><span class="p">(</span><span class="kt">int32_t</span><span class="w"> </span><span class="n">a</span><span class="p">,</span><span class="w"> </span><span class="kt">int32_t</span><span class="w"> </span><span class="n">b</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="k">if</span><span class="p">(</span><span class="n">a</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="n">b</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="k">return</span><span class="w"> </span><span class="n">a</span><span class="p">;</span><span class="w">
    </span><span class="p">}</span><span class="w">
    </span><span class="k">return</span><span class="w"> </span><span class="n">b</span><span class="p">;</span><span class="w">
</span><span class="p">}</span></pre><pre class="highlight code c"><span class="k">return</span><span class="w"> </span><span class="p">(</span><span class="n">a</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="n">b</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="p">(</span><span class="n">a</span><span class="w"> </span><span class="o">&lt;=</span><span class="w"> </span><span class="n">b</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">b</span><span class="p">;</span></pre><pre class="highlight code c"><span class="k">return</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="p">((</span><span class="n">a</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">b</span><span class="p">)</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="p">((</span><span class="n">a</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">b</span><span class="p">)</span><span class="w"> </span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="mi">31</span><span class="p">)</span></pre><div class="notes"><p>Probably not relevant in most cases, as compiler are usually smart, but CAN
be a life saver in really hot loops.</p></div></div><div class="step step-level-1" step="47" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="75200" data-y="0" data-z="0"><h1 id="loop-unrolling">Loop unrolling</h1><ul><li>A for loop is just a repeated branch condition.</li><li>Compilers unroll simple loops.</li><li>If they don't hand unrolling can be useful (very seldom!)</li></ul><p>TODO: Example</p></div><div class="step step-level-1" step="48" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="76800" data-y="0" data-z="0"><h1 id="reduce-number-of-instructions">Reduce number of instructions</h1><p>memcpy example</p><p>TODO: Instrinsic</p></div><div class="step step-level-1" step="49" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="78400" data-y="0" data-z="0"><h1 id="i-want-to-mov-mov-it">I want to MOV, MOV it</h1><pre class="highlight code">MOV &lt;dst&gt; &lt;src&gt;</pre><pre class="highlight code">MOV &lt;reg&gt; &lt;reg&gt;
MOV &lt;mem&gt; &lt;reg&gt;
MOV &lt;reg&gt; &lt;mem&gt;</pre><p>-&gt; Access to main memory is 125ns, L1 cache is ~1ns</p><p>Fun fact: MOV alone is Turing complete: <a href="https://github.com/xoreaxeaxeax/movfuscator">https://github.com/xoreaxeaxeax/movfuscator</a></p></div><div class="step step-level-1" step="50" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="80000" data-y="0" data-z="0"><h1 id="the-von-neumann-bottleneck">The von Neumann Bottleneck</h1><p>von Neumann Architektur:</p><ul><li>Computer Architecture where there is common memory accessible by all cores</li><li>Memory contains Data as well as code instructions</li><li>All data/code goes over a common bus</li><li>Pretty much all computer nowadays are build this way</li></ul><p>Bottleneck: Memory acess is much slower than CPUs can process the data.</p></div><div class="step step-level-1" step="51" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="81600" data-y="0" data-z="0"><h1 id="l1-l2-l3">L1, L2, L3</h1><p>Just add caches!</p><img src="images/whatcouldgowrong.jpeg"></img><p>TODO: Add picture of cache architecture.</p></div><div class="step step-level-1" step="52" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="83200" data-y="0" data-z="0"><h1 id="cache-lines">Cache lines</h1><p>typicall 64 byte
Read an written in one go!</p></div><div class="step step-level-1" step="53" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="84800" data-y="0" data-z="0"><h1 id="caches-misses">Caches misses</h1><p>Unsure if you have cache misses? Use the perf stat -p &lt;PID&gt; command!</p><p><a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/monitoring_and_managing_system_status_and_performance/getting-started-with-perf_monitoring-and-managing-system-status-and-performance">https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/monitoring_and_managing_system_status_and_performance/getting-started-with-perf_monitoring-and-managing-system-status-and-performance</a>
<a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/monitoring_and_managing_system_status_and_performance/overview-of-performance-monitoring-options_monitoring-and-managing-system-status-and-performance">https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/monitoring_and_managing_system_status_and_performance/overview-of-performance-monitoring-options_monitoring-and-managing-system-status-and-performance</a></p><p>counter example 1-3</p></div><div class="step step-level-1" step="54" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="86400" data-y="0" data-z="0"><h1 id="struct-size-matters">Struct size matters</h1><pre class="highlight code go"><span class="c1">// How big is this struct?</span><span class="w">
</span><span class="kd">type</span><span class="w"> </span><span class="nx">XXX</span><span class="w"> </span><span class="kd">struct</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nx">A</span><span class="w"> </span><span class="kt">int64</span><span class="w">
    </span><span class="nx">B</span><span class="w"> </span><span class="kt">uint32</span><span class="w">
    </span><span class="nx">C</span><span class="w"> </span><span class="kt">byte</span><span class="w">
    </span><span class="nx">D</span><span class="w"> </span><span class="kt">bool</span><span class="w">
    </span><span class="nx">E</span><span class="w"> </span><span class="kt">string</span><span class="w">
    </span><span class="nx">F</span><span class="w"> </span><span class="p">[]</span><span class="kt">byte</span><span class="w">
    </span><span class="nx">G</span><span class="w"> </span><span class="kd">map</span><span class="p">[</span><span class="kt">string</span><span class="p">]</span><span class="kt">int64</span><span class="w">
    </span><span class="nx">H</span><span class="w"> </span><span class="kd">interface</span><span class="p">{}</span><span class="w">
    </span><span class="nx">I</span><span class="w"> </span><span class="kt">int</span><span class="w">
</span><span class="p">}</span></pre></div><div class="step step-level-1" step="55" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="88000" data-y="0" data-z="0"><h1 id="padding-can-happen">Padding can happen</h1><pre class="highlight code go"><span class="nx">x</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">XXX</span><span class="p">{}</span><span class="w">  </span><span class="c1">// measured with Go 1.20!</span><span class="w">
</span><span class="nx">fmt</span><span class="p">.</span><span class="nx">Println</span><span class="p">(</span><span class="s">"A"</span><span class="p">,</span><span class="w"> </span><span class="nx">unsafe</span><span class="p">.</span><span class="nx">Sizeof</span><span class="p">(</span><span class="nx">x</span><span class="p">.</span><span class="nx">A</span><span class="p">))</span><span class="w">  </span><span class="c1">// 8</span><span class="w">
</span><span class="nx">fmt</span><span class="p">.</span><span class="nx">Println</span><span class="p">(</span><span class="s">"B"</span><span class="p">,</span><span class="w"> </span><span class="nx">unsafe</span><span class="p">.</span><span class="nx">Sizeof</span><span class="p">(</span><span class="nx">x</span><span class="p">.</span><span class="nx">B</span><span class="p">))</span><span class="w">  </span><span class="c1">// 4</span><span class="w">
</span><span class="nx">fmt</span><span class="p">.</span><span class="nx">Println</span><span class="p">(</span><span class="s">"C"</span><span class="p">,</span><span class="w"> </span><span class="nx">unsafe</span><span class="p">.</span><span class="nx">Sizeof</span><span class="p">(</span><span class="nx">x</span><span class="p">.</span><span class="nx">C</span><span class="p">))</span><span class="w">  </span><span class="c1">// 1</span><span class="w">
</span><span class="nx">fmt</span><span class="p">.</span><span class="nx">Println</span><span class="p">(</span><span class="s">"D"</span><span class="p">,</span><span class="w"> </span><span class="nx">unsafe</span><span class="p">.</span><span class="nx">Sizeof</span><span class="p">(</span><span class="nx">x</span><span class="p">.</span><span class="nx">D</span><span class="p">))</span><span class="w">  </span><span class="c1">// 1 (&lt;-- +2 padding)</span><span class="w">
</span><span class="nx">fmt</span><span class="p">.</span><span class="nx">Println</span><span class="p">(</span><span class="s">"E"</span><span class="p">,</span><span class="w"> </span><span class="nx">unsafe</span><span class="p">.</span><span class="nx">Sizeof</span><span class="p">(</span><span class="nx">x</span><span class="p">.</span><span class="nx">E</span><span class="p">))</span><span class="w">  </span><span class="c1">// 16</span><span class="w">
</span><span class="nx">fmt</span><span class="p">.</span><span class="nx">Println</span><span class="p">(</span><span class="s">"F"</span><span class="p">,</span><span class="w"> </span><span class="nx">unsafe</span><span class="p">.</span><span class="nx">Sizeof</span><span class="p">(</span><span class="nx">x</span><span class="p">.</span><span class="nx">F</span><span class="p">))</span><span class="w">  </span><span class="c1">// 24</span><span class="w">
</span><span class="nx">fmt</span><span class="p">.</span><span class="nx">Println</span><span class="p">(</span><span class="s">"G"</span><span class="p">,</span><span class="w"> </span><span class="nx">unsafe</span><span class="p">.</span><span class="nx">Sizeof</span><span class="p">(</span><span class="nx">x</span><span class="p">.</span><span class="nx">G</span><span class="p">))</span><span class="w">  </span><span class="c1">// 8</span><span class="w">
</span><span class="nx">fmt</span><span class="p">.</span><span class="nx">Println</span><span class="p">(</span><span class="s">"H"</span><span class="p">,</span><span class="w"> </span><span class="nx">unsafe</span><span class="p">.</span><span class="nx">Sizeof</span><span class="p">(</span><span class="nx">x</span><span class="p">.</span><span class="nx">H</span><span class="p">))</span><span class="w">  </span><span class="c1">// 16</span><span class="w">
</span><span class="nx">fmt</span><span class="p">.</span><span class="nx">Println</span><span class="p">(</span><span class="s">"I"</span><span class="p">,</span><span class="w"> </span><span class="nx">unsafe</span><span class="p">.</span><span class="nx">Sizeof</span><span class="p">(</span><span class="nx">x</span><span class="p">.</span><span class="nx">I</span><span class="p">))</span><span class="w">  </span><span class="c1">// 8</span><span class="w">
</span><span class="nx">fmt</span><span class="p">.</span><span class="nx">Println</span><span class="p">(</span><span class="s">"x"</span><span class="p">,</span><span class="w"> </span><span class="nx">unsafe</span><span class="p">.</span><span class="nx">Sizeof</span><span class="p">(</span><span class="nx">x</span><span class="p">))</span><span class="w">    </span><span class="c1">// 88 (not 86!)</span></pre><div class="notes"><p>If a struct is bigger than a cache line, then accessing .A and .I would
cause the CPU to always require to get a new cache line!</p></div></div><div class="step step-level-1" step="56" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="89600" data-y="0" data-z="0"><h1 id="binary-size-matters">Binary size matters</h1><ul><li>More debug symbols, functions and instructions make the binary bigger.</li><li>A process needs <em>at least</em> as much memory as the binary size (caveat: only the first one)</li><li>The bigger the binary, the longer the startup size. Important for shortlived processes (scripts!)</li><li>CPUs have caches for code instructions. If your program is so fat that that the caches get evicted,
you might have created a performance issue. (ex: jumping between two functions in your binary, located across)</li></ul><div class="notes"><p>Binaries can be compressed with UPX, but that does make start up time faster - contrary to that.</p><p>Also, in the embedded world the binary size is way more important, but 30M binaries seem excessive
even on servers. Go is doing a bad job here while Rust produces tiny outputs.</p></div></div><div class="step step-level-1" step="57" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="91200" data-y="0" data-z="0"><h1 id="detour-perf-command">Detour: perf command</h1><p>System wide profiling</p><pre class="highlight code bash">perf stat -a &lt;command&gt;   <span class="c1"># Like `time` but much better.
</span>perf stat -a -p &lt;PID&gt;    <span class="c1"># Attach to existin process.
</span>perf mem                 <span class="c1"># Detailed report about memory access / misses
</span>perf c2c                 <span class="c1"># Can find false sharing (see next chapter)</span></pre></div><div class="step step-level-1" step="58" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="92800" data-y="0" data-z="0"><h1 id="detour-pprof">Detour: <tt>pprof</tt></h1><p>Visualize where the program spends time:</p><ul><li>Call graph is annotated times.</li><li>Alternatively available as flamegraph.</li></ul><pre class="highlight code bash"><span class="c1"># pprof server under port 3000:
</span>$ go tool pprof localhost:3000/debug/pprof/profile</pre><div class="notes"><p>Look at images/dashboard_pprof.svg here.</p><p>Pprof is also available for Python, but not as well integrated:
<a href="https://github.com/timpalpant/pypprof">https://github.com/timpalpant/pypprof</a></p></div></div><div class="step step-level-1" step="59" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="94400" data-y="0" data-z="0"><h1 id="detour-flame-graphs">Detour: Flame graphs</h1><pre class="highlight code go"><span class="c1">// Alternative for shortlived programs.</span><span class="w">
</span><span class="c1">// Paste this in main():</span><span class="w">
</span><span class="nx">f</span><span class="p">,</span><span class="w"> </span><span class="nx">_</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">os</span><span class="p">.</span><span class="nx">Create</span><span class="p">(</span><span class="s">"cpu.pprof"</span><span class="p">)</span><span class="w">
</span><span class="nx">pprof</span><span class="p">.</span><span class="nx">StartCPUProfile</span><span class="p">(</span><span class="nx">f</span><span class="p">)</span><span class="w">
</span><span class="k">defer</span><span class="w"> </span><span class="nx">pprof</span><span class="p">.</span><span class="nx">StopCPUProfile</span><span class="p">()</span><span class="w">

</span><span class="c1">// ... do your work here ...</span></pre><div class="notes"><p>See images/brig_flamegraph.png
See images/brig_flamegraph.html</p><p>Perfect to see what time is spend in in what symbol.
Available for:</p><ul><li>CPU</li><li>Memory Allocations (although I like pprof more here)</li><li>Off-CPU (i.e. I/O)</li></ul></div></div><div class="step step-level-1" step="60" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="96000" data-y="0" data-z="0"><h1 id="cache-coherency">Cache coherency</h1><p>In multithreaded programs, a cache gets evicted</p></div><div class="step step-level-1" step="61" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="97600" data-y="0" data-z="0"><h1 id="false-sharing">False sharing</h1><p>Counter4 example.</p><p>Multiple threads use the same memory</p><p>Can be fixed by introducing padding!</p><ul><li>False sharing / True sharing (i.e. when to pad your data structures
<a href="https://alic.dev/blog/false-sharing.html">https://alic.dev/blog/false-sharing.html</a> )</li></ul></div><div class="step step-level-1" step="62" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="99200" data-y="0" data-z="0"><h1 id="true-sharing">True sharing</h1><p>This is when the idea of introducing caches between CPU and memory works out.
Good news: Can be controlled by:</p><ul><li>Limiting struct sizes to 64 bytes</li><li>Grouping often accessed data together.
(arrays of data, not array of structs of data)</li></ul><p>-&gt; employee example</p></div><div class="step step-level-1" step="63" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="100800" data-y="0" data-z="0"><h1 id="data-oriented-programming">Data oriented programming</h1><p>The science of designing programs in a CPU friendly way.</p><div class="notes"><p>DOP is often mentioned as contrast to OOP, but both concepts can complement each other.</p><p>Object oriented program is designing the program in a way that is friendly to humans.</p><p>It does by encapsulating data and methods together. By coincidence, this is not exactly
helpful to the machine your program runs on. Why?</p><ul><li>global state (i.e. impure functions) make branch/cache predictions way harder.</li><li>hurts cache locality.</li></ul></div></div><div class="step step-level-1" step="64" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="102400" data-y="0" data-z="0"><h1 id="matrix-traversal">Matrix Traversal</h1><ul><li>Why is column traversal so much slower?</li></ul><p>Good picture source: <a href="https://medium.com/mirum-budapest/introduction-to-data-oriented-programming-85b51b99572d">https://medium.com/mirum-budapest/introduction-to-data-oriented-programming-85b51b99572d</a></p></div><div class="step step-level-1" step="65" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="104000" data-y="0" data-z="0"><h1 id="employees">Employees</h1><ul><li>Why is the variant with two arrays faster?</li><li>What happens if we make the name array longer/shorter?</li></ul><p>Array-of-Structures vs Structures-of-Arrays</p><p><a href="https://www.dataorienteddesign.com/dodmain/">https://www.dataorienteddesign.com/dodmain/</a></p></div><div class="step step-level-1" step="66" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="105600" data-y="0" data-z="0"><h1 id="memcpy"><tt>memcpy</tt></h1><ul><li>Why is the single-byte memcpy so much slower?</li><li>What evil trick is the system memcpy doing?</li><li>Can we do even faster?</li></ul><div class="notes"><p>-&gt; Problem: von-Neumann-Bottleneck.
-&gt; CPU can work on data faster than typical RAM can deliver it.
-&gt; Workaround: Caches in the CPU, Prefetching.
-&gt; Actual solution: Data oriented design.
-&gt; Sequential access, tight packing of data, SIMD (and if you're crazy: DMA)
-&gt; Still best way to speed up copies: don't copy.</p></div><div class="notes"><p>Object oriented design tends to fuck this up and many Games (at their core)
do not use OOP. You can use both at the same time though!</p></div></div><div class="step step-level-1" step="67" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="107200" data-y="0" data-z="0"><h1 id="process-scheduler">Process scheduler</h1><p>We're not alone on a system. Every process get assigned a share of time that it may execute.</p><ul><li>After execution: Store state in RAM.</li><li>Before execution: Load state from RAM.</li></ul><img src="images/process_states.jpg"></img><img src="images/process_states.webp"></img><p>-&gt; Expensive. Switching too often is expensive.</p><div class="notes"><ul><li>scheduler types (O(n), O(1), CFS, BFS)</li><li>scheduler is determined at compile time.</li><li>there are some knobs to tune the scheduler, but not that interesting.</li><li>Show process states with ps a.</li></ul></div></div><div class="step step-level-1" step="68" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="108800" data-y="0" data-z="0"><h1 id="process-load">Process load</h1><ul><li>Load param counts the number of processes in running or waiting state.</li><li>"0" describes an idle system.</li><li>If the system has a higher load number than cores it is overloaded.</li><li>load is averaged over 5, 10, 15 by default.</li><li>use load5 for graphs, load15 for quick judgmenet.</li></ul></div><div class="step step-level-1" step="69" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="110400" data-y="0" data-z="0"><h1 id="process-niceness">Process niceness</h1><p>Niceness is the "weight" for a certain process during scheduling:</p><ul><li>Ranges from -20 to +19.</li><li>-20 gives the process more time to execute.</li><li>0 is the default.</li><li>+19 gives the process way less to execute.</li></ul><p>Can be set via nice (new commands), renice (running programs)
Exact behaviour depends on scheduler (scheduling frequency vs time slice size)</p></div><div class="step step-level-1" step="70" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="112000" data-y="0" data-z="0"><h1 id="rough-rules-to-take-away">Rough Rules to take away</h1><ol><li>Only use so much memory as you really need.</li><li>Writes modify the cache. Directly use your data or declare it later.</li><li>Keep your structs small. (&lt;64 byte)</li><li>Avoid nesting of data, if possible.</li><li>For small structures (&lt;64 byte) prefer copying over pointers.</li><li>Avoid jumpin around in your memory a lot.</li><li>Avoid virtual methods and inheritance.</li></ol><p>TODO: Revisit those rules.</p><div class="notes"><p>Go even warns about too structures (if they are used as values):</p><p>gocritic hugeParam: cfg is heavy (240 bytes); consider passing it by pointer</p></div></div><div class="step step-level-1" step="71" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="113600" data-y="0" data-z="0"><p>TODO: sync.Pool
TODO: pprof for memory.</p><h1 id="memory">Memory</h1></div><div class="step step-level-1" step="72" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="115200" data-y="0" data-z="0"><h1 id="ram">RAM</h1><ul><li><strong>RAM</strong> = Random Access Memory</li><li>Huge, sequential line of individual memory cells</li><li>Usually can only be addressed in pages</li><li>Memory controller that handles the actual interaction.</li><li>Two major types: Static RAM (SRAM) vs Dynamic RAM (DRAM)</li></ul><div class="notes"><p>SDRAM = Synchronous DRAM
DDR-SDRAM = Double Data Rate SDRAM</p></div></div><div class="step step-level-1" step="73" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="116800" data-y="0" data-z="0"><h1 id="dram-one-bit-please">DRAM - one bit, please</h1><img src="images/dram.png" width="100%" align="center"></img><div class="notes"><p>Dynamic sounds good, doesn't it?</p><ul><li>Very simple and cheap to produce.</li><li>High density (many cells per area)</li><li>Needs to be refreshed constantly (64ns or so)</li></ul><p>Fun fact: DRAM enables a hardware-based security attack: ROWHAMMER.
Changing a row of DRAM cells can, if done very often, switch a nearby row.
This can be used to change data like "userIsLoggedIn".</p></div></div><div class="step step-level-1" step="74" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="118400" data-y="0" data-z="0"><h1 id="sram-one-bit-please">SRAM - one bit, please</h1><img src="images/sram.png" width="100%" align="center"></img><div class="notes"><ul><li>Very fast. 10x speed of DRAM</li><li>No refresh required.</li><li>Low power consumption</li><li>Expensive, not so high density</li></ul></div></div><div class="step step-level-1" step="75" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="120000" data-y="0" data-z="0"><h1 id="why-use-dram-at-all">Why use DRAM at all?</h1><ul><li>Because it's cheap,  and we need tons of it.</li><li>Main memory is all DRAM.</li><li>Caches (L1-L3) are SRAM.</li><li>A lightbulb is maybe OSRAM (Sorry.)</li></ul><div class="notes"><p>So basically...</p><p>again, hardware is at fault
and instead of fixing it with some Pfiffikus
we software devs have to cope with slow main memory.</p></div></div><div class="step step-level-1" step="76" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="121600" data-y="0" data-z="0"><h1 id="numa">NUMA</h1><p>Is the access to all memory offsets equally fast?</p><ul><li>Not if you have more than one CPU!</li><li>Every CPU gets 1/nth of the memory.</li><li>Every CPU can access the completely memory.</li><li>Non-local access is costly.</li></ul><div class="notes"><p>NUMA - non uniform memory access</p><p>Linux is NUMA very well capable and that's why it's such a popular server operating system.
Or one of the reasons at least.</p></div></div><div class="step step-level-1" step="77" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="123200" data-y="0" data-z="0"><h1 id="how-the-heck-does-this-stuff-relate-to-me">How the heck does this stuff relate to me?</h1><p>Not so much on a daily basis, to be fair. But:</p><ul><li>Memory allocations are expensive.</li><li>Strategies to make less/smaller allocations help performance</li><li>Requires sadly an understanding how the OS handles memory.</li></ul></div><div class="step step-level-1" step="78" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="124800" data-y="0" data-z="0"><p>TODO: Maybe use graphics from here: <a href="https://medium.com/eureka-engineering/understanding-allocations-in-go-stack-heap-memory-9a2631b5035d">https://medium.com/eureka-engineering/understanding-allocations-in-go-stack-heap-memory-9a2631b5035d</a></p><h1 id="the-stack-heap-1">The stack &amp; heap #1</h1><pre class="highlight code go"><span class="c1">//go:noinline</span><span class="w">
</span><span class="kd">func</span><span class="w"> </span><span class="nx">f</span><span class="p">()</span><span class="w"> </span><span class="o">*</span><span class="kt">int</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nx">v</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="mi">3</span><span class="w">
    </span><span class="k">return</span><span class="w"> </span><span class="o">&amp;</span><span class="nx">v</span><span class="w">
</span><span class="p">}</span><span class="w">

</span><span class="kd">func</span><span class="w"> </span><span class="nx">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="c1">// Two for the stack:</span><span class="w">
    </span><span class="c1">// a=0xc00009aef8 b=0xc00009aef0</span><span class="w">
    </span><span class="nx">a</span><span class="p">,</span><span class="w"> </span><span class="nx">b</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="mi">23</span><span class="p">,</span><span class="w"> </span><span class="mi">42</span><span class="w">

    </span><span class="c1">// Two for the heap:</span><span class="w">
    </span><span class="c1">// c=0xc0000b2000 d=0xc0000b2008</span><span class="w">
    </span><span class="nx">c</span><span class="p">,</span><span class="w"> </span><span class="nx">d</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">f</span><span class="p">(),</span><span class="w"> </span><span class="nx">f</span><span class="p">()</span><span class="w">
</span><span class="p">}</span></pre></div><div class="step step-level-1" step="79" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="126400" data-y="0" data-z="0"><h1 id="the-stack-heap-2">The stack &amp; heap #2</h1><p><strong>Stack</strong> is...</p><ul><li>...cleaned up automatically on return</li><li>...bound to a function call</li><li>...preferred if possible.</li><li>...can be reasoned about during compile time</li><li>...good for small amounts of data.</li></ul><p><strong>Heap</strong> is...</p><ul><li>...needs to be explicitly requested</li><li>...needs to be explititly cleaned up</li><li>...can be used until freed.</li><li>...should be used when required.</li><li>...usually required for a lot of data.</li></ul></div><div class="step step-level-1" step="80" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="128000" data-y="0" data-z="0"><h1 id="the-stack-heap-3">The stack &amp; heap #3</h1><p>Go is clever and hides this from you via
<strong>escape analysis</strong>:</p><pre class="highlight code go"><span class="kd">func</span><span class="w"> </span><span class="nx">f</span><span class="p">()</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="nx">v</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="mi">3</span><span class="p">;</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="o">&amp;</span><span class="nx">v</span><span class="w"> </span><span class="p">}</span><span class="w">
</span><span class="kd">func</span><span class="w"> </span><span class="nx">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nx">fmt</span><span class="p">.</span><span class="nx">Println</span><span class="p">(</span><span class="nx">f</span><span class="p">())</span><span class="w">
</span><span class="p">}</span></pre><pre class="highlight code bash">$ go build -gcflags<span class="o">=</span><span class="s2">"-m"</span> .
./main.go:3:2: moved to heap: v</pre><p>The more you allocate on the heap, the more pressure you put on the
memory bookkeeping and the garbage collector.</p><p><strong>Performance tip:</strong> Avoid variables escaping to the heap:</p><ul><li></li><li>Avoid using pointers if unnecessary</li><li>Prefer return by value if value is small (&lt; 128 byte) (small copy is faster than GC)</li><li>Don't overreact here though. Don't make your APIs ugly just because you know this little fact. Use this in hot loops. AFTER measurement.</li></ul><div class="notes"><p>Never heard of this stuff, why should I care?</p><p>Difference is important in C
Well, you're lucky enough that your compiler does it for you
Or you're unlucky enough to use python where all hope is forlorn</p></div></div><div class="step step-level-1" step="81" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="129600" data-y="0" data-z="0"><h1 id="detour-what-is-a-stackoverflow">Detour: What is a StackOverflow?</h1><p>Why using the stack only for small data if you can also use it for somewhat dynamic allocations?</p><p>Because stack size is limited (on linux about 8MB, but don't rely on that)</p><p>How can you hit this limit?</p><ul><li>By recursion - lots of nested stacks.</li><li>By running over the extents of a buffer (in C)</li></ul><p>See example: stackoverflow.</p></div><div class="step step-level-1" step="82" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="131200" data-y="0" data-z="0"><h1 id="gc-pressure-locality-and-memory-management">GC pressure, locality and memory management</h1><p>Prefer this:</p><pre class="highlight code go"><span class="nx">m</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nb">make</span><span class="p">(</span><span class="kd">map</span><span class="p">[</span><span class="kt">string</span><span class="p">]</span><span class="nx">someStruct</span><span class="p">)</span></pre><p>over:</p><pre class="highlight code go"><span class="nx">m</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nb">make</span><span class="p">(</span><span class="kd">map</span><span class="p">[</span><span class="kt">string</span><span class="p">]</span><span class="o">*</span><span class="nx">someStruct</span><span class="p">)</span></pre><ul><li>Way less memory in total</li><li>Data is packed together (good for caching!)</li><li>Less work for the GC and the allocator to do</li><li>Pointers give you more potential to fuck up.</li></ul><pre class="highlight code bash">noptr  <span class="m">577</span>.7 ns/op   <span class="m">336</span> B/op             <span class="m">2</span> allocs/op
ptr    <span class="m">761</span>.4 ns/op   <span class="m">384</span> B/op            <span class="m">10</span> allocs/op

<span class="o">(</span>The <span class="m">10</span> will increase with input! Longer runs will cause more GC <span class="k">for</span> the ptr <span class="k">case</span><span class="o">)</span></pre></div><div class="step step-level-1" step="83" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="132800" data-y="0" data-z="0"><h1 id="virtual-memory">Virtual memory</h1><img src="images/virtual_memory.svg.png"></img><ul><li>The physical memory of a system is splitted up into 4k pages.</li><li>Each process maintains a virtual memory mapping table, mapping
from the virtual range of memory to physical memory.</li><li>Address translation is handled efficiently by the MMU</li></ul><div class="notes"><p>Wait, those addresses I saw earlier... are those the addrs in RAM?
Hopefully not, because otherwise you could somehow find out where the OpenSSH
server lives in memory and steal it's keys. For security reasons it must look
for each process like he's completely alone on the system. What you saw above
are virtual memory addresses and they stay very similar on each run.</p><p>The concept how this achieved is called "virtual memory" and it's probably one of
the more clever things we did in computer science.</p></div></div><div class="step step-level-1" step="84" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="134400" data-y="0" data-z="0"><h1 id="virtual-memory-implementation">Virtual memory implementation</h1><ul><li>Each process has a list of page tables mapping virtual to physical memory ("page table")</li><li>On process start this table is filled with a few default kilobytes of mapped pages
(the first few pages are not mapped, so dereferencing a NULL pointer will always crash)</li><li>When the program first accesses those addresses the CPU will generate a page fault, indicating
that there is no such mapping. The OS receives this and will find a free physical page, map
it and retry execution. If another page fault occurs the OS will kill the process with SIGSEGV.</li></ul></div><div class="step step-level-1" step="85" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="136000" data-y="0" data-z="0"><h1 id="virtual-memory-advantages">Virtual memory advantages</h1><ul><li>Pages can be mapped only once it is needed (CoW)</li><li>Processes can share the same page for shared memory.</li><li>Pages do not need to be mapped to physical memory: Disk, DMA or even network is possible!</li><li>Processes are isolated from each other.</li><li>Processes consume only as much physical ("residual") memory as really needed.</li><li>Programs get easier to write because they can just assume that the memory is not fragmented.</li><li>Pages can be swaped by the OS without the process even noticing (Swapping)</li><li>The kernel can give away more memory than there is on the system (overcommiting)</li><li>Pages with the same content can be deduplicated</li></ul></div><div class="step step-level-1" step="86" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="137600" data-y="0" data-z="0"><h1 id="residual-vs-virtual-memory-usage">Residual vs virtual memory usage</h1><p>TODO: look a  htop and free</p></div><div class="step step-level-1" step="87" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="139200" data-y="0" data-z="0"><h1 id="quick-peak-memory-measurement">Quick peak memory measurement</h1><pre class="highlight code">/usr/bin/time -v &lt;command&gt;</pre></div><div class="step step-level-1" step="88" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="140800" data-y="0" data-z="0"><h1 id="malloc">malloc()</h1><p><tt>`c
char *one_kb_buf = malloc(1024 * sizeof(char));
/* use one_kb_buf somehow */
free(onone_kb_buf);
`</tt></p><ul><li><tt>malloc</tt> itself is implemented in user space, not by the kernel.</li><li>Think of it as some sort of memory pool management library (implemente by glibc)</li><li>When <tt>malloc</tt> runs out of space it asks the kernel for more space by using either the <tt>sbrk</tt> call (for small allocations)
or <tt>mmap</tt> (for big allocations). Allocations have as multiple of PAGE_SIZE (4KB)</li><li><tt>sbrk</tt> is a system call that moves the <em>program break</em> of a program upwards (or downwards) by a certain amount.</li><li>The new space is then managed by <tt>malloc</tt>. Each allocation gets added a header by <tt>malloc</tt> at the start (~10 byte),
so many small allocations are wasteful.</li><li>Memory that is not directly used is kept in a freelist. Only once the freelist is empty, new memory is fetched
from the operating system.</li><li>On <tt>free</tt> a memory block is added back to the freelist.</li><li><tt>malloc</tt> is optimized for the usecase of allocating many (typically) small sized objects with minimal fragmentation.
Since every program tends to have different needs it makes sense to do this in userspace.</li><li>Go uses a similar implementation, but is more sophisticated. Main difference:
it keeps pre-allocated arenas for differently sized objects. i.e. 4, 8, 16,
32, 64 and so on.</li></ul><div class="notes"><p>What the fuck happens on allocation?</p><p>In C you have to explicitly what <tt>Go</tt> does in the background for you:</p></div></div><div class="step step-level-1" step="89" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="142400" data-y="0" data-z="0"><h1 id="swapping">Swapping</h1><pre class="highlight code bash">$ dd <span class="k">if</span><span class="o">=</span>/dev/zero <span class="nv">of</span><span class="o">=</span>swapfile <span class="nv">count</span><span class="o">=</span><span class="m">1024</span> <span class="nv">bs</span><span class="o">=</span>1M
$ swapon ./swapfile</pre><pre class="highlight code bash">$ cat /proc/sys/vm/swappiness
<span class="o">(</span>value between <span class="m">0</span>-100<span class="o">)</span>
<span class="nv">0</span> <span class="o">=</span> only swap <span class="k">if</span> OOM would hit otherwise.
<span class="nv">100</span> <span class="o">=</span> swap everything not actively used.</pre><div class="notes"><p>Linux can use swap space as second-prio memory if main memory runs low.
Swap is already used before memory goes low. Inactive processes and stale IO pages
get put to swap so that memory management can make use of that space to provide less
fragmented memory regions.</p><p>How aggressive this happens can be set using vm.swappiness. A value between</p><p>Rules:</p><ul><li>If you want to hibernate (i.e. powerless suspend) then you need as much swap as RAM.</li><li>Otherwise about half of RAM is a good rule of thumb.</li><li>Systems that rely on low latency (i.e. anything that goes in the direction of realtime) should not swap.</li></ul></div></div><div class="step step-level-1" step="90" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="144000" data-y="0" data-z="0"><h1 id="the-oom-killer">The OOM Killer</h1><ul><li>Kicks in if sytem almost completely ran out of RAM.</li><li>Selects a process based on a scoring system and kills it.</li><li>Processes can be given a priority in advance.</li></ul><div class="notes"><ul><li>Last resort mechanism.</li><li>Reports in dmesg.</li><li>Sometimes comes too late and is not able to operate anymore.</li></ul><p>Alternatives:</p><ul><li>earlyoom</li><li>systemd-oomd</li></ul><p>Userspace-Daemons that monitor memory usage and kill processes
in a very configurable way. Well suited for server systems.</p></div></div><div class="step step-level-1" step="91" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="145600" data-y="0" data-z="0"><h1 id="mmap"><tt>mmap()</tt></h1><ul><li>Can map files (among other things) to a processes' memory.</li><li>File contents are loaded</li></ul><div class="notes"><p>Maybe one of the most mysterious system features we have on Linux.</p><p>Typical open/read/write/close APIs see files as streams.
With mmap() we can handle files as arrays and the memory needed for
this can be shared by several processes!</p><p>Great for implementing databases
or implementing random access to a big file (ex: reading every tenth byte of a file)</p></div><h1 id="mmap-for-databases"><tt>mmap</tt> for databases</h1><p>Short answer: Don't. Not enough control. Random order + writes hurt mmap.</p><p>Long answer: <a href="https://db.cs.cmu.edu/mmap-cidr2022">https://db.cs.cmu.edu/mmap-cidr2022</a></p><p>Good mmap use cases:</p><ul><li>Reading large files (+ telling the OS how to read)</li><li>Sharing the file data with several processes in a very efficient way.</li><li>Zero copy during reading.</li><li>Ease-of-use. No buffers, no file handles.</li></ul></div><div class="step step-level-1" step="92" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="147200" data-y="0" data-z="0"><h1 id="madvise-and-fadvise"><tt>madvise()</tt> and <tt>fadvise()</tt></h1><ul><li>You can give tips to the kernel.</li><li>When you know that you need a certain memory page soon,
then you can do <tt>madvise(addr, 4096, MADV_WILLNEED)</tt>.</li><li>With <tt>fadvise()</tt> you can do the same for files.</li></ul><div class="notes"><p>This is greatly notice-able with file I/O!</p><p>Caveat: Complex orders (like tree traversal) cannot be requested
by userspace.</p></div><p>TODO: epoll / select and sorts. Maybe later in parallel programming?
TODO: closing files
TODO: Agenda</p><h1 id="latency-and-throughput">Latency and Throughput</h1><p>Metaphor: a water pipe.</p><p>Latency = time until the first drop of water arrives
Throughput = How many liters of water we can pipe through</p><p>Examples:</p><ul><li>SSDs: Low latency, high throughput.</li><li>HDDs: Medium latency, high throughput.</li><li>Network: Often high latency, any kind of throughput.</li><li>USB Stick over carrier pigeon: High latency, high throughput.</li></ul></div><div class="step step-level-1" step="93" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="148800" data-y="0" data-z="0"><h1 id="hardware-hdds">Hardware: HDDs</h1><img src="images/hdd.jpg"></img><ul><li>Rotational, stacked disks.</li><li>Reading head needs to seek to the right position.</li><li>Mechanical system, bad at low/high temperature or moving systems.</li><li>Dying technology, but battled tested &amp; still widely used.</li></ul><div class="notes"><p>Big advantage: You could debug issues with too many seeks by audio!</p><p>Also: did you ever try to shake your old laptop with a hdd while reading?</p></div></div><div class="step step-level-1" step="94" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="150400" data-y="0" data-z="0"><h1 id="hardware-sdds">Hardware: SDDs</h1><img src="images/ssd.jpg"></img><ul><li>Flash technology</li><li>No expensive seek necessary.</li><li>Limited number of write cycles.</li><li>Becoming cheaper and better every year.</li></ul><div class="notes"><p>Write software for SSDs. There were some crazy tricks like FIEMAP to make
applications re-order their reads in the order of how they are placed on disk.
(Huge speedup on HDD, small speedup on SSD)</p></div></div><div class="step step-level-1" step="95" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="152000" data-y="0" data-z="0"><h1 id="hardware-write-amplification">Hardware: Write amplification</h1><img src="images/ssd_write_amplification.png"></img><div class="notes"><p>Source: <a href="http://databasearchitects.blogspot.com/2021/06/what-every-programmer-should-know-about.html?m=1">http://databasearchitects.blogspot.com/2021/06/what-every-programmer-should-know-about.html?m=1</a></p><p>SSDs are divided into blocks (seveal MB), which are divided into pages (often 4K).
Pages cannot be erased, only blocks can be. Updates of a pages are written to new blocks.
If space runs out, old blocks with many stale pages are erased and can be re-used.
The number of physical writes is therefore higher than the number of logical writes.
The more space is used, the higher the write amplication factor though.</p><p>What we can do about it: Buy bigger SSDs than you need. Also avoid rewriting pages if possible.
Secret: SSD have some spare space to keep working they don't tell you about.</p><p>Also enable TRIM support if your OS did not yet, but nowadways always enabled.
This makes it possible for the OS to tell the SSD additional blocks that are not needed anymore.</p></div></div><div class="step step-level-1" step="96" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="153600" data-y="0" data-z="0"><h1 id="virtual-file-system">Virtual File System</h1><img src="images/vfs.webp"></img><div class="notes"><p>Below device drivers: hardware controllers - beyond this talk.
They can also re-order writes and are mostly concerned with durability,
i.e. a SSD controller will try to distribute the blocks he used to make sure
they have a similar amount of write cycles.</p></div></div><div class="step step-level-1" step="97" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="155200" data-y="0" data-z="0"><h1 id="how-do-syscalls-work">How do syscalls work?</h1><p>Only method of userspace to talk to kernel.
How to call is interrupt specific.</p><pre class="highlight code asm"><span class="nf">movl</span><span class="w"> </span><span class="no">$4</span><span class="p">,</span><span class="w"> </span><span class="nv">%eax</span><span class="w">   </span><span class="c1">; use the `write` system call
</span><span class="nf">movl</span><span class="w"> </span><span class="no">$1</span><span class="p">,</span><span class="w"> </span><span class="nv">%ebx</span><span class="w">   </span><span class="c1">; write to stdout
</span><span class="nf">movl</span><span class="w"> </span><span class="err">"</span><span class="no">Hello</span><span class="w"> </span><span class="no">World</span><span class="p">!</span><span class="err">\</span><span class="no">n</span><span class="err">"</span><span class="p">,</span><span class="w"> </span><span class="nv">%ecx</span><span class="w"> </span><span class="c1">; use string "Hello World"
</span><span class="nf">movl</span><span class="w"> </span><span class="no">$12</span><span class="p">,</span><span class="w"> </span><span class="nv">%edx</span><span class="w">  </span><span class="c1">; write 12 characters
</span><span class="nf">syscall</span><span class="w">         </span><span class="c1">; make system call via special instruction
</span><span class="w">                </span><span class="c1">; The return code is in the eax register.</span></pre><div class="notes"><p>The syscall instruction performs a context switch.</p><p>This means the current state of the process (i.e. the state of all registers
in the CPU) is saved away, so it can be restored later. Once done, the kernel
sets the register to its needs, does whatever is required to serve the system call.
When finished, the process state is restored and execution continues.</p><p>Context switches also happen when you're not calling any syscalls.
Simply when the scheduler decide this process is done with execution.</p></div></div><div class="step step-level-1" step="98" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="156800" data-y="0" data-z="0"><h1 id="typical-syscalls">Typical syscalls</h1><ul><li>IO: <tt>read</tt>, <tt>write</tt>, <tt>close</tt></li><li>Files: <tt>stat</tt>, <tt>chmod</tt>, <tt>mkdir</tt></li><li>Memory: <tt>sbrk</tt>, <tt>mmap</tt></li><li>Processes: <tt>fork</tt>, <tt>kill</tt>, <tt>wait</tt></li><li>Network: <tt>listen</tt>, <tt>connect</tt>, <tt>epoll</tt></li><li>Mysterious: <tt>ioctl</tt>, <tt>chroot</tt>, <tt>mount</tt></li></ul><div class="notes"><p>Luckily for us, glibc and Go provide us nice names and interfaces to make those system calls.
They usually provide thin wrappers that also do some basic error checking. Watch out: <tt>fread</tt>
is doing buffering in userspace!</p><p>Can anyone think of another syscall not in the list above? exit! chdir, mkdir, chmod, ...</p></div></div><div class="step step-level-1" step="99" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="158400" data-y="0" data-z="0"><h1 id="syscalls-are-expensive">Syscalls are expensive</h1><pre class="highlight code bash">$ dd <span class="k">if</span><span class="o">=</span>/dev/urandom <span class="nv">of</span><span class="o">=</span>./big-file <span class="nv">bs</span><span class="o">=</span>1M <span class="nv">count</span><span class="o">=</span><span class="m">1024</span>
$ dd <span class="k">if</span><span class="o">=</span>big-file <span class="nv">of</span><span class="o">=</span>/dev/null <span class="nv">bs</span><span class="o">=</span>1b
<span class="m">4</span>,07281 s, <span class="m">264</span> MB/s
$ dd <span class="k">if</span><span class="o">=</span>big-file <span class="nv">of</span><span class="o">=</span>/dev/null <span class="nv">bs</span><span class="o">=</span>32b
<span class="m">0</span>,255229 s, <span class="m">4</span>,2 GB/s
$ dd <span class="k">if</span><span class="o">=</span>big-file <span class="nv">of</span><span class="o">=</span>/dev/null <span class="nv">bs</span><span class="o">=</span>1024b
<span class="m">0</span>,136717 s, <span class="m">7</span>,9 GB/s
$ dd <span class="k">if</span><span class="o">=</span>big-file <span class="nv">of</span><span class="o">=</span>/dev/null <span class="nv">bs</span><span class="o">=</span>32M
<span class="m">0</span>,206027 s, <span class="m">5</span>,2 GB/s</pre><p>Good buffer sizes: 1K - 32k</p><div class="notes"><p>Many syscalls vs a few big ones.</p><p>Try to reduce the number of syscalls,
but too big buffers hurt too.</p></div></div><div class="step step-level-1" step="100" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="160000" data-y="0" data-z="0"><h1 id="making-syscalls-visible">Making syscalls visible</h1><pre class="highlight code bash">$ strace ls /tmp</pre><div class="notes"><p>Insanely useful tool to debug hanging tools
or tools that crash without a proper error message.</p><p>Usually the last syscall they do give a hint.</p></div></div><div class="step step-level-1" step="101" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="161600" data-y="0" data-z="0"><h1 id="terminology-inode">Terminology: inode?</h1><p>The unique id of a file.
Several paths can have the same Inode.</p><pre class="highlight code bash">$ <span class="nb">echo</span> hello-world &gt; file
$ stat --format <span class="s1">'%i'</span> file
<span class="m">1883</span>
$ ln file hardlink
$ stat --format <span class="s1">'%i'</span> hardlink
<span class="m">1883</span></pre><div class="notes"><p>There's a theoretical maximum of inodes per filesystem.
Most filesystems prefer bigger files, since every file
lookup has to lookup the right one in a big set of inodes.</p></div></div><div class="step step-level-1" step="102" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="163200" data-y="0" data-z="0"><h1 id="page-cache">Page cache</h1><ul><li>All I/O access is cached using the page cache (dir + inode)</li><li>Free pages are used to store recently accessed file contents.</li><li>Performance impact can be huge.</li></ul></div><div class="step step-level-1" step="103" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="164800" data-y="0" data-z="0"><h1 id="clearing-the-cache">Clearing the cache</h1><p>For I/O benchmarks always clear all caches:</p><pre class="highlight code bash">sync<span class="p">;</span> <span class="nb">echo</span> <span class="m">3</span> <span class="p">|</span> sudo tee /proc/sys/vm/drop_caches</pre><div class="notes"><p>Example: code/io_cache</p></div></div><div class="step step-level-1" step="104" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="166400" data-y="0" data-z="0"><h1 id="caveat-writes-are-buffered">Caveat: Writes are buffered!</h1><p>Make sure all file data is written to the hardware:</p><pre class="highlight code bash">sync</pre><pre class="highlight code c"><span class="n">fsync</span><span class="p">(</span><span class="n">fd</span><span class="p">)</span></pre><div class="notes"><p>That's why we have the sync command before the drop_cache command.</p></div></div><div class="step step-level-1" step="105" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="168000" data-y="0" data-z="0"><h1 id="alternative-to-fsync">Alternative to <tt>fsync()</tt></h1><pre class="highlight code bash"><span class="c1"># Move is atomic!
</span>$ cp /src/bigfile /dst/bigfile.tmp
$ mv /dst/bigfile.tmp /dst/bigfile</pre><div class="notes"><p>This only works obviously if you're not constantly updating the file,
i.e. for files that are written just once.</p></div></div><div class="step step-level-1" step="106" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="169600" data-y="0" data-z="0"><h1 id="typical-read-i-o">Typical read I/O</h1><pre class="highlight code c"><span class="kt">char</span><span class="w"> </span><span class="n">buf</span><span class="p">[</span><span class="mi">1024</span><span class="p">];</span><span class="w">
</span><span class="kt">int</span><span class="w"> </span><span class="n">fd</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">open</span><span class="p">(</span><span class="s">"/some/path"</span><span class="p">,</span><span class="w"> </span><span class="n">O_CREAT</span><span class="o">|</span><span class="n">O_RDONLY</span><span class="o">|</span><span class="n">O_TRUNC</span><span class="p">);</span><span class="w">
</span><span class="kt">size_t</span><span class="w"> </span><span class="n">bytes_read</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w">
</span><span class="k">while</span><span class="p">((</span><span class="n">bytes_read</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">read</span><span class="p">(</span><span class="n">fd</span><span class="p">,</span><span class="w"> </span><span class="n">buf</span><span class="p">,</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="n">buf</span><span class="p">)))</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="cm">/* do something with buf[:bytes_read] */</span><span class="w">
</span><span class="p">}</span><span class="w">
</span><span class="n">close</span><span class="p">(</span><span class="n">fd</span><span class="p">);</span></pre></div><div class="step step-level-1" step="107" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="171200" data-y="0" data-z="0"><h1 id="typical-write-i-o">Typical write I/O</h1><pre class="highlight code c"><span class="kt">char</span><span class="w"> </span><span class="n">buf</span><span class="p">[</span><span class="mi">1024</span><span class="p">];</span><span class="w">
</span><span class="kt">size_t</span><span class="w"> </span><span class="n">bytes_in_buf</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w">
</span><span class="kt">int</span><span class="w"> </span><span class="n">fd</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">open</span><span class="p">(</span><span class="s">"/some/path"</span><span class="p">,</span><span class="w"> </span><span class="n">O_CREAT</span><span class="o">|</span><span class="n">O_WRONLY</span><span class="o">|</span><span class="n">O_TRUNC</span><span class="p">);</span><span class="w">
</span><span class="k">do</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="cm">/* fill buf somehow with data you'd like to write,
     * set bytes_in_buf accordingly.
     */</span><span class="w">
</span><span class="p">}</span><span class="w"> </span><span class="k">while</span><span class="p">(</span><span class="n">write</span><span class="p">(</span><span class="n">fd</span><span class="p">,</span><span class="w"> </span><span class="n">buf</span><span class="p">,</span><span class="w"> </span><span class="n">bytes_in_buf</span><span class="p">)</span><span class="w"> </span><span class="o">&gt;=</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w">
</span><span class="n">fsync</span><span class="p">(</span><span class="n">fd</span><span class="p">);</span><span class="w">
</span><span class="n">close</span><span class="p">(</span><span class="n">fd</span><span class="p">);</span></pre><div class="notes"><p>There is a bug here:</p><p>write() returns the number of written bytes.
It might be less than bytes_in_buf and this is not counted as an error.
The write call might have simply been interrupted and we expect that it is
called another time with the remaining data.</p><p>Also: Does the application that the data is immediately for read()?
Answer: nope. You have to use fsync()</p><p>Also please note: There is some error handling missing here.</p></div></div><div class="step step-level-1" step="108" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="172800" data-y="0" data-z="0"><h1 id="fixed-write-version">Fixed write version</h1><pre class="highlight code c"><span class="cm">/* ... */</span><span class="w">
</span><span class="kt">char</span><span class="w"> </span><span class="o">*</span><span class="n">buf_ptr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">buf</span><span class="p">;</span><span class="w">
</span><span class="k">while</span><span class="p">(</span><span class="n">bytes_in_buf</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
   </span><span class="kt">size_t</span><span class="w"> </span><span class="n">written</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">write</span><span class="p">(</span><span class="n">fd</span><span class="p">,</span><span class="w"> </span><span class="n">buf_ptr</span><span class="p">,</span><span class="w"> </span><span class="n">bytes_in_buf</span><span class="p">);</span><span class="w">
   </span><span class="n">bytes_in_buf</span><span class="w"> </span><span class="o">-=</span><span class="w"> </span><span class="n">written</span><span class="p">;</span><span class="w">
   </span><span class="n">buf_ptr</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">written</span><span class="p">;</span><span class="w">
   </span><span class="k">if</span><span class="p">(</span><span class="n">errno</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
       </span><span class="k">return</span><span class="p">;</span><span class="w">
   </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">
</span><span class="cm">/* ... */</span></pre><div class="notes"><p>Many write utils actually handle this for you.
But io.Writer() behaves the same! Depending on the underlying
file type and system this might be rare but is a real thing.</p></div></div><div class="step step-level-1" step="109" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="174400" data-y="0" data-z="0"><h1 id="what-about-fread">What about <tt>fread()</tt>?</h1><p>A weird accident in history.</p><p><strong>Usecases:</strong></p><ul><li>You need to read byte by byte.</li><li>You need to unread some bytes frequently.</li><li>You need to read easily line by line.</li></ul><p>Otherwise: Do not use.</p><div class="notes"><p>Userspace buffered functions. No real advantage, but limiting and confusing API.
Has some extra features like printf-style formatting.</p><p>In Go the normal read/write is using the syscall directly,
bufio is roughly equivalent to f{read,write} etc.
fsync() is a sycasll, not part of that.</p></div></div><div class="step step-level-1" step="110" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="176000" data-y="0" data-z="0"><h1 id="detour-filesystems">Detour: Filesystems</h1><p>Performance depends a little on filesystem:</p><ul><li>ext2/3/4: good, stable &amp; fast choice.</li><li>fat8/16/32: simple, but legacy, do not use.</li><li>NTFS: slow and only for compatibility.</li><li>XFS: good with big files.</li><li>btrfs: feature-rich, can do CoW &amp; snapshots.</li><li>ZFS: highly scalable and very complex.</li><li>sshfs: remote access over FUSE</li></ul><div class="notes"><p>Actual implementation of read/write/etc. for a single
filesystem like FAT, ext4, btrfs. There are different ways
to layout and maintain data on disk, depending on your use case.</p><p>Syscalls all work the same, but some filesystems have
better performance regarding writes/reads/syncs or
are more targeted at large files or many files.</p><p>Most differences are admin related (i.e. integrity, backups,
snapshots etc.)</p></div></div><div class="step step-level-1" step="111" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="177600" data-y="0" data-z="0"><h1 id="detour-fragmentation">Detour: Fragmentation</h1><p>The problem that file content is distributed over many blocks.</p><pre class="highlight code">Windows sucks but this term stuck in our heads.

ext4 does not require fragmentation.</pre></div><div class="step step-level-1" step="112" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="179200" data-y="0" data-z="0"><h1 id="detour-fuse">Detour: FUSE</h1><img src="images/fuse.png"></img></div><div class="step step-level-1" step="113" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="180800" data-y="0" data-z="0"><h1 id="a-note-on-seeking">A note on seeking</h1><ul><li>Rotational disks have only one reading head.</li><li>They re-order read requests</li><li>This can increase latency!</li><li>SSDs use</li></ul></div><div class="step step-level-1" step="114" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="182400" data-y="0" data-z="0"><h1 id="i-o-improving-performance">I/O improving performance</h1><ul><li>Avoid I/O.</li><li>Use a sane buffer size.</li><li>Use append only data for writing.</li><li>Batch writes as they evict caches.</li><li>Prefer few big files over many small files.</li><li>Avoid directories with high amount of files (<tt>git</tt>)</li><li>For modifying big files use mmap.</li><li>Buy faster hardware.</li></ul></div><div class="step step-level-1" step="115" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="184000" data-y="0" data-z="0"><h1 id="i-o-improving-performance-2">I/O improving performance #2</h1><ul><li>Use a different I/O scheduler (<tt>none</tt>).</li><li>Use a different filesystem (<tt>tmpfs</tt>)</li><li>Leverage the page cache and trust the OS</li><li>Use zero-copy techniques: <tt>sendfile</tt>, <tt>splice</tt></li><li>Not crazy: Use DMA if possible (hardware dependent)</li><li>Slightly crazy: fadvise() if you need prefetch</li><li>Maybe crazy: use O_DIRECT</li><li>Likely crazy: skip fsync()</li><li>Definitely crazy: FIEMAP</li></ul></div><div class="step step-level-1" step="116" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="185600" data-y="0" data-z="0"><h1 id="i-o-scheduler">I/O scheduler</h1><p>Re-orders read and write requests for performance.</p><ul><li><tt>none</tt>: Does no reordering.</li><li><tt>bfq</tt>: Complex, designed for desktops.</li><li><tt>mq-deadline</tt>, <tt>kyber</tt>: Simpler, good allround schedulers.</li></ul><div class="notes"><p>In the age of SSDs we can use dumber schedulers.
In the age of HDDs schedulers were vital.</p></div></div><div class="step step-level-1" step="117" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="187200" data-y="0" data-z="0"><h1 id="ionice"><tt>ionice</tt></h1><pre class="highlight code c"><span class="n">$</span><span class="w"> </span><span class="n">ionice</span><span class="w"> </span><span class="o">-</span><span class="n">c</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="o">-</span><span class="n">n</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="o">&lt;</span><span class="n">some</span><span class="o">-</span><span class="n">pid</span><span class="o">&gt;</span></pre><ul><li>Default level is 4. Lower is higher.</li></ul><div class="notes"><p>Well, you can probably guess what it does.</p></div></div><div class="step step-level-1" step="118" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="188800" data-y="0" data-z="0"><h1 id="reduce-number-of-copies">Reduce number of copies</h1><ul><li>Do not copy buffers in your program too often</li><li>You can use <tt>readv</tt> to splice existing buffers to one.</li><li>Use hardlinks if possible</li><li>Use CoW reflinks if possible.</li><li><tt>sendfile()</tt> to copy files to Network.</li><li><tt>copy_file_range()</tt> to copy between files.</li></ul></div><div class="step step-level-1" step="119" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="190400" data-y="0" data-z="0"><h1 id="why-is-cp-faster">Why is cp faster?</h1><pre class="highlight code go"><span class="kn">package</span><span class="w"> </span><span class="nx">main</span><span class="w">

</span><span class="kn">import</span><span class="p">(</span><span class="w">
    </span><span class="s">"os"</span><span class="w">
    </span><span class="s">"io"</span><span class="w">
</span><span class="p">)</span><span class="w">

</span><span class="kd">func</span><span class="w"> </span><span class="nx">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nx">src</span><span class="p">,</span><span class="w"> </span><span class="nx">_</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">os</span><span class="p">.</span><span class="nx">Open</span><span class="p">(</span><span class="nx">os</span><span class="p">.</span><span class="nx">Args</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="w">
    </span><span class="nx">dst</span><span class="p">,</span><span class="w"> </span><span class="nx">_</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">os</span><span class="p">.</span><span class="nx">Create</span><span class="p">(</span><span class="nx">os</span><span class="p">.</span><span class="nx">Args</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span><span class="w">
    </span><span class="nx">io</span><span class="p">.</span><span class="nx">Copy</span><span class="p">(</span><span class="nx">dst</span><span class="p">,</span><span class="w"> </span><span class="nx">src</span><span class="p">)</span><span class="w">
</span><span class="p">}</span></pre><div class="notes"><p>cp is not faster because it copies data faster, but
because it avoids copies to user space by using specialized calls like:</p><ul><li>ioctl(5, BTRFS_IOC_CLONE or FICLONE, 4) = 0 (on btrfs)</li><li>copy_file_range() - performs in-kernel copy, sometimes even using DMA</li></ul><p>Find out using strace cp src dst.
If no trick is possible it falls back to normal buffered read/write.</p></div><h1 id="parallel-programming">Parallel programming</h1><p>The art of distributing work so that we maximize
the number of used CPU cores with minimal overhead.</p><div class="notes"><p>There are two ways to be comfortable writing parallel code:</p><ul><li>Being very experienced and having made a lot of mistakes.</li><li>Being fearless and not be aware of the possible problems.</li></ul></div></div><div class="step step-level-1" step="120" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="192000" data-y="0" data-z="0"><h1 id="concurrent-vs-parallel">Concurrent vs Parallel</h1><p>Please define it.</p><div class="notes"><p>Concurrent = execution might be interrupted at an time.
Parallel = several instructions get executed at the same time.</p></div></div><div class="step step-level-1" step="121" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="193600" data-y="0" data-z="0"><h1 id="what-are-processes">What are processes?</h1><ul><li>Processes are a lightweight way to schedule work over all available cpu cores.</li><li>Processes get started by <tt>fork()</tt> (except the first one...)</li><li>Processes focus on memory isolation - memory can only be shared via IPC (unix sockets, pipes, shared memory, ...)</li><li>Processes have their own ID (PID)</li></ul></div><div class="step step-level-1" step="122" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="195200" data-y="0" data-z="0"><h1 id="what-are-threads">What are threads?</h1><ul><li>Threads are lightweight processes (huh?)</li><li>Threads get started by <tt>pthread_create()</tt></li><li>Threads share the heap of the process but have each their own stack</li><li>Threads have their own ID (TID)</li></ul><div class="notes"><p>Threads are scheduled like processes by the kernel. No real difference is made between
processes and threads in that regard.</p></div></div><div class="step step-level-1" step="123" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="196800" data-y="0" data-z="0"><h1 id="what-are-coroutines">What are coroutines?</h1><ul><li>Coroutines are lightweight threads (oh come on)</li><li>Coroutines are implemented completely in user space using a scheduler</li><li>Every detail depends on the individual programming languages' implementation</li><li>Goroutines are one example of a coroutine implementation. Fibers are another often used term.</li></ul><div class="notes"><p>Good example of software evolution. Old concepts are never cleaned up. Just new concepts
get added that enhance (in the best case) the old concepts. I call this toilet paper development:
If it stinks, put another layer over it.</p></div></div><div class="step step-level-1" step="124" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="198400" data-y="0" data-z="0"><h1 id="summary">Summary</h1><img src="images/time_sharing_threads.png"></img></div><div class="step step-level-1" step="125" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="200000" data-y="0" data-z="0"><h1 id="synchronization-primitives">Synchronization primitives</h1><p>Threads &amp; coroutines need to be in sync.</p><p>Big toolset of possible ways to do so.</p><div class="notes"><p>If you use processes you obviously need to synchronize too sometimes.
Potential ways can be to use filesystem locks or mlock() on shared memory.</p><p>If not used they can be a hell to debug. Debuggers won't work and prints
might change timings so deadlocks or race conditions might not always occur.</p></div></div><div class="step step-level-1" step="126" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="201600" data-y="0" data-z="0"><h1 id="primitive-sleep">Primitive: Sleep</h1><p>Just kidding. Don't!</p></div><div class="step step-level-1" step="127" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="203200" data-y="0" data-z="0"><h1 id="primitive-semaphor">Primitive: Semaphor</h1><div class="notes"><p>A bouncer before a club.
It's corona times and he knows that only 10 people may be in the club (sad times)
He counts up when he let's somebody in and counts down when someone leaves.
If the club is full new visitors have to wait</p></div></div><div class="step step-level-1" step="128" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="204800" data-y="0" data-z="0"><h1 id="primitive-mutex">Primitive: Mutex</h1><p>A binary semaphore.</p></div><div class="step step-level-1" step="129" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="206400" data-y="0" data-z="0"><h1 id="primitive-barrier-latch-wait-group">Primitive: Barrier (latch, wait group)</h1><p>An inverted semaphore</p><div class="notes"><p>All threads have to arrive a certain point before any can continue.</p></div><p>Dining Philosopher's problem as intro to synchronisation -&gt; explain deadlock scenarios and how to debug them.</p></div><div class="step step-level-1" step="130" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="208000" data-y="0" data-z="0"><h1 id="primitive-condition-variable">Primitive: Condition variable</h1><pre class="highlight code">c.L.Lock()
for !condition() {
    c.Wait()
}
// condition changed, do something.
c.L.Unlock()</pre><pre class="highlight code">c.L.Lock()
changeCondition()
c.Broadcast() // or c.Signal() for a single go routine.
c.L.Unlock()</pre><div class="notes"><ul><li>Broadcast or notify a single thread.</li><li>Seldomly used in Go, but has their use cases.</li><li>Use case: waiting on a condition without busy polling
and where the use of channels would be awkward (channels
suck if you have to wake up several go routines, as messages
are consumed)</li></ul></div></div><div class="step step-level-1" step="131" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="209600" data-y="0" data-z="0"><h1 id="primitive-atomics">Primitive: Atomics</h1><ul><li>Store</li><li>Load</li><li>Increment</li><li>Swap</li><li>Compare-And-Swap</li></ul><div class="notes"><p>Several atomic operations are not atomic of course!</p></div></div><div class="step step-level-1" step="132" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="211200" data-y="0" data-z="0"><h1 id="primitive-channel">Primitive: Channel</h1><pre class="highlight code go"><span class="c1">// buffered channel with 10 items</span><span class="w">
</span><span class="nx">c</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nb">make</span><span class="p">(</span><span class="kd">chan</span><span class="w"> </span><span class="kt">int</span><span class="p">,</span><span class="w"> </span><span class="mi">10</span><span class="p">)</span><span class="w">
</span><span class="nx">c</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="c1">// send</span><span class="w">
</span><span class="nx">fmt</span><span class="p">.</span><span class="nx">Println</span><span class="p">(</span><span class="o">&lt;-</span><span class="nx">c</span><span class="p">)</span><span class="w"> </span><span class="c1">// recv</span></pre><div class="notes"><p>Might be called prioq or something in other languages.
Basically a slice or linked list protected with a mutex.</p><p>Channels can be buffered or unbuffered:</p><ul><li>unbuffered: reads and writes block until the other end is ready.</li><li>buffer: blocks only when channel is full.</li></ul><p>Channels can be closed, which can be used as signal to stop.
A send to a closed channel panics.
A recv from a closed channel blocks forever.</p><p>We will see channels later in action.</p></div></div><div class="step step-level-1" step="133" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="212800" data-y="0" data-z="0"><h1 id="pattern-pool">Pattern: Pool</h1><p>Classical producer-consumer problem.</p><ol><li>Start a limited number of goroutines.</li><li>Pass each a shared channel.</li><li>Let each goroutine receive on the channel.</li><li>Producer sends jobs over the channel.</li><li>Tasks are distributed over the go routines.</li></ol><div class="notes"><p>Pools often use a queue (i.e. a channel or some other prioq). I.e. you can
produce more to some point than you consume. Can be a problem.</p></div></div><div class="step step-level-1" step="134" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="214400" data-y="0" data-z="0"><h1 id="pattern-limiter">Pattern: Limiter</h1><pre class="highlight code go"><span class="nx">tokens</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nb">make</span><span class="p">(</span><span class="kd">chan</span><span class="w"> </span><span class="kt">bool</span><span class="p">,</span><span class="w"> </span><span class="mi">10</span><span class="p">)</span><span class="w">
</span><span class="k">for</span><span class="w"> </span><span class="nx">i</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="nx">i</span><span class="w"> </span><span class="p">&lt;</span><span class="w"> </span><span class="nb">cap</span><span class="p">(</span><span class="nx">tokens</span><span class="p">);</span><span class="w"> </span><span class="nx">i</span><span class="o">++</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nx">tokens</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nx">i</span><span class="w">
</span><span class="p">}</span><span class="w">
</span><span class="k">for</span><span class="w"> </span><span class="nx">_</span><span class="p">,</span><span class="w"> </span><span class="nx">job</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="k">range</span><span class="w"> </span><span class="nx">jobs</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="o">&lt;-</span><span class="nx">tokens</span><span class="w">
    </span><span class="k">go</span><span class="w"> </span><span class="kd">func</span><span class="p">(</span><span class="nx">job</span><span class="w"> </span><span class="nx">Job</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="c1">// ... do work ...</span><span class="w">
        </span><span class="nx">tokens</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="kc">true</span><span class="w">
    </span><span class="p">}(</span><span class="nx">job</span><span class="p">)</span><span class="w">
</span><span class="p">}</span></pre><div class="notes"><p>Very easy way to limit the number of go routines.
Basically a lightweight pool - good for one-time jobs.</p></div></div><div class="step step-level-1" step="135" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="216000" data-y="0" data-z="0"><h1 id="pattern-pipeline">Pattern: Pipeline</h1><p>Several pools connected over channels.</p><pre class="highlight code go"><span class="c1">// DO NOT:</span><span class="w">
</span><span class="kd">func</span><span class="w"> </span><span class="nx">work</span><span class="p">()</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nx">report</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">generateReport</span><span class="p">()</span><span class="w">
    </span><span class="nx">encoded</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">report</span><span class="p">.</span><span class="nx">Marshal</span><span class="p">()</span><span class="w">
    </span><span class="nx">compressed</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">compress</span><span class="p">(</span><span class="nx">encoded</span><span class="p">)</span><span class="w">
    </span><span class="nx">sendToNSA</span><span class="p">(</span><span class="nx">compressed</span><span class="p">)</span><span class="w">
</span><span class="p">}</span></pre><div class="notes"><p>Talk about the naive implementation where time of finish will
be influenced by a single long running job.</p></div></div><div class="step step-level-1" step="136" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="217600" data-y="0" data-z="0"><h1 id="pattern-parallel-iterator">Pattern: Parallel Iterator</h1><pre class="highlight code go"><span class="kd">func</span><span class="w"> </span><span class="nx">iter</span><span class="p">()</span><span class="w"> </span><span class="kd">chan</span><span class="w"> </span><span class="nx">Elem</span><span class="w"> </span><span class="p">{</span><span class="w">
     </span><span class="nx">ch</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nb">make</span><span class="p">(</span><span class="kd">chan</span><span class="w"> </span><span class="nx">Elem</span><span class="p">,</span><span class="w"> </span><span class="mi">10</span><span class="p">)</span><span class="w">
     </span><span class="k">go</span><span class="w"> </span><span class="kd">func</span><span class="p">()</span><span class="w"> </span><span class="p">{</span><span class="w">
         </span><span class="nx">a</span><span class="p">,</span><span class="w"> </span><span class="nx">b</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="w">
         </span><span class="k">for</span><span class="w"> </span><span class="p">{</span><span class="w">
             </span><span class="nx">ch</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nx">a</span><span class="w">
             </span><span class="nx">a</span><span class="p">,</span><span class="w"> </span><span class="nx">b</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">b</span><span class="p">,</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nx">b</span><span class="w">
         </span><span class="p">}</span><span class="w">
     </span><span class="p">}()</span><span class="w">
     </span><span class="k">return</span><span class="w"> </span><span class="nx">ch</span><span class="w">
</span><span class="p">}</span><span class="w">
</span><span class="k">for</span><span class="w"> </span><span class="nx">elem</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="k">range</span><span class="w"> </span><span class="nx">iter</span><span class="p">()</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="o">...</span><span class="w"> </span><span class="p">}</span></pre><div class="notes"><p>Problem: How to stop? Best to use context.Contex</p><p>Note: You should probably buffer a little here.</p></div></div><div class="step step-level-1" step="137" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="219200" data-y="0" data-z="0"><h1 id="problem-shared-state">Problem: Shared state</h1><div class="notes"><p>Easiest solution: Communicate via copies, do not share memory.</p></div></div><div class="step step-level-1" step="138" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="220800" data-y="0" data-z="0"><h1 id="problem-race-conditions">Problem: Race conditions</h1><pre class="highlight code go"><span class="kd">var</span><span class="w"> </span><span class="nx">counter</span><span class="w"> </span><span class="kt">int</span><span class="w">
</span><span class="kd">func</span><span class="w"> </span><span class="nx">f</span><span class="p">()</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="k">for</span><span class="p">(</span><span class="nx">idx</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="nx">idx</span><span class="w"> </span><span class="p">&lt;</span><span class="w"> </span><span class="mi">10000</span><span class="p">;</span><span class="w"> </span><span class="nx">idx</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="nx">counter</span><span class="o">++</span><span class="w">
    </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">
</span><span class="c1">// ...</span><span class="w">
</span><span class="k">go</span><span class="w"> </span><span class="nx">f</span><span class="p">()</span><span class="w">
</span><span class="k">go</span><span class="w"> </span><span class="nx">f</span><span class="p">()</span></pre><p>---</p><h1 id="solution-race-conditions">Solution: Race conditions</h1><ul><li>Avoid shared state. Limit scope where possible.</li><li>Prefer copy over references.</li><li>Use proper synchronisation.</li><li>Use a race detector. (<tt>helgrind</tt>, <tt>go test -race</tt>)</li><li>Write tests that are multithreaded.</li><li>Use Rust.</li></ul></div><div class="step step-level-1" step="139" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="222400" data-y="0" data-z="0"><h1 id="problem-deadlocks">Problem: Deadlocks</h1><pre class="highlight code go"><span class="nx">ch</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nb">make</span><span class="p">(</span><span class="kd">chan</span><span class="w"> </span><span class="kt">int</span><span class="p">)</span><span class="w">

</span><span class="c1">// thread1:</span><span class="w">
</span><span class="nx">ch</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="mi">42</span><span class="w">

</span><span class="c1">// thread2:</span><span class="w">
</span><span class="k">if</span><span class="w"> </span><span class="p">!</span><span class="nx">something</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="k">return</span><span class="w">
</span><span class="p">}</span><span class="w">

</span><span class="o">&lt;-</span><span class="nx">ch</span></pre></div><div class="step step-level-1" step="140" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="224000" data-y="0" data-z="0"><h1 id="problem-deadlock-2">Problem Deadlock #2</h1><pre class="highlight code go"><span class="kd">func</span><span class="w"> </span><span class="nx">foo</span><span class="p">()</span><span class="w"> </span><span class="kt">error</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nx">mu</span><span class="p">.</span><span class="nx">Lock</span><span class="p">()</span><span class="w">
    </span><span class="k">if</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">bar</span><span class="p">();</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="k">return</span><span class="w"> </span><span class="nx">err</span><span class="w">
    </span><span class="p">}</span><span class="w">

    </span><span class="nx">mu</span><span class="p">.</span><span class="nx">Unlock</span><span class="p">()</span><span class="w">
    </span><span class="k">return</span><span class="w"> </span><span class="kc">nil</span><span class="w">
</span><span class="p">}</span></pre></div><div class="step step-level-1" step="141" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="225600" data-y="0" data-z="0"><h1 id="problem-deadlock-3">Problem Deadlock #3</h1><pre class="highlight code go"><span class="kd">func</span><span class="w"> </span><span class="nx">foo</span><span class="p">()</span><span class="w"> </span><span class="kt">error</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nx">mu1</span><span class="p">.</span><span class="nx">Lock</span><span class="p">()</span><span class="w">
    </span><span class="nx">mu2</span><span class="p">.</span><span class="nx">Lock</span><span class="p">()</span><span class="w">
    </span><span class="c1">// ...</span><span class="w">
    </span><span class="k">defer</span><span class="w"> </span><span class="nx">mu1</span><span class="p">.</span><span class="nx">Lock</span><span class="p">()</span><span class="w">
    </span><span class="k">defer</span><span class="w"> </span><span class="nx">mu2</span><span class="p">.</span><span class="nx">Lock</span><span class="p">()</span><span class="w">
</span><span class="p">}</span><span class="w">
</span><span class="kd">func</span><span class="w"> </span><span class="nx">bar</span><span class="p">()</span><span class="w"> </span><span class="kt">error</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nx">mu2</span><span class="p">.</span><span class="nx">Lock</span><span class="p">()</span><span class="w">
    </span><span class="nx">mu1</span><span class="p">.</span><span class="nx">Lock</span><span class="p">()</span><span class="w">
    </span><span class="c1">// ...</span><span class="w">
    </span><span class="k">defer</span><span class="w"> </span><span class="nx">mu2</span><span class="p">.</span><span class="nx">Lock</span><span class="p">()</span><span class="w">
    </span><span class="k">defer</span><span class="w"> </span><span class="nx">mu1</span><span class="p">.</span><span class="nx">Lock</span><span class="p">()</span><span class="w">
</span><span class="p">}</span></pre></div><div class="step step-level-1" step="142" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="227200" data-y="0" data-z="0"><h1 id="solution-deadlocks">Solution: Deadlocks</h1><ul><li>Obtain a stacktrace if they happen.</li><li>Debugger (if deadlock is not timing sensitive)</li><li>Keep critical sections small.</li><li>Use defer for the <tt>Unlock</tt>.</li><li>Respect the lock hierarchy.</li><li>Double think if an unbuffered channel will work out.</li><li>Use unidirectional channels and <tt>select</tt> in Go.</li><li>Don't be clever.</li></ul><div class="notes"><p>Deadlocks happen frequently when working with channels.</p><p>Tip: In Go progamms you can press Ctrl+or send SIGABRT or SIGTERM
to the program to make it print a stack trace.
Or use a debugger.</p><p>Don't be clever: There's a saying:</p><p>If you write the code as cleverly as possible, you are,
by definition, not smart enough to debug it.
--Brian Kernighan,</p><p>And our mind's horizon is never far away when doing parallel programming.</p></div></div><div class="step step-level-1" step="143" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="228800" data-y="0" data-z="0"><h1 id="problem-livelock">Problem: Livelock</h1><p>Example:</p><ul><li>Two persons walking in opposite directions,
trying to pass each other in a tight corridor.</li><li>When both persons move at the same time left and right
then hallway is still blocked.</li><li>If infinitely done, then it's a livelock.</li></ul><div class="notes"><p>A system that does not make any progress for prolonged times.
Relatively seldom, but can happen.</p><p>Usual cause: Too primitive retry mechanism.</p></div></div><div class="step step-level-1" step="144" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="230400" data-y="0" data-z="0"><h1 id="solution-livelock">Solution: Livelock</h1><ul><li>Avoid circular dependencies.</li><li>Use an arbitrator.</li><li>Use exponential backoff.</li></ul><div class="notes"><ul><li>Arbitrator: In the metaphor above somebody that has an overview of the situation and tells one person to move.</li><li>Exponential backoff: Proper retry mechanism with random jitter between retries.</li></ul><p>Real life example: Two processes trying to execute an SQL transaction that depend on each other.
SQL server will stop the transaction and make them retry - if the retry mechanism is the same, then
it might take a long time to resolve the situation.</p></div></div><div class="step step-level-1" step="145" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="232000" data-y="0" data-z="0"><h1 id="problem-resource-starvation">Problem: Resource starvation</h1><p>&#xBB;Greedy&#xAB; threads can block resources used by other threads.</p><div class="notes"><p>Resource: a database, some webserver, the CPU, the filesystem.</p><p>Can be caused by a deadlock, a livelock or any performance issues
or just duplicate work.</p><p>Typical in queuing systems:</p><ul><li>SlowConsumer</li><li>SlowProducer</li></ul></div></div><div class="step step-level-1" step="146" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="233600" data-y="0" data-z="0"><h1 id="solution-resource-starvation">Solution: Resource starvation</h1><ul><li>Make sure threads can not use resources exclusively.</li><li>Queuing: Allow a lot of buffering.</li><li>Benchmark: Are all resources used to full extent?</li></ul><div class="notes"><p>Buffering is necessary in queuing systems to account for slow producers / slow consumers.</p><p>Resource starvation is hard to fix in general and often goes unnoticed as it's often silent.
(i.e. system works, but is not as fast as it could have been)</p></div></div><div class="step step-level-1" step="147" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="235200" data-y="0" data-z="0"><h1 id="brainfuck-time">Brainfuck time</h1><img src="images/philosophers.png"></img><ul><li>Each philosopher changes state between "thinking" and "eating".</li><li>During "eating" he requires two forks (it's spaghetti)</li><li>The state changes happend randomly after some time.</li></ul><p>Goal: no philosopher should starve.</p><div class="notes"><p>Two problems that can occur:</p><ul><li>Deadlock: Every philosopher took the left fork. None can pick the right fork.</li><li>Starvation: A single philspopher might be unlucky and never get two forks.</li></ul><p>Solution:</p><ul><li>Simple: Use a single mutex as "waiter" to stop concurrency.</li><li>Hard &amp; correct: Use global mutex pluse "hungry" state with semaphor per philosopher.</li><li>Easier: Give philosophers invdividual rights and priorities.</li><li>Weird: philosopher talk to each other if they need a fork (i.e. channels)</li></ul></div></div></div><script type="text/javascript" src="js/impress.js"></script><script type="text/javascript" src="js/gotoSlide.js"></script><script type="text/javascript" src="js/hovercraft.js"></script></body></html>