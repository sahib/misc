<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml"><head><title>Performance Performance: Intro</title><meta charset="UTF-8"></meta><meta name="generator" content="Hovercraft! 1.0 http://regebro.github.io/hovercraft"></meta><link rel="stylesheet" href="css/hovercraft.css" media="all"></link><link rel="stylesheet" href="css/highlight.css" media="all"></link><link rel="stylesheet" href="hovercraft.css" media="screen,projection"></link><link rel="stylesheet" href="hovercraft.css" media="screen,projection"></link><script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        showProcessingMessages: false,
        messageStyle: "none",
        TeX : { extensions : ['color.js'] }
      });
    </script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script></head><body class="impress-not-supported"><div id="impress-help"></div><div id="impress" data-transition-duration="1500 950"><div class="step step-level-1" step="0" data-x="2500" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-y="0" data-z="0"><p class="chapter">Intro</p><p>General intro and sideproject &#x1F3E0;</p></div><div class="step step-level-1" step="1" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="5000" data-y="0" data-z="0"><h1 id="agenda">Agenda</h1><ul><li>Rant &amp; Motivation</li><li>Benchmarking Basics</li><li>Complexity Theory</li><li>Sideproject</li></ul><img src="images/door.svg" width="42%"></img></div><div class="step step-level-1" step="2" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="7500" data-y="0" data-z="0"><h1 id="what-s-that">What's that?</h1><img src="images/moores_law.png" width="100%"></img></div><div class="step step-level-1" step="3" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="10000" data-y="0" data-z="0"><h1 id="who-s-that">Who's that?</h1><img src="images/bill_taketh.jpg" width="100%"></img><div class="notes"><p>General idea:</p><ul><li>Maybe you heard of Moore's law? Computing power doubles every two years</li><li>Andy and Bill's law: What Andy Grove (Intel ex-CEO) produces in Hardware speed, Bill Gates takes away.</li><li>Wirt's law: Software gets slower more rapdidly than hardware gets faster.</li><li>Lemur's law: Software engineers get twice as incompentent every decade (only half ironic) - seriously, as an engineering discipline we should be
ashamed of how bad we performed over the last decades. We introduced so many layers of bad software and hacks that we depend on that we can't
change anymore. It's like building a complete city on sand. Part of this because we don't really do engineerings and focus so much on providing
company value that many of us did not even learn how good, performance optimized is supposed to look like. The costs of software engineers
is more expensive than hardware these days, but this is short sighted. Investing in quality long term benefits us all.
I hope to change your perspective a bit in this talk. We all lost the connection to the machine our programs run on and while the things in this
talk were somewhat common knowledge 20 years ago (at least parts of it) it became somehow obscure knowledge over time and universities just focused
on disciplines like web development and data science where you're not supposed to have this knowledge. Because you know, numpy and pandas does it for you.
Or the browser will just do the right thing.</li></ul></div></div><div class="step step-level-1" step="4" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="12500" data-y="0" data-z="0"><h1 id="performance-inflation">Performance inflation</h1><img src="images/meme.jpg" width="100%"></img><div class="notes"><p>NOTE: Exaggerated of course and I kinda see myself also in that meme. Programming got much easier now, but also much ...broader. Much more languages,
frameworks, concepts that a programmer is expected to know. Still: In earlier days, programming required a much more thorough approach with more experimentation
and there was no StackOverflow, AI, Auto complete or even documentation. Throwing more hardware at the problem was also no choice.
Knowledge today is much more superficial than it was before. Mostly, because deep understanding of how a computer works is simply not required to produce something that works.</p><p>The thing is: Not requiring this kind of knowledge is a blessing and a curse at the same time. A blessing for our productivity, but
in general a curse for the software we produce:</p><ul><li>In the 90s we still squeezed every byte of memory out of game consoles and did both amazing and scary optimizations to get basic functionality.</li><li>And last decade we invented things like Electron, a lazy-ass way to make
applications "portable" by just starting a browser for every application</li><li>The main motivation of this workshop was actually being annoyed by things like Electron
and I wanted that you guys do not invent something like Electron.</li><li>If you think Electron is a good idea, then please stop doing anything related to software engineering.</li><li>Maybe try gardening, or do waterboarding in Guantanamo. Just do something less hurtful to mankind than Electron</li><li>Seriously take some pride as software engineerings and try to leave a solid legacy to the next generation of engineers.</li><li>Understanding how a computer works helps to not be like Bill Gates and just eat up hardware advancements with
worse software.</li></ul><p>Also, this is the only meme. I promise.</p></div></div><div class="step step-level-1" step="5" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="15000" data-y="0" data-z="0"><img src="images/hyper.png" width="100%"></img><div class="notes"><p>No joke: This electron based terminal took 700M (!) of residual memory.
This is absolutely insane and should not have been released.</p><p>You could argue now: Well, we would care more about performance but in todays world
we don't have time to time because we're agile and have to produce feature after feature.
We care only once we get into troubles. This also leads to developers never learning about
writing software that has to fulfill some performance criterias.</p><p>There is some truth about that. Product management usually is not that much aware of Performance
if it's not a hard product requirement. But that's your job - you're supposed to measure performance
and predict that it will be an issue. If you think it will be an issue, then you should fight for
the time to fix it.</p><p>Anyways, in this workshop we will enter a fantasy world, where we have infinite amounts of time.</p></div></div><div class="step step-level-1" step="6" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="17500" data-y="0" data-z="0"><h1 id="simple">&#xBB;Simple&#xAB;</h1><pre class="highlight code python"><span class="c1"># Read in a line, print the line without whitespaces.</span><span class="w">
</span><span class="c1"># Most complexity here is hidden.</span><span class="w">
</span><span class="kn">import</span> <span class="nn">sys</span><span class="w">
</span><span class="nb">print</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">stdin</span><span class="o">.</span><span class="n">readline</span><span class="p">()</span><span class="o">.</span><span class="n">strip</span><span class="p">())</span></pre><div class="notes"><p>Complexity is often hidden, without you noticing, often with the intent to "make things simple".</p><p>Simple can mean different things.
"simple" can mean "small cognitive load", i.e. programs that are simple to understand.
"simple" can also mean "programs with few instructions and few abstractions"</p><p>The prior rules assume that we're able to understand what's going on
in our program. After all we have to judge what gets executed ultimately.
Turns out, in interpreted language this is very hard.</p><p>Interpreted -&gt; compiled to byte code.
sys.stdin.readline are two dict lookups.
memory allocations
file I/O from stdin to stdout
calling a c function (strip)
several syscalls
unicode conversion!</p></div></div><div class="step step-level-1" step="7" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="20000" data-y="0" data-z="0"><h1 id="inside-python">Inside Python &#x1F40D;</h1><pre class="highlight code c"><span class="k">static</span><span class="w"> </span><span class="n">PyObject</span><span class="w"> </span><span class="o">*</span><span class="w">
</span><span class="nf">strip</span><span class="p">(</span><span class="n">PyObject</span><span class="w"> </span><span class="o">*</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">PyObject</span><span class="w"> </span><span class="o">*</span><span class="n">args</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="kt">char</span><span class="w"> </span><span class="o">*</span><span class="n">s</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">NULL</span><span class="p">;</span><span class="w">
    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="o">!</span><span class="n">PyArg_ParseTuple</span><span class="p">(</span><span class="n">args</span><span class="p">,</span><span class="w"> </span><span class="s">"s"</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">s</span><span class="p">))</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="k">return</span><span class="w"> </span><span class="nb">NULL</span><span class="p">;</span><span class="w">
    </span><span class="p">}</span><span class="w">

    </span><span class="cm">/* ... actual "strip" logic here ... */</span><span class="w">
    </span><span class="k">return</span><span class="w"> </span><span class="n">PyUnicode_FromString</span><span class="p">(</span><span class="n">s</span><span class="p">);</span><span class="w">
</span><span class="p">}</span></pre><div class="notes"><p>All functions eventuall call functions implemented in C:</p><p>And that happens for every function call in Python. Very often.
All those objects are allocated on the heap. Python is easy, but the price you pay for it
is high. This might give you a first feeling on how much stuff happens in a simple program.</p><p>Printing to stdout and drawing something on the screen is insanely complex too and beyond
this workshop.</p><p>This slides could be also a talk about "Why interpreted languages suck"</p><p>Most optimizations will not work with python.
As a language it's really disconnected from the HW - every single statement
will cause 100s or 1000s of assembly instructions. Also there are no almost
no guarantees how big e.g. arrays or other data structures will be and how
they are layout in memory. You have to rely on your interpreter (and I count
Java's JIT as one!) to be fast on modern hardware - most are not and that's
why there's so much C libraries in python, making the whole packaging system
a bloody mess.</p><p>Side note: There are also declarative languages like SQL (as compared to
imperative languages like C) that this workshop is not focusing on. Working
on performance there is indirect, i.e. achieved by tricks.</p></div></div><div class="step step-level-1" step="8" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="22500" data-y="0" data-z="0"><h1 id="workshop-contents">Workshop contents</h1><p>We try to answer these questions:</p><ul><li>Why is performance important?</li><li>How does the machine we program on work?</li><li>How do we notice this as developers?</li><li>Are there ways to exploit this machine?</li></ul><p><em>Remember:</em> <strong>Work</strong> shop.</p><div class="notes"><p>Disclaimer:</p><ul><li>We're working from low level to slightly higher level here. Don't expect tips like "use this data structure to make
stuff incredibly fast". I'll won't go over all possible performance tips for your language (there are better
lists on the internet). I also won't go over a lot of data structures - what I do show is to show you how to choose
a data structure.</li><li>The talk is loosely tied to the hardware: General intro, cpu, mem, io, parallel programming</li><li>Most code examples will be in Go and C, as most ideads require a compiled language.</li><li>Interpreted languages like Python/Typescript might take away a few concepts, but to be honest,
your language is fucked up and will never achieve solid performance.</li><li>For Python you can at least put performance criticals into C libraries, for the blistering cestpool
that web technology is... well, I guess your only hope is Webassembly.</li><li>If you are unsure how a specific concept translates to your language: just ask. I might have no idea,
but often there is only a limited choice of design decisions language designers can make.</li><li>In this talk you will learn why people invent things Webassembly - even though it's kinda sad.</li></ul><p>My main goal is though to give you a "table of contents" of most things related to performance.
The whole thing is at least one semester of contents. We don't have enough time though, so we will
jump a lot from topic to topic while barely scratching the surface. This should not matter too much
though as long you just remember later "Ah, Lemur said something about this behavior, but I dont recall
the details, let's Google" (or maybe even open those slides again). The hardest part of experience
is that concepts exists. Applying them is often easier. If you manage to do that I will be fairly happy.</p><p>This also means that you don't need to worry if you don't understand something at first glance. Note it down
or directly ask during the workshop, but try to follow th ecurrent slides instead of trying to understand
every last detail.</p></div></div><div class="step step-level-1" step="9" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="25000" data-y="0" data-z="0"><h1 id="what-can-you-do-with-it">What can you do with it?</h1><table cellpadding="0" cellspacing="0" class="colwidths-given list-table"><thead><tr><th><p><tt>go bench</tt></p></th><th><p>&#x3BC;s/op</p></th><th><p>B/op</p></th><th><p>allocs/op</p></th><th><p>speedup</p></th></tr></thead><tbody><tr><td><p><tt>sqlite3-Pop</tt></p></td><td><p>4.878</p></td><td><p>1.239.961</p></td><td><p>6.217</p></td><td><p>1x</p></td></tr><tr><td><p><tt>sqlite3-Push</tt></p></td><td><p>2.420</p></td><td><p>688.887</p></td><td><p>32.034</p></td><td><p>1x</p></td></tr><tr><td><p><tt>timeq-Pop</tt></p></td><td><p>35</p></td><td><p>232</p></td><td><p>4</p></td><td><p>136x</p></td></tr><tr><td><p><tt>timeq-Push</tt></p></td><td><p>61</p></td><td><p>98</p></td><td><p>3</p></td><td><p>39x</p></td></tr></tbody></table><p>OP = Push/Pop 2k Items with 40 Bytes each.</p><p><a href="https://github.com/sahib/timeq">https://github.com/sahib/timeq</a></p><div class="notes"><p>It may not look like it, but this was the slide I put the most work into.</p><p>I wrote a persistent priority queue in Go. It's kinda fast.
At the end of the workshop you should be able to understand why it is fast
and why it's designed that way. Maybe you can even improve it!</p><p>By the way, this doesn't mean that SQLite is bad. It's a general purpose database
that was forced into being a priority queue. There are obviously some assumptions
that allow better performance.</p></div></div><div class="step step-level-1" step="10" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="27500" data-y="0" data-z="0"><h1 id="was-it-worth-it">Was it worth it?</h1><ul><li>Choosing SQLite is an absolutely sane decision.</li><li>Building your own database probably not so much.</li><li>Designing/testing a production-grade DB is hard.</li><li>Handling edgecases &amp; errors gracefully is even worse.</li><li>It's 50x faster, but it took 50x the work.</li><li>Still, you should know how, if you have to.</li></ul><div class="notes"><p>At time of writing, timeq has roughly 1.5k lines of code.
The amount of testcode is about the same amount. However,
SQLite has 173x times the amount of test code and is readily packaged.</p></div></div><div class="step step-level-1" step="11" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="30000" data-y="0" data-z="0"><h1 id="what-s-not-in-here">What's not in here?</h1><ul><li>An exhausting list of tips. You'd forget them.</li><li>A full lecture on algorithm and data structures.</li><li>A lecture you just have to listen to make it click.</li><li>Language specific optimization techniques.</li><li>Performance in distributed systems.</li><li>Application specific performance tips (<em>Networking, SQL, Marshalling, IPC</em> ...)</li></ul><div class="notes"><p>Google: I mean that. After the workshop you know what to google for. Hopefully.</p><p>There are plenty free online courses and many books. I can't really recommend one,
as my lecture in university is also already 10 years ago now.</p><p>Languages: includes C, Go, Python and a bit of Bash though.
Most code examples are written with compiled languages in mind.
Users of interpreted languages may find some things unintuitive.</p><p>Check that "interpreted" and "compiled" is a known distinction.</p></div></div><div class="step step-level-1" step="12" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="32500" data-y="0" data-z="0"><h1 id="help">Help!</h1><ul><li>This workshop is written in a markup language (.rst).</li><li>Almost every slide has speaker notes.</li><li>I tried to make them generally understandable.</li><li>If you need more background, read them:</li></ul><p><a href="https://github.com/sahib/misc/blob/master/performance/1_intro.rst#workshop-contents">Link to Github</a></p></div><div class="step step-level-1" step="13" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="35000" data-y="0" data-z="0"><h1 id="experiments-mandatory">Experiments mandatory &#x1F97C;</h1><p>You'll write your own <em>cute</em> database:</p><ul><li>You can group up or do it on our own.</li><li>You can use your favourite language.</li><li>You can always ask me outside or in the workshop about your progress and problems.</li></ul><div class="notes"><p>But do the database for yourself, not for me. Also, not every topic in
the slides has to be present in your database. I'm only sharing general ideads
here, not implementation tips. You don't have to remember all of them,
but hopefully you will take away the core thoughts behind those ideads.</p><p>Also, please note that I'm not expert in everything myself. I do those
workshops to educate myself on a certain topic. Also, I'm guilty of breaking
most of the "tips" I give in this talk. That should not come as a surprise,
as every rule is made to be broken. Most of the time for stupid reasons
though.</p><p>This might serve as career tip though: If you want to deep dive into a certain
topic, then prepare a presentation about it. If you're able to explain it to
others, then you're probably kind of good in it.</p><p>So: this is also some kind of test for myself.</p></div></div><div class="step step-level-1" step="14" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="37500" data-y="0" data-z="0"><h1 id="what-is-optimization">What is optimization?</h1><p>Please define it in your words.</p><div class="notes"><p>In computer science, optimization is the process of modifying
a software system to make some aspect of it work more efficiently
or use fewer resources. -- Wikipedia</p><p>The "fewer resources" is the more important bit. See yourself as tenant
of resources like CPU, Mem, disk, network, dbs, ... that you share with
other tenants of the same system. Be nice to other tenants, don't just
make your own life pleasant.</p></div></div><div class="step step-level-1" step="15" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="40000" data-y="0" data-z="0"><h1 id="when-to-optimize">When to optimize?</h1><p>If <strong>performance requirements</strong> are not met <strong>and</strong> when doing so does not hurt other requirements.</p><div class="notes"><p>Wait, there are such requirements?</p><p>Most of us do implicit requirements: Does it feel fast enough?
So probably more often than you do now.</p><p>Other requirements: Maintenability and readability e.g.
or correctness.</p></div></div><div class="step step-level-1" step="16" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="42500" data-y="0" data-z="0"><h1 id="questions-to-ask">Questions to ask:</h1><ul><li>On what kind of system the software will run on?</li><li>How many requests will there be in parallel?</li><li>What kind of latency is the user willing to accept? (<em>Games, Websites, ATMs</em>, ...)</li><li>How much scaling is expected in the next time?</li><li>How long can we do without? Do we need it now?</li><li>Will my technology choice be a bottleneck? (<em>Python, React, Electron, ...</em>)</li><li>Does <em>EdgeCaseX</em> need to perform well?</li><li>Are the optimizations worth the risk/effort?</li><li>...</li></ul><div class="notes"><p>It's your job to figure out the performance requirements. Your PM will likely not be
technical enough to set realisitc goals, so you need to discuss with him what kind
of use cases you have and what kind of performance is acceptable for them (the latter is your part)
Figure out possible edge cases together (i.e. pathological use cases bringing down your requirement)
The engineer is the driver of the conversation, as he know's where the problems are.</p><p>Do some basic calculations based on these questions and add X to your goals. Those are your
requirements.</p></div></div><div class="step step-level-1" step="17" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="45000" data-y="0" data-z="0"><h1 id="when-not-to-optimize">When not to optimize?</h1><p class="quote">Programmers waste enormous amounts of time thinking about, or worrying about, the speed of noncritical parts of their programs, and these attempts at efficiency actually have a strong negative impact when debugging and maintenance are considered. We should forget about small efficiencies, say about 97% of the time: <strong>premature optimization is the root of all evil.</strong> Yet we should not pass up our opportunities in that critical 3%.</p><div class="line-block">(Donald Knuth)<br></br></div><img src="images/knuth.jpg" width="35%" class="knuth"></img><div class="notes"><p>I used the full quote here, since it's often abbreviated as "premature optimization is the root of all evil" which
has a totally different meaning.</p><p>Many programmers just asked "how fast can it be?" and not "how fast should it be?"
That's a fine question for personal learning but not for an actual product where time is a resource.</p><p>If you don't have a problem you really should not do anything.
It is difficult to define what a "problem" is.</p><p>Electron apparently defined that it's not a problem if low-memory devices
can't use their framework.</p></div></div><div class="step step-level-1" step="18" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="47500" data-y="0" data-z="0"><h1 id="huh-premature">Huh, premature?</h1><img src="images/premature_optimization_flowchart.png" width="35%"></img><p><strong>Reminder:</strong> <em>It does not matter how fast you compute a wrong result.</em></p><div class="notes"><p>Proof: There's a xkcd for everything.</p><p>The main point is: Take your time to do things the right away. Don't drop the pen
when it worked for the first time and didn't feel slow, really take some to measure.</p><p>However, don't just blindly optimize things before you measured or optimize the small
things after measuring.</p><p>Optimizations come at a price. It's usually more and harder code to maintain (and if not,
why didn't you do it in the first place?) or they have some other disadavntages (an index
in a database for example slows drown writes and needs space!). Is it worth the risk?</p></div></div><div class="step step-level-1" step="19" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="50000" data-y="0" data-z="0"><h1 id="the-mantra">The mantra</h1><ol><li>Make it correct.</li><li>Make it beautiful.</li><li>Make it fast.</li></ol><p>In that order. Don't jump.</p><div class="notes"><p>Often enough we do not even get to step 2 though. Sometimes not even step 1 :D
Making things fast should always be a consideration. Software is not done once
you are happy with the beautiful abstractions you found.</p></div></div><div class="step step-level-1" step="20" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="52500" data-y="0" data-z="0"><h1 id="how-do-i-measure">How do I measure?</h1><p>In a reproducible environment.</p><p>(<a href="https://gernot-heiser.org/benchmarking-crimes.html">Best practices</a>)</p><img src="diagrams/1_how_do_i_measure.svg" width="100%"></img><div class="notes"><p>Only ever compare apples with apples. Don't compare numbers
between:</p><ul><li>Different machines.</li><li>Different runs with different load on the same machine.</li><li>Different inputs.</li><li>Different implementations if they do not produce the same results.</li></ul><p>Use benchmarks primarily to compare numbers of older benchmarks.
And if you have to compare different implementations: Stay fair.</p><p>Profiling    = Performance debugging.
Benchmarking = Performance testing (i.e. ware optimizations still working?)</p></div></div><div class="step step-level-1" step="21" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="55000" data-y="0" data-z="0"><h1 id="example-go">Example: Go</h1><pre class="highlight code go"><span class="kd">func</span><span class="w"> </span><span class="nx">BenchmarkFoo</span><span class="p">(</span><span class="nx">b</span><span class="w"> </span><span class="o">*</span><span class="nx">testing</span><span class="p">.</span><span class="nx">B</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nx">possiblyExpensiveSetup</span><span class="p">()</span><span class="w">
    </span><span class="nx">b</span><span class="p">.</span><span class="nx">ResetTimer</span><span class="p">()</span><span class="w">
    </span><span class="k">for</span><span class="w"> </span><span class="nx">i</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="nx">i</span><span class="w"> </span><span class="p">&lt;</span><span class="w"> </span><span class="nx">b</span><span class="p">.</span><span class="nx">N</span><span class="p">;</span><span class="w"> </span><span class="nx">i</span><span class="o">++</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="nx">functionUnderTest</span><span class="p">()</span><span class="w">
    </span><span class="p">}</span><span class="w">
</span><span class="p">}</span></pre><div class="notes"><p>Example taken from here: <a href="https://www.p99conf.io/2023/08/16/how-to-write-accurate-benchmarks-in-go/">https://www.p99conf.io/2023/08/16/how-to-write-accurate-benchmarks-in-go/</a></p><p>(it also has some good tips about many accidents that might happen, we'll see some of them later)</p></div></div><div class="step step-level-1" step="22" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="57500" data-y="0" data-z="0"><h1 id="how-to-optimize">How to optimize?</h1><ul><li>Do less work.</li><li>Do the same work faster.</li><li>Do the work at the same time.</li><li>Do it in another order.</li></ul><div class="notes"><p>Examples:</p><ul><li>Use a different data structure (map vs btree)</li><li>Do things like caching.</li><li>threads, processes, coroutines.</li><li>Do not wait for the longest part to finish to continue</li></ul><p>Rules stolen from here: <a href="https://eblog.fly.dev/startfast.html">https://eblog.fly.dev/startfast.html</a></p></div></div><div class="step step-level-1" step="23" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="60000" data-y="0" data-z="0"><h1 id="measurement-first">Measurement first</h1><p>Requires a strong understanding of your program and experience.</p><ul><li>No way around measurements as <strong>first</strong> step.</li><li>A certain level of experience helps.</li><li>The model of your program in your head
is different to what gets actually executed.</li></ul><div class="notes"><p>No short answer and no shortcuts to this.
It will be a long journey and this is workshop will be only a step on the journey.
Very many different languages, OS (Python, Go) and many different applications
(SQL - 90%: just add an index) that cannot all be covered.</p><p>Example: If an application starts slow, then do you know what happens on startup?
Maybe not - but it's important to take the right decisions.</p></div></div><div class="step step-level-1" step="24" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="62500" data-y="0" data-z="0"><h1 id="hot-section">Hot section</h1><pre class="highlight code go"><span class="ln"> 1 </span><span class="w"> </span><span class="kd">func</span><span class="w"> </span><span class="nx">process</span><span class="p">(</span><span class="nx">b</span><span class="w"> </span><span class="p">[]</span><span class="kt">byte</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
</span><span class="ln"> 2 </span><span class="w">     </span><span class="k">if</span><span class="p">(</span><span class="nx">edgeCaseCondition</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
</span><span class="ln"> 3 </span><span class="w">         </span><span class="c1">// ...</span><span class="w">
</span><span class="ln"> 4 </span><span class="w">     </span><span class="p">}</span><span class="w">
</span><span class="ln"> 5 </span><span class="w">
</span><span class="ln"> 6 </span><span class="w">     </span><span class="k">for</span><span class="w"> </span><span class="nx">i</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="nx">i</span><span class="w"> </span><span class="p">&lt;</span><span class="w"> </span><span class="nb">len</span><span class="p">(</span><span class="nx">b</span><span class="p">);</span><span class="w"> </span><span class="nx">i</span><span class="o">++</span><span class="w"> </span><span class="p">{</span><span class="w">
</span><span class="ln"> 7 </span><span class="w">         </span><span class="k">for</span><span class="w"> </span><span class="nx">j</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">i</span><span class="p">;</span><span class="w"> </span><span class="nx">j</span><span class="w"> </span><span class="p">&lt;</span><span class="w"> </span><span class="nb">len</span><span class="p">(</span><span class="nx">b</span><span class="p">);</span><span class="w"> </span><span class="nx">j</span><span class="o">++</span><span class="w"> </span><span class="p">{</span><span class="w">
</span><span class="ln"> 8 </span><span class="w">             </span><span class="nx">b</span><span class="p">[</span><span class="nx">i</span><span class="p">]</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">magicCalculation</span><span class="p">(</span><span class="nx">b</span><span class="p">,</span><span class="w"> </span><span class="nx">i</span><span class="p">,</span><span class="w"> </span><span class="nx">j</span><span class="p">)</span><span class="w">
</span><span class="ln"> 9 </span><span class="w">         </span><span class="p">}</span><span class="w">
</span><span class="ln">10 </span><span class="w">
</span><span class="ln">11 </span><span class="w">         </span><span class="k">if</span><span class="w"> </span><span class="nx">i</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="nb">len</span><span class="p">(</span><span class="nx">b</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="p">{</span><span class="w">
</span><span class="ln">12 </span><span class="w">             </span><span class="k">for</span><span class="w"> </span><span class="nx">j</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="nx">j</span><span class="w"> </span><span class="p">&lt;</span><span class="w"> </span><span class="nb">len</span><span class="p">(</span><span class="nx">b</span><span class="p">);</span><span class="w"> </span><span class="nx">j</span><span class="o">++</span><span class="w"> </span><span class="p">{</span><span class="w">
</span><span class="ln">13 </span><span class="w">                 </span><span class="nx">b</span><span class="p">[</span><span class="nx">j</span><span class="p">]</span><span class="w"> </span><span class="o">*=</span><span class="w"> </span><span class="mi">2</span><span class="p">;</span><span class="w">
</span><span class="ln">14 </span><span class="w">             </span><span class="p">}</span><span class="w">
</span><span class="ln">15 </span><span class="w">         </span><span class="p">}</span><span class="w">
</span><span class="ln">16 </span><span class="w">     </span><span class="p">}</span><span class="w">
</span><span class="ln">17 </span><span class="w"> </span><span class="p">}</span></pre><div class="notes"><p>The Hot section is the path through your programs that is taken
most often. This path defines the order in which you should optimize.
If you optimize some edge case that is only taken 1% of the time,
then the speedup of your optimization is also only worth 1%, because
99% of your program is still as slow as before.</p><p>How to find the path? Tools like pprof can find it, but also coverage
tools or even debuggers can help you find them. Chances are that you
the Hot section for your module anyways.</p><p>Other related terms are "Hot section" or "tight loop".</p><p>A related term is "slow path". This is often the path taken by a program
that hits some edge case. Edge cases are often (purposefully) not optimized
for, instead we generally optimize for the common "fast path".</p></div></div><div class="step step-level-1" step="25" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="65000" data-y="0" data-z="0"><h1 id="a-rule-of-thumb">A rule of thumb &#x1F44D;</h1><p><strong>Go from big to small</strong>:</p><ol><li>Do the obvious implementation first.</li><li>Check if your requirements are met.</li><li>If not, find the <strong>biggest</strong> hot section.</li><li>Optimize it and repeat from step 1.</li></ol><div class="notes"><ol><li>"obvious" depends a lot on experience. Example: Open a CSV file 10k times
to extract a single row because you have a convenience function.
Do not use this as excuse for bad software.</li><li>If you don't have concrete functional/performance requirements, make some.</li><li>We are incredible bad at guessing! Never ever skip this step!</li><li>Never mix up this order.</li></ol><p>Step 3 is the most difficult one. You should start measuring the full speed of your program
then of one module and so on until you know what part consumes the most time/resources.</p></div></div><div class="step step-level-1" step="26" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="67500" data-y="0" data-z="0"><h1 id="theory-complexity">Theory: Complexity</h1><ul><li>Algorithms/Structures can be divided in classes.</li><li>General types are <strong>time</strong> and <strong>space</strong> complexity.</li><li>Each divided in <strong>worst, best &amp; average case</strong>.</li><li>For datastructures specific operations are scored.</li><li>Complexity classes are given in Big-O notation.</li></ul><div class="notes"><p>It's a bit like Pokemon for algorithms.
"Merge sort, use worst case on quick sort!"
"It's very effective!"</p><p>Good example (thanks Alex): <a href="https://sortvisualizer.com">https://sortvisualizer.com</a>
(compare quick sort and merge sort)</p><p>The general idea is to have a function that relates the number
of elements given to an algorithm to the number of operations the
algorithm has to do to produce a result.</p></div></div><div class="step step-level-1" step="27" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="70000" data-y="0" data-z="0"><h1 id="theory-big-o-notation">Theory: Big-O Notation</h1><img src="images/bigo.svg" width="100%"></img><p><a href="https://www.bigocheatsheet.com">https://www.bigocheatsheet.com</a></p><div class="notes"><p>O(1) -&gt; constant
O(n) -&gt; linear
O(log n) -&gt; logarithmic
O(n * log n) -&gt; sorting
O(n ** x) -&gt; polynomial
O(x ** n) -&gt; exponential
O(n!) -&gt; fucktorial (oops, typo)</p><p>Data structures and algorithms:</p><p>-&gt; Some have better space / time complexity.
-&gt; Most have tradeoffs, only few are universally useful like arrays / hash tables
-&gt; Some are probalibisitic: i.e. they save you work or space at the expense of accuracy (bloom filters)
-&gt; Difference between O(log n) and O(1) is not important most of the time. (database developers might disagree here though)</p><p>For small n the difference doe snot mattern. It can be difficult to figure out what "n" is small.</p></div></div><div class="step step-level-1" step="28" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="72500" data-y="0" data-z="0"><h1 id="complexity-exercises">Complexity exercises:</h1><ol><li><em>Time</em> complexity of <em>Bubble Sort</em>?</li><li><em>Time</em> complexity of <em>Binary Search</em> (<em>worst</em> &amp; <em>best</em>)?</li><li><em>Space</em> complexity of <em>Merge Sort</em> versus <em>Quick Sort</em>?</li><li><em>Removing</em> an element from an <em>Array</em> vs a <em>Linked List</em>?</li><li><em>Best/Worst</em> case time complexity of <em>Get/Set</em> of <em>Dicts</em>?</li><li><em>Space complexity</em> of a <em>Dict</em>?</li></ol><div class="notes"><ol><li>O(n**2)</li><li>O(log2 n) (both)</li><li>O(n) vs O(1)</li><li>O(n) vs O(1)</li><li>O(1) and O(n) (but much more expensive than an array index)</li><li>O(n)</li></ol><p>Makes you wonder why you don't use hash maps all the time?
Indeed they are a wonderful invention, but:</p><ul><li>get is still much more expensive than an array index.</li><li>collisions can happen, making things inefficient.</li><li>range queries and sorting are impossible.</li><li>self balancing trees have O(log n) for get/set but are stable.</li></ul></div></div><div class="step step-level-1" step="29" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="75000" data-y="0" data-z="0"><h1 id="data-structures-lecture">&lt;/Data structures lecture&gt;</h1><img src="images/book_algorithm.png" width="50%"></img><p>That's all. Go and remember a list of:</p><ul><li>Sorting algorithms (+ external sorting)</li><li>Common &amp; some specialized data structures.</li><li>Typical algorithms like binary search.</li><li><strong>How much space common types use.</strong></li><li>Levenshtein, Graphs, Backtracking, ...</li><li>...whatever is of interest to you.</li></ul><div class="notes"><p>Data structures and algorithms is something you gonna have to learn yourself.
Would totally go over the scope of this workshop and does not work as frontal lecture.</p><p>Do not ignore primitive algorithms like bubble sort.
Remember: Fancy algorithms are slow when n is small, and n is usually small.</p></div></div><div class="step step-level-1" step="30" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="77500" data-y="0" data-z="0"><h1 id="performance-metrics">Performance metrics</h1><p>Automated tests that assert the <em>performance requirements</em> of a piece of code
by computing <strong>performance metrics</strong> and...</p><ul><li>...either plot them for human consumption.</li><li>...compare against old versions.</li><li>...compare against constant thresholds.</li></ul><div class="notes"><p>Collect possible performance metrics (unit in parans):</p><ul><li>Execution time (time, cpu cycles)</li><li>Latency (time)</li><li>Throughput (IO, bytes/sec)</li><li>Memory (allocations, peak, total bytes)</li></ul><p>NOTE: Execution is heavily tied to hardware.</p><p>For CI/CD tools you can use something like this:</p><p><a href="https://github.com/dandavison/chronologer">https://github.com/dandavison/chronologer</a></p><p>In an ideal world, performance requirements are tested just like
normal functional requirements.</p><p>Challenges:</p><ul><li>Different machines that benchmarks run on.</li><li>Only comparison between releases makes sense.</li></ul><p>Makes sense only for big projects. Many projects have
their own set of scripts to do this. I'm not aware of a standard solution.</p></div></div><div class="step step-level-1" step="31" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="80000" data-y="0" data-z="0"><h1 id="humans-vs-magnitudes">Humans vs Magnitudes</h1><p><a href="https://colin-scott.github.io/personal_website/research/interactive_latency.html">Interactive Latency Visualization</a></p><p><strong>Optimize bottom up:</strong></p><img src="images/cache_pyramid.jpg" width="100%"></img><div class="notes"><p>Network is far below that.</p></div></div><div class="step step-level-1" step="32" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="82500" data-y="0" data-z="0"><h1 id="profiling">Profiling</h1><pre class="highlight code bash"><span class="c1"># Just the total time is already helpful.
</span>$<span class="w"> </span><span class="nb">time</span><span class="w"> </span>&lt;some-command&gt;<span class="w">

</span><span class="c1"># Better: With statistics.
</span>$<span class="w"> </span>hyperfine<span class="w"> </span>&lt;some-command&gt;</pre><div class="notes"><p>Profiling is usually used for finding a bottleneck.
Basically a throw away benchmark, like a non-automated, manual test.</p><p>So most of the time the terms can be used interchangeably.</p><ul><li>Run several times.</li><li>If the variance is not big, take the maximum.</li><li>If the variance is rather large, use min...max.</li></ul></div></div><div class="step step-level-1" step="33" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="85000" data-y="0" data-z="0"><h1 id="sideproject">Sideproject</h1><p class="quote">What I cannot create, I do not understand.</p><div class="line-block">(Richard Feynman)<br></br></div><div class="notes"><p>Words don't cut it. To understand something you have to lay your hands on something
and start exploring. Workshop is about tacit knowledge, you have to connect the little dots
on my slides by working on this small slide project. I can only show you things, not understand and
learn it for you.</p><p>tacit = unausgeprochen</p><p>I will share a sort of reference implementation some time after the workshop. There is no one right
solution, but I will try to keep my solution well understandable and documented.</p></div></div><div class="step step-level-1" step="34" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="87500" data-y="0" data-z="0"><h1 id="memory-only-1">Memory only #1</h1><pre class="highlight code go"><span class="kd">type</span><span class="w"> </span><span class="nx">KV</span><span class="w"> </span><span class="kd">map</span><span class="p">[</span><span class="kt">string</span><span class="p">][]</span><span class="kt">byte</span><span class="w">

</span><span class="kd">func</span><span class="w"> </span><span class="p">(</span><span class="nx">kv</span><span class="w"> </span><span class="o">*</span><span class="nx">KV</span><span class="p">)</span><span class="w"> </span><span class="nx">Get</span><span class="p">(</span><span class="nx">key</span><span class="w"> </span><span class="kt">string</span><span class="p">)</span><span class="w"> </span><span class="p">[]</span><span class="kt">byte</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="k">return</span><span class="w"> </span><span class="nx">kv</span><span class="p">[</span><span class="nx">key</span><span class="p">]</span><span class="w">
</span><span class="p">}</span><span class="w">

</span><span class="kd">func</span><span class="w"> </span><span class="p">(</span><span class="nx">kv</span><span class="w"> </span><span class="o">*</span><span class="nx">KV</span><span class="p">)</span><span class="w"> </span><span class="nx">Set</span><span class="p">(</span><span class="nx">key</span><span class="w"> </span><span class="kt">string</span><span class="p">,</span><span class="w"> </span><span class="nx">val</span><span class="w"> </span><span class="p">[]</span><span class="kt">byte</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="k">return</span><span class="w"> </span><span class="nx">kv</span><span class="p">[</span><span class="nx">key</span><span class="p">]</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">val</span><span class="w">
</span><span class="p">}</span></pre></div><div class="step step-level-1" step="35" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="90000" data-y="0" data-z="0"><h1 id="memory-only-2">Memory only #2</h1><pre class="highlight code go"><span class="kd">func</span><span class="w"> </span><span class="p">(</span><span class="nx">kv</span><span class="w"> </span><span class="o">*</span><span class="nx">KV</span><span class="p">)</span><span class="w"> </span><span class="nx">sync</span><span class="p">()</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="kd">var</span><span class="w"> </span><span class="nx">b</span><span class="w"> </span><span class="nx">bytes</span><span class="p">.</span><span class="nx">Buffer</span><span class="w">
    </span><span class="k">for</span><span class="w"> </span><span class="nx">k</span><span class="p">,</span><span class="w"> </span><span class="nx">v</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="k">range</span><span class="w"> </span><span class="nx">kv</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="nx">b</span><span class="p">.</span><span class="nx">WriteString</span><span class="p">(</span><span class="nx">fmt</span><span class="p">.</span><span class="nx">Sprintf</span><span class="p">(</span><span class="s">"%s=%s\n"</span><span class="p">,</span><span class="w"> </span><span class="nx">k</span><span class="p">,</span><span class="w"> </span><span class="nx">v</span><span class="p">))</span><span class="w">
    </span><span class="p">}</span><span class="w">
    </span><span class="k">return</span><span class="w"> </span><span class="nx">ioutil</span><span class="p">.</span><span class="nx">WriteFile</span><span class="p">(</span><span class="s">"/blah"</span><span class="p">,</span><span class="w"> </span><span class="nx">b</span><span class="p">.</span><span class="nx">Bytes</span><span class="p">(),</span><span class="w"> </span><span class="mo">0644</span><span class="p">)</span><span class="w">
</span><span class="p">}</span><span class="w">

</span><span class="kd">func</span><span class="w"> </span><span class="nx">load</span><span class="p">()</span><span class="w"> </span><span class="o">*</span><span class="nx">KV</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nx">data</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">ioutil</span><span class="p">.</span><span class="nx">ReadFile</span><span class="p">(</span><span class="s">"/blah"</span><span class="p">)</span><span class="w">
    </span><span class="c1">// ... parse file and assign to map ...</span><span class="w">
    </span><span class="k">return</span><span class="w"> </span><span class="nx">kv</span><span class="w">
</span><span class="p">}</span></pre><div class="notes"><p>Obvious issue: It's all in the memory. If you need more entries than you have RAM
you're in for a bad time.</p><p>You could use a big in-memory hash table and sync that to disk sometimes.</p><p>When do you call sync()? After every write? Inefficient.
Less often? Then you will suffer data loss on power loss or crash.</p><p>Sounds impractical, but surprise: Redis actually works this way. They do not
use a hash map internally though, but a tree structure as index. Oh, and
they perform most work in a single thread. Still fast, but you have to
consider its drawbacks.</p><p>Technical detail: Redis relies on the OS' paging mechanism, assuming that not every key
in the database is used all the time. This allows to allocate a lot of memory, but to let
the OS do the heavy lifting in the background to actually use a small portion of it only.
This will be covered more in the memory chapter under "virtual memory".</p></div></div><div class="step step-level-1" step="36" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="92500" data-y="0" data-z="0"><h1 id="append-only">Append only</h1><pre class="highlight code bash">init<span class="o">()</span><span class="w"> </span><span class="o">{</span><span class="w">
    </span>touch<span class="w"> </span>./db<span class="w">
</span><span class="o">}</span><span class="w">

</span>set<span class="o">()</span><span class="w"> </span><span class="o">{</span><span class="w">
    </span><span class="nb">printf</span><span class="w"> </span><span class="s2">"%s=%s\n"</span><span class="w"> </span><span class="s2">"</span><span class="nv">$1</span><span class="s2">"</span><span class="w"> </span><span class="s2">"</span><span class="nv">$2</span><span class="s2">"</span><span class="w"> </span>&gt;&gt;<span class="w"> </span>./db<span class="w">
</span><span class="o">}</span><span class="w">

</span>get<span class="o">()</span><span class="w"> </span><span class="o">{</span><span class="w">
    </span>grep<span class="w"> </span><span class="s2">"^</span><span class="nv">$1</span><span class="s2">="</span><span class="w"> </span>./db<span class="w"> </span><span class="p">|</span><span class="w"> </span>tail<span class="w"> </span>-1<span class="w"> </span><span class="p">|</span><span class="w"> </span>cut<span class="w"> </span>-d<span class="o">=</span><span class="w"> </span>-f2-<span class="w">
</span><span class="o">}</span></pre><div class="notes"><p>Simple append only write, get() reads only the last value.
Every update of an existing key writes it again.</p><p>Terribly slow because get needs to scan the whole db, but
very easy to implement and set is pretty fast. If you hardly
ever call get then this might be a viable solution.</p></div></div><div class="step step-level-1" step="37" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="95000" data-y="0" data-z="0"><h1 id="indexed">Indexed</h1><pre class="highlight code go"><span class="kd">type</span><span class="w"> </span><span class="nx">KV</span><span class="w"> </span><span class="kd">map</span><span class="p">[</span><span class="kt">string</span><span class="p">]</span><span class="kt">int64</span><span class="w">

</span><span class="kd">func</span><span class="w"> </span><span class="p">(</span><span class="nx">kv</span><span class="w"> </span><span class="o">*</span><span class="nx">KV</span><span class="p">)</span><span class="w"> </span><span class="nx">Set</span><span class="p">(</span><span class="nx">key</span><span class="w"> </span><span class="kt">string</span><span class="p">,</span><span class="w"> </span><span class="nx">val</span><span class="w"> </span><span class="p">[]</span><span class="kt">byte</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="c1">// 1. Build entry with key and value.</span><span class="w">
    </span><span class="c1">// 2. Append entry to end of db file.</span><span class="w">
    </span><span class="c1">// 3. Update key-value index with offset.</span><span class="w">
</span><span class="p">}</span><span class="w">

</span><span class="kd">func</span><span class="w"> </span><span class="p">(</span><span class="nx">kv</span><span class="w"> </span><span class="o">*</span><span class="nx">KV</span><span class="p">)</span><span class="w"> </span><span class="nx">Get</span><span class="p">(</span><span class="nx">key</span><span class="w"> </span><span class="kt">string</span><span class="p">)</span><span class="w"> </span><span class="p">[]</span><span class="kt">byte</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="c1">// 1. Get offset &amp; seek to it.</span><span class="w">
    </span><span class="c1">// 2. Read value from db file at offset.</span><span class="w">
</span><span class="p">}</span></pre><div class="notes"><p>This is actually already quite nice and I would be happy if you guys can
implement it like that already.</p><p>This approach is called "log structured", because values are handled
like a stream of logs, just timestamped (or offset stamped) data.</p><p>We can handle any number of values as long as we do not run out of memory.
If we throw in a little caching, we could probably get decent performance.
This would also be a decent usage for something called mmap which we will
look into later in this series.</p><p>When loading the db file, we can reconstruct the index map easily.</p><p>Problems:</p><ul><li>There will be many duplicates if we update the same keys over and over.</li><li>The database file will grow without bound. Might turn out problematic.</li><li>There may only be one writer at a point (race condition between size of db
and actual write).</li></ul></div></div><div class="step step-level-1" step="38" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="97500" data-y="0" data-z="0"><h1 id="segmented">Segmented</h1><img src="diagrams/1_segments.svg" width="100%"></img><div class="notes"><p>Solution:</p><ol><li>If the db file gets too big (&gt; 32M), start a new one.</li><li>Old one gets compacted in background (i.e. duplicates get removed)</li><li>Index structure remembers what file we need to read.</li></ol><p>The compaction step can be easily done in the background.</p><p>Open issues:</p><ul><li>We still need to have all keys in memory.</li><li>Range queries are kinda impossible.</li><li>We can't delete stuff.</li></ul><p>TODO(FEEDBACK): Insert another step here to explain that we can replace the map ("memory segment") with a btree. Also explain roughly what a btree is.</p></div></div><div class="step step-level-1" step="39" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="100000" data-y="0" data-z="0"><h1 id="deletion">Deletion</h1><img src="images/tombstones.png" width="50%"></img><div class="notes"><p>When we want to delete something, we just write a special value
that denotes that this key was deleted. If a tombstone is the last
value then the key is gone. Compaction can use it to clean up old
traces of that value.</p><p>At this point we already build a key value store that is used out there: Bitcask.</p></div></div><div class="step step-level-1" step="40" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="102500" data-y="0" data-z="0"><h1 id="range-queries">Range queries</h1><img src="diagrams/1_lsm.svg" width="100%"></img><div class="notes"><blockquote><p>Change approach quite a bit:</p><ol><li>Keep a batch of key-value pairs in memory, but sorted by key.</li><li>If batch gets too big, then swap to disk.</li><li>Keep every 100th key in the offset index.</li><li>If key not in index, go to file and scan the range.</li></ol></blockquote><p>This technique is called a Log-Structured-Merge tree (LSM).</p><p>"tree" because usually a tree is used instead of a hash table for easy handling,
but this is not strictly necessary and the main point of the concept.</p><p>Since the index can be "sparse" (not all keys need to be stored), we have very
fine grained control over memory usage. Worst thing is a bit of extra scanning
in the file.</p><p>Open problems:</p><ul><li>Get on non-existing keys.</li><li>Crash safety</li></ul></div></div><div class="step step-level-1" step="41" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="105000" data-y="0" data-z="0"><h1 id="wal">WAL &#x1F40B;</h1><img src="diagrams/1_wal.svg" width="100%"></img><div class="notes"><p>What if a crash occurs before things get written to disk?</p><p>We have to use a WAL like above! On a crash we can reconstruct the memory index from it.
Postgres and many other databases make use of this technique too.</p></div></div><div class="step step-level-1" step="42" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="107500" data-y="0" data-z="0"><h1 id="fynn">Fynn!</h1><div class="line-block"><br></br></div><p class="big-text">&#x1F3C1;</p><div class="line-block"><br></br></div><div class="notes"><p>I left quite some details out, but that's something you should be able to figure out.</p></div><p class="next-link"><strong>Next:</strong> <a href="../2_cpu/index.html">CPU</a>: The secrets of the computer &#x1F9E0;</p></div><div class="step step-level-1" step="43" data-x="110000" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-y="0" data-z="0"><p class="chapter">CPU</p><p>The secrets of the computer &#x1F9E0;</p></div><div class="step step-level-1" step="44" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="112500" data-y="0" data-z="0"><h1 id="agenda-1">Agenda</h1><ul><li>How to get from source to machine code?</li><li>How does the CPU execute machine code?</li><li>What performance effects does this have?</li><li>Profiling &amp; Benchmarking thoughts &amp; tips</li></ul><img src="images/cpu.jpg" width="50%"></img></div><div class="step step-level-1" step="45" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="115000" data-y="0" data-z="0"><h1 id="quiz">Quiz</h1><ol><li>If two CPUs have the same frequency, can we make assumptions on their speed?</li><li>If two programs <em>A</em> and <em>B</em> execute the same number of machine instructions will they have roughly the same runtime?</li></ol><div class="notes"><p>Those are the questions we will be looking into in detail today.
Here's the TL;DR:</p><ol><li>Answer no.
Every instruction can take a different amount of cpu cycles.
Every instruction can do a lot of different work (SIMD vs normal)</li><li>Also no.
Speed of a CPU largely relies on many many factors (#core, cache size, ...)
The frequency also did not increase much over the years since CPUs get
manufactured much smaller, causing heat issues with higher freqs.</li></ol></div></div><div class="step step-level-1" step="46" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="117500" data-y="0" data-z="0"><h1 id="compilers">Compilers</h1><img src="diagrams/2_compilers.svg" width="140%"></img><div class="notes"><p>Steps to compile something:</p><ul><li>Lexer/Tokenizer (break code in tokens)</li><li>Parser (build AST from code)</li><li>High Level IR (build generic language from it)</li><li>Low level IR (optimize and make it suitable for machines)</li><li>Convert to actual target machine code</li></ul></div></div><div class="step step-level-1" step="47" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="120000" data-y="0" data-z="0"><h1 id="fun-fact-supercompilers">Fun fact: Supercompilers</h1><img src="images/supercompiler.png"></img><div class="notes"><ul><li>Compilers do not usually produce the best code and rely heavily on pattern matching, heuristics
and just being smart. They can miss room for optimizations although this is rather rare in practice.
(except Go, which is just a developing compiler)</li><li>Super compilers brute force compilation (sometimes with benchmarks) until they found the best performing
piece of code.</li><li>Not used in practice, since freaking slow but helpful for developing new compiler optimizations.</li></ul><p>As you will see in the rest of the workshop,
70% of optimization is to help the compiler
make the right decisions.</p><p>STOKE: <a href="https://github.com/StanfordPL/stoke">https://github.com/StanfordPL/stoke</a></p></div></div><div class="step step-level-1" step="48" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="122500" data-y="0" data-z="0"><h1 id="how-is-code-executed">How is code executed?</h1><img src="diagrams/2_assembler.svg" width="120%"></img><div class="notes"><ul><li>Assembly: 1:1 human readable interpretation of machine code.</li><li>Machine code: machine readable instructions (each instruction has an id)</li><li>Assembler: Program that converts assembly to machine code.</li></ul></div></div><div class="step step-level-1" step="49" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="125000" data-y="0" data-z="0"><h1 id="other-terminology">Other terminology</h1><ul><li><strong>ISA:</strong> Instruction Set Architecture (<tt>x86</tt>, <tt>arm</tt>, ...)</li><li><strong>CISC</strong> Complex Instruction Set Computer (<tt>x86</tt>)</li><li><strong>RISC:</strong> Reduced Instruction Set Computer (<tt>arm</tt>)</li><li><strong>SIMD:</strong> Single Instruction, Multiple Data</li><li><strong>ISE:</strong> Instruction Set Extensions (<tt>AVX</tt>, <tt>AES</tt>, <tt>SSE</tt>...)</li><li>Micro{architecture,code} (<tt>Pentium3</tt>, <tt>Alder Lake</tt>, <tt>Zen</tt>...)</li></ul><div class="notes"><p>Example of a CISC instruction set: x86
Today, most complex operations get translated to RISC code though by the CPU.
CISC turned out to be slower, surprisingly.</p><p>RISC: ARM. Usually cheaper to build and also faster.</p><p>Microarchitecture: Implementation of a certain ISA.</p><p>ISE (Instruction Set Extensions) are not directly available in Go, only if the compiler decides to use them.</p></div></div><div class="step step-level-1" step="50" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="127500" data-y="0" data-z="0"><h1 id="how-is-machine-code-stored">How is machine code stored?</h1><p>As ELF (Executable and Linkable Format)</p><pre class="highlight code bash">$<span class="w"> </span>readelf<span class="w"> </span>--sections<span class="w"> </span>/usr/bin/ls<span class="w">
</span><span class="o">[</span>...<span class="o">]</span><span class="w">
</span><span class="o">[</span><span class="m">12</span><span class="o">]</span><span class="w"> </span>.text<span class="w">             </span>PROGBITS<span class="w">
</span><span class="o">[</span>...<span class="o">]</span><span class="w">
</span><span class="o">[</span><span class="m">22</span><span class="o">]</span><span class="w"> </span>.data<span class="w">             </span>PROGBITS<span class="w">
</span>$<span class="w"> </span>objdump<span class="w"> </span>--disassemble<span class="w"> </span>/usr/bin/ls</pre><div class="notes"><p>Beside storing the actual instructions ELF solves:</p><ul><li>Storing debugging info</li><li>Making it possible to link with existing other libraries.</li><li>Includes a text (code) and data section (pre-initialized variables)</li><li>Different OS use different formats, but ELF is probably the most relevant for you
and also the most widely known. Windows has a different one.</li></ul></div></div><div class="step step-level-1" step="51" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="130000" data-y="0" data-z="0"><h1 id="go-assembler-1">Go Assembler #1</h1><pre class="highlight code go"><span class="ln"> 1 </span><span class="w"> </span><span class="kn">package</span><span class="w"> </span><span class="nx">main</span><span class="w">
</span><span class="ln"> 2 </span><span class="w">
</span><span class="ln"> 3 </span><span class="w"> </span><span class="c1">//go:noinline</span><span class="w">
</span><span class="ln"> 4 </span><span class="w"> </span><span class="kd">func</span><span class="w"> </span><span class="nx">add</span><span class="p">(</span><span class="nx">a</span><span class="p">,</span><span class="w"> </span><span class="nx">b</span><span class="w"> </span><span class="kt">int</span><span class="p">)</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="p">{</span><span class="w">
</span><span class="ln"> 5 </span><span class="w">     </span><span class="k">return</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nx">b</span><span class="w">
</span><span class="ln"> 6 </span><span class="w"> </span><span class="p">}</span><span class="w">
</span><span class="ln"> 7 </span><span class="w">
</span><span class="ln"> 8 </span><span class="w"> </span><span class="kd">func</span><span class="w"> </span><span class="nx">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span><span class="w">
</span><span class="ln"> 9 </span><span class="w">     </span><span class="nx">add</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">)</span><span class="w">
</span><span class="ln">10 </span><span class="w"> </span><span class="p">}</span></pre><div class="notes"><p>The official Go compiler is not based on LLVM or GCC.
However, it also uses a IR which it calls "Go assembler".
It's basically an assembler like dialect for a fantasy CPU.
After it was optimized, it gets translated to actual target machine code.</p></div></div><div class="step step-level-1" step="52" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="132500" data-y="0" data-z="0"><h1 id="go-assembler-2">Go Assembler #2</h1><pre class="highlight code bash">$<span class="w"> </span>go<span class="w"> </span>build<span class="w"> </span>-gcflags<span class="o">=</span><span class="s2">"-S"</span><span class="w"> </span>add.go<span class="w">
</span><span class="o">[</span>...<span class="o">]</span><span class="w">
</span>main.add<span class="w"> </span>STEXT<span class="w"> </span>nosplit<span class="w"> </span><span class="nv">size</span><span class="o">=</span><span class="m">4</span><span class="w"> </span><span class="o">[</span>...<span class="o">]</span><span class="w">
  </span><span class="o">(</span>test.go:4<span class="o">)</span><span class="w"> </span>TEXT<span class="w">   </span>main.add<span class="o">(</span>SB<span class="o">)</span>,<span class="w"> </span><span class="o">[</span>...<span class="o">]</span><span class="w">
  </span><span class="o">(</span>test.go:4<span class="o">)</span><span class="w"> </span>PCDATA<span class="w"> </span><span class="nv">$3</span>,<span class="w"> </span><span class="nv">$1</span><span class="w">
  </span><span class="o">(</span>test.go:5<span class="o">)</span><span class="w"> </span>ADDQ<span class="w">   </span>BX,<span class="w"> </span>AX<span class="w">
  </span><span class="o">(</span>test.go:5<span class="o">)</span><span class="w"> </span>RET<span class="w">
</span><span class="o">[</span>...<span class="o">]</span><span class="w">
</span>main.main<span class="w"> </span>STEXT<span class="w"> </span><span class="nv">size</span><span class="o">=</span><span class="m">121</span><span class="w"> </span><span class="o">[</span>...<span class="o">]</span><span class="w">
</span><span class="o">[</span>...<span class="o">]</span><span class="w">
  </span><span class="o">(</span>test.go:9<span class="o">)</span><span class="w"> </span>MOVL<span class="w"> </span><span class="nv">$2</span>,<span class="w"> </span>AX<span class="w">
  </span><span class="o">(</span>test.go:9<span class="o">)</span><span class="w"> </span>MOVL<span class="w"> </span><span class="nv">$3</span>,<span class="w"> </span>BX<span class="w">
  </span><span class="o">(</span>test.go:9<span class="o">)</span><span class="w"> </span>CALL<span class="w"> </span>main.add<span class="o">(</span>SB<span class="o">)</span><span class="w">
  </span><span class="c1"># result is in AX</span></pre><p><a href="https://go.dev/doc/asm">https://go.dev/doc/asm</a></p><div class="notes"><p>Important: There are many assembler dialects for many ISA. This is a IR.
Also Important: Explain registers!</p><p>Can we just say: To make things faster you have to reduce the number of instructions?</p><p>Sadly no. Modern CPUs are MUCH complexer than machines that sequentially execute instructions.
They take all kind of shortcuts to execute things faster - most of the time.
See also: Megaherz myth (-&gt; higher clock = more cycles per time)</p><p>Effects that may play a role</p><ul><li>Not every instruction takes the same amount of cycles (AND = 1 cycle, MOV = 1-6+)</li><li>Pipelining</li><li>Superscalar Execution</li><li>Branch prediction / Cache prefetching</li><li>Out-of-order execution</li><li>Cache misses (fetching from main memory)</li></ul><p>List of typical cycles per instructions ("latency"): <a href="https://www.agner.org/optimize/instruction_tables.pdf">https://www.agner.org/optimize/instruction_tables.pdf</a></p></div></div><div class="step step-level-1" step="53" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="135000" data-y="0" data-z="0"><h1 id="von-neumann-architecture">Von-Neumann Architecture</h1><img src="images/vn_cpu.png" width="100%"></img><div class="notes"><p>Von Neumann Computer: Memory contains data and code.
CPU adresses memory as whole and can address I/O device the same way
over a bus system.</p><p>Greatly simplified.</p><ul><li>Clocked with a certain frequency.</li><li>A cycle is the basic work synchronization.</li><li>Registers for internal usage. (CPUs have more than x86 says)</li><li>Peripherals look to the CPU like memory.</li></ul><p>Intel 8086 kinda worked this way.</p></div></div><div class="step step-level-1" step="54" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="137500" data-y="0" data-z="0"><h1 id="single-instruction-pipeline">Single instruction pipeline</h1><img src="images/pipeline.png" width="70%"></img><ol><li><strong>Load:</strong> Instruction gets loaded (<tt>0x012345</tt>)</li><li><strong>Decode:</strong> Check type/args of instruction.</li><li><strong>Memory:</strong> Load data from memory (if necessary)</li><li><strong>Execute:</strong> Calculate (e.g. add 2+3 in the ALU)</li><li><strong>Write back:</strong> Save result in some register.</li></ol><div class="notes"><p>The location of the next instruction is stored in the PC register.
(program counter)</p><p>This would need 5 cycles per instruction.
You kinda assumed, that one cycle is one instruction, did you?</p></div></div><div class="step step-level-1" step="55" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="140000" data-y="0" data-z="0"><h1 id="pipelining-ooo-superscalar-wtf">Pipelining, OoO, Superscalar, wtf?</h1><ul><li><strong>Pipelining:</strong> The 5 steps get done in parallel.</li><li><strong>Out-of-Order:</strong>  Instructions get re-ordered.</li><li><strong>Superscalar:</strong> Several instructions per cycle (~5x)</li></ul><p><em>Ergo:</em></p><ul><li>1 Cycle &#x2260; 1 instruction.</li><li>CPU might do unnecessary work!</li><li>Reducing instructions alone does not get us far.</li></ul><div class="notes"><ul><li>Every instruction needs to do all 5 steps</li><li>Modern CPUs can work on many instructions at the same time</li><li>They can be also re-ordered by the CPU! (think of a queue that gets reordered)</li><li>This can lead to issues when an instruction depends on results of another instructions! (branches!)</li><li>It can even happen that we do unncessary work!
This made the SPECTRE and MELTDOWN security issues possible that made cloud computing 20% slower "over night" (well, it got better after a few months)</li><li>CPUs can also execute more than one instruction per cycle (e.g. one MOV, ADD, CMP, as they all use different parts of the CPU)
(Superscalar CPUs)</li><li>This is the reason why focussing on reducing the number of instructions alone is not
too helpful when optimizing.</li></ul><p><a href="https://de.wikipedia.org/wiki/Pipeline_(Prozessor">https://de.wikipedia.org/wiki/Pipeline_(Prozessor</a>)</p></div></div><div class="step step-level-1" step="56" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="142500" data-y="0" data-z="0"><h1 id="disclaimer-cpu-effects">Disclaimer: CPU effects</h1><ul><li>Modern CPUs are insanely complex (like humans).</li><li>Compilers are insanely smart (unlike humans).</li><li>This tandem is probably smarter than you and me.
The following slides are mostly for educational purpose.
Trust the compiler in 95% of the time.</li><li>Still helpful to know what happens in the 5%.</li></ul></div><div class="step step-level-1" step="57" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="145000" data-y="0" data-z="0"><h1 id="branch-prediction">Branch prediction</h1><pre class="highlight code c"><span class="c1">// NOTE: works only in C/C++
</span><span class="k">if</span><span class="p">(</span><span class="n">likely</span><span class="p">(</span><span class="n">a</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">1</span><span class="p">))</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="c1">// ...
</span><span class="p">}</span><span class="w">

</span><span class="c1">// Branch mis-prediction are very costly!
// ~20 - ~35 cycles can be lost per miss.
</span><span class="k">if</span><span class="p">(</span><span class="n">unlikely</span><span class="p">(</span><span class="n">err</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">0</span><span class="p">))</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="c1">// ...
</span><span class="p">}</span></pre><div class="notes"><p>Modern cpus guess what branch is taken due to pipelining. The accuracy is done to 96%,
they even use neural networks for that. Branch mispredictions are costly: Roughly 10-30 cycles lost.</p><p>No likely() in Go, compiler tries to insert those hints automayically.
Not much of an important optimization nowadays though as CPUs get a lot better:</p><p><a href="https://de.wikipedia.org/wiki/Sprungvorhersage">https://de.wikipedia.org/wiki/Sprungvorhersage</a></p><p>(but can be relevant for very hot paths on cheap ARM cpus)</p><p>Penalty Source: <a href="https://users.elis.ugent.be/~leeckhou/papers/ispass06-eyerman.pdf">https://users.elis.ugent.be/~leeckhou/papers/ispass06-eyerman.pdf</a></p></div></div><div class="step step-level-1" step="58" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="147500" data-y="0" data-z="0"><h1 id="can-we-observe-it">Can we observe it?</h1><pre class="highlight code go"><span class="c1">// Which loop runs faster?</span><span class="w">
</span><span class="k">for</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="nx">i</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="nx">i</span><span class="w"> </span><span class="p">&lt;</span><span class="w"> </span><span class="nx">N</span><span class="p">;</span><span class="w"> </span><span class="nx">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="nx">unsorted</span><span class="p">[</span><span class="nx">i</span><span class="p">]</span><span class="w"> </span><span class="p">&lt;</span><span class="w"> </span><span class="nx">X</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="nx">sum</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="nx">unsorted</span><span class="p">[</span><span class="nx">i</span><span class="p">];</span><span class="w">
    </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">
</span><span class="k">for</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="nx">i</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="nx">i</span><span class="w"> </span><span class="p">&lt;</span><span class="w"> </span><span class="nx">N</span><span class="p">;</span><span class="w"> </span><span class="nx">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="nx">sorted</span><span class="p">[</span><span class="nx">i</span><span class="p">]</span><span class="w"> </span><span class="p">&lt;</span><span class="w"> </span><span class="nx">X</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="nx">sum</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="nx">sorted</span><span class="p">[</span><span class="nx">i</span><span class="p">];</span><span class="w">
    </span><span class="p">}</span><span class="w">
</span><span class="p">}</span></pre><p class="example">Example: code/branchpredict</p><div class="notes"><p>Effect is unnotice-able if optimizations are enabled.
Why? Compilers can make the inner branch a branchless statement.</p></div></div><div class="step step-level-1" step="59" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="150000" data-y="0" data-z="0"><h1 id="profile-guided-optimization">Profile Guided Optimization</h1><img src="images/pgo.png" width="80%"></img><div class="notes"><p>Idea:</p><ul><li>Let program run in analysis mode.</li><li>Capture data about what branches were hit how often.</li><li>Use this data on the next compile to decide which branch is likely!</li></ul><p>Feature is available as part of Go 1.20
and since around 20 years as part of GCC/clang</p><p>Also decides on where to inline functions.</p><p><a href="https://tip.golang.org/doc/pgo">https://tip.golang.org/doc/pgo</a></p><p>Old news for languages like C.</p></div></div><div class="step step-level-1" step="60" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="152500" data-y="0" data-z="0"><h1 id="branchless-programming">Branchless programming</h1><pre class="highlight code c"><span class="c1">// Don't optimize this at home, kids.
// Your compiler does this for you.
</span><span class="kt">uint32_t</span><span class="w"> </span><span class="nf">max</span><span class="p">(</span><span class="kt">uint32_t</span><span class="w"> </span><span class="n">a</span><span class="p">,</span><span class="w"> </span><span class="kt">uint32_t</span><span class="w"> </span><span class="n">b</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="k">if</span><span class="p">(</span><span class="n">a</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="n">b</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="k">return</span><span class="w"> </span><span class="n">a</span><span class="p">;</span><span class="w">
    </span><span class="p">}</span><span class="w">
    </span><span class="k">return</span><span class="w"> </span><span class="n">b</span><span class="p">;</span><span class="w">
</span><span class="p">}</span></pre><pre class="highlight code c"><span class="c1">// variant 1; not possible in Go:
</span><span class="k">return</span><span class="w"> </span><span class="p">(</span><span class="n">a</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="n">b</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="o">!</span><span class="p">(</span><span class="n">a</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="n">b</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">b</span><span class="p">;</span><span class="w">

</span><span class="c1">// variant 2; possible in Go:
</span><span class="k">return</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="p">(</span><span class="n">a</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">b</span><span class="p">)</span></pre><div class="notes"><p>Not relevant, as the compiler will optimize this for you in most cases
by using branchless code.</p><p>It can be however a life safer in hot loops if the compiler does not know.
Always check the assembly output if unsure.</p><p><a href="https://gcc.godbolt.org/#%7B%22version%22%3A3%2C%22filterAsm%22%3A%7B%22labels%22%3Atrue%2C%22directives%22%3Atrue%2C%22commentOnly%22%3Atrue%2C%22intel%22%3Atrue%2C%22colouriseAsm%22%3Atrue%7D%2C%22compilers%22%3A%5B%7B%22source%22%3A%22%23include%20%3Calgorithm%3E%5Cnint%20max%28int%20x%2C%20int%20y%29%20%7B%5Cn%20%20return%20std%3A%3Amax%28x%2Cy%29%3B%5Cn%7D%5Cn%22%2C%22compiler%22%3A%22%2Fusr%2Fbin%2Fg%2B%2B-4.7%22%2C%22options%22%3A%22-O2%20-m32%20-march%3Dnative%22%7D%5D%7D">https://gcc.godbolt.org/#%7B%22version%22%3A3%2C%22filterAsm%22%3A%7B%22labels%22%3Atrue%2C%22directives%22%3Atrue%2C%22commentOnly%22%3Atrue%2C%22intel%22%3Atrue%2C%22colouriseAsm%22%3Atrue%7D%2C%22compilers%22%3A%5B%7B%22source%22%3A%22%23include%20%3Calgorithm%3E%5Cnint%20max%28int%20x%2C%20int%20y%29%20%7B%5Cn%20%20return%20std%3A%3Amax%28x%2Cy%29%3B%5Cn%7D%5Cn%22%2C%22compiler%22%3A%22%2Fusr%2Fbin%2Fg%2B%2B-4.7%22%2C%22options%22%3A%22-O2%20-m32%20-march%3Dnative%22%7D%5D%7D</a></p></div></div><div class="step step-level-1" step="61" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="155000" data-y="0" data-z="0"><h1 id="loop-unrolling-and-ilp">Loop unrolling and ILP</h1><p><em>ILP</em> = Instruction Level Parallelism</p><pre class="highlight code go"><span class="c1">// a loop is just a repeated if condition:</span><span class="w">
</span><span class="k">for</span><span class="w"> </span><span class="nx">idx</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="nx">idx</span><span class="w"> </span><span class="p">&lt;</span><span class="w"> </span><span class="mi">3</span><span class="p">;</span><span class="w"> </span><span class="nx">idx</span><span class="o">++</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nx">sum</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="nx">sin</span><span class="p">(</span><span class="nx">idx</span><span class="p">)</span><span class="w">
</span><span class="p">}</span><span class="w">

</span><span class="c1">// same, but no "idx &lt; 3" needed:</span><span class="w">
</span><span class="c1">// (can be computed in parallel!)</span><span class="w">
</span><span class="nx">sum</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="nx">sin</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="w">
</span><span class="nx">sum</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="nx">sin</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="w">
</span><span class="nx">sum</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="nx">sin</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span></pre><div class="notes"><ul><li>A for loop is just a repeated branch condition.</li><li>Compilers unroll simple loops.</li><li>If they don't hand unrolling can be useful (very seldom!)</li></ul><p>Example with interdependent code will not work as good:</p><pre class="highlight code go"><span class="nx">v</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="mi">1234</span><span class="w">
</span><span class="k">for</span><span class="w"> </span><span class="nx">v</span><span class="w"> </span><span class="p">&gt;</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nx">digit</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">v</span><span class="w"> </span><span class="o">%</span><span class="w"> </span><span class="mi">10</span><span class="w">
    </span><span class="nx">v</span><span class="w"> </span><span class="o">/=</span><span class="w"> </span><span class="mi">10</span><span class="w">
</span><span class="p">}</span></pre></div></div><div class="step step-level-1" step="62" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="157500" data-y="0" data-z="0"><h1 id="just-use-less-instructions">Just use less instructions?</h1><pre class="highlight code c"><span class="c1">// How to reduce the number of instructions?
</span><span class="kt">char</span><span class="w"> </span><span class="o">*</span><span class="nf">memcpy_basic</span><span class="p">(</span><span class="kt">char</span><span class="w"> </span><span class="o">*</span><span class="n">dst</span><span class="p">,</span><span class="w"> </span><span class="kt">char</span><span class="w"> </span><span class="o">*</span><span class="n">src</span><span class="p">,</span><span class="w"> </span><span class="kt">size_t</span><span class="w"> </span><span class="n">n</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="k">for</span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">n</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="n">dst</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">src</span><span class="p">[</span><span class="n">i</span><span class="p">];</span><span class="w">
    </span><span class="p">}</span><span class="w">
    </span><span class="k">return</span><span class="w"> </span><span class="n">dst</span><span class="p">;</span><span class="w">
</span><span class="p">}</span></pre><p class="example">Example: code/memcpy</p><div class="notes"><p>-&gt; Problem: von-Neumann-Bottleneck.
-&gt; CPU can work on data faster than typical RAM can deliver it.
-&gt; Workaround: Caches in the CPU, Prefetching.
-&gt; Actual solution: Data oriented design.
-&gt; Sequential access, tight packing of data, SIMD (and if you're crazy: DMA)
-&gt; Still best way to speed up copies: don't copy.</p><p>Object oriented design tends to fuck this up and many Games (at their core)
do not use OOP. You can use both at the same time though!</p></div></div><div class="step step-level-1" step="63" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="160000" data-y="0" data-z="0"><h1 id="simd">SIMD</h1><img src="images/simd.png" width="100%"></img><p><a href="https://github.com/mmcloughlin/avo">https://github.com/mmcloughlin/avo</a></p><div class="notes"><p>SISD = Single Instruction / Single Data
SIMD = Single Instruction / Multiple Data</p><p>Can be really worth the effort, since compilers can't figure out
all cases where SIMD can be used.</p><p>Example use cases:</p><ul><li>Image computation (i.e. changing brightness of several pixels at once)</li><li>Math operations like vector / matrix multiplications.</li><li>Audio/DSP processing.</li></ul><p>Disadvantage: Code gets ugly, hard to maintain and has additional obstacles
to solve like memory alignment. Also freaking complicated, which is why
we won't go into detail. Read up more here if you really want to:</p><p><a href="https://en.wikipedia.org/wiki/Single_instruction,_multiple_data">https://en.wikipedia.org/wiki/Single_instruction,_multiple_data</a></p></div></div><div class="step step-level-1" step="64" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="162500" data-y="0" data-z="0"><h1 id="optimization-inlining">Optimization: Inlining</h1><img src="diagrams/2_inlining.svg" width="130%"></img><div class="notes"><p>Inlining functions can speed up things at the cost of increased ELF size.</p><p>Advantage: Parameters do not need to get copied, but CPU can re-use whatever
is in the registers alreadys. Also return values do not need to be copied.</p><p>Only done for small functions and only in hot paths.</p></div></div><div class="step step-level-1" step="65" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="165000" data-y="0" data-z="0"><h1 id="i-like-to-mov-mov-it">I like to MOV, MOV it</h1><pre class="highlight code asm"><span class="c1"># General syntax:
# MOV &lt;dst&gt;,&lt;src&gt;
</span><span class="w">
</span><span class="c1"># Possible:
</span><span class="nf">MOV</span><span class="w"> </span><span class="no">reg1</span><span class="p">,</span><span class="w"> </span><span class="mi">1234</span><span class="w">
</span><span class="nf">MOV</span><span class="w"> </span><span class="no">reg1</span><span class="p">,</span><span class="w"> </span><span class="no">reg2</span><span class="w">
</span><span class="nf">MOV</span><span class="w"> </span><span class="no">reg1</span><span class="p">,</span><span class="w"> </span><span class="p">[</span><span class="mi">1234</span><span class="p">]</span><span class="w">
</span><span class="nf">MOV</span><span class="w"> </span><span class="p">[</span><span class="mi">1234</span><span class="p">],</span><span class="w"> </span><span class="no">reg1</span><span class="w">
</span><span class="nf">MOV</span><span class="w"> </span><span class="p">[</span><span class="no">reg2</span><span class="p">],</span><span class="w"> </span><span class="no">reg1</span><span class="w">

</span><span class="c1"># Not possible:
</span><span class="nf">MOV</span><span class="w"> </span><span class="p">[</span><span class="mi">1234</span><span class="p">],</span><span class="w"> </span><span class="p">[</span><span class="mi">4321</span><span class="p">]</span></pre><div class="notes"><p>How does access to main memory work? By using the MOV instruction.
And MOV from main memory is very costly:
Access to main memory is 125ns, L1 cache is ~1ns</p><p>Fun fact: MOV alone is Turing complete: <a href="https://github.com/xoreaxeaxeax/movfuscator">https://github.com/xoreaxeaxeax/movfuscator</a></p><p>TODO: Move this slide a bit before cache lines?</p></div></div><div class="step step-level-1" step="66" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="167500" data-y="0" data-z="0"><h1 id="von-neumann-bottleneck">von Neumann Bottleneck</h1><img src="diagrams/2_bottleneck.svg" width="100%"></img><div class="notes"><p>von Neumann Architektur:</p><ul><li>Computer Architecture where there is common memory accessible by all cores</li><li>Memory contains Data as well as code instructions</li><li>All data/code goes over a common bus</li><li>Pretty much all computer nowadays are build this way</li></ul><p>Bottleneck: Memory acess is much slower than CPUs can process the data.</p></div></div><div class="step step-level-1" step="67" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="170000" data-y="0" data-z="0"><h1 id="just-add-some-caches">Just add some caches!</h1><img src="images/whatcouldgowrong.jpeg"></img><div class="notes"><p>Good example of our industry really.</p><p>Instead of fixing an issue we wrap layers aorund it
until we just don't see the problem. But we never fix it.</p></div></div><div class="step step-level-1" step="68" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="172500" data-y="0" data-z="0"><h1 id="l1-l2-l3">L1, L2, L3</h1><img src="images/l1l2l3.png" width="70%"></img></div><div class="step step-level-1" step="69" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="175000" data-y="0" data-z="0"><h1 id="cache-lines-64b">Cache lines (64B)</h1><img src="diagrams/2_cache_line.svg" width="100%"></img><div class="notes"><p>Minimal line size is 64 byte!
It can only be written and evicted as one.
No partial reads or writes possible.
(Reason: adress space would be too big otherwise)</p><p>Some platforms have different cache lines and future CPUs might change too.
So instead on relying on the magic 64 you should use some const or cpu.CacheLinePad.</p></div></div><div class="step step-level-1" step="70" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="177500" data-y="0" data-z="0"><h1 id="caches-misses">Caches misses</h1><p class="example">Example: code/counter</p><pre class="highlight code bash"><span class="c1"># Use this to check your cache miss count:
</span>$<span class="w"> </span>perf<span class="w"> </span>stat<span class="w"> </span>-p<span class="w"> </span>&lt;PID&gt;</pre><div class="notes"><p><a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/monitoring_and_managing_system_status_and_performance/getting-started-with-perf_monitoring-and-managing-system-status-and-performance">https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/monitoring_and_managing_system_status_and_performance/getting-started-with-perf_monitoring-and-managing-system-status-and-performance</a>
<a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/monitoring_and_managing_system_status_and_performance/overview-of-performance-monitoring-options_monitoring-and-managing-system-status-and-performance">https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/monitoring_and_managing_system_status_and_performance/overview-of-performance-monitoring-options_monitoring-and-managing-system-status-and-performance</a></p></div></div><div class="step step-level-1" step="71" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="180000" data-y="0" data-z="0"><h1 id="struct-size-matters">(Struct) size matters!</h1><pre class="highlight code go"><span class="c1">// Quiz: How big is this struct?</span><span class="w">
</span><span class="kd">type</span><span class="w"> </span><span class="nx">XXX</span><span class="w"> </span><span class="kd">struct</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nx">A</span><span class="w"> </span><span class="kt">int64</span><span class="w">
    </span><span class="nx">B</span><span class="w"> </span><span class="kt">uint32</span><span class="w">
    </span><span class="nx">C</span><span class="w"> </span><span class="kt">byte</span><span class="w">
    </span><span class="nx">D</span><span class="w"> </span><span class="kt">bool</span><span class="w">
    </span><span class="nx">E</span><span class="w"> </span><span class="kt">string</span><span class="w">
    </span><span class="nx">F</span><span class="w"> </span><span class="p">[]</span><span class="kt">byte</span><span class="w">
    </span><span class="nx">G</span><span class="w"> </span><span class="kd">map</span><span class="p">[</span><span class="kt">string</span><span class="p">]</span><span class="kt">int64</span><span class="w">
    </span><span class="nx">H</span><span class="w"> </span><span class="kd">interface</span><span class="p">{}</span><span class="w">
    </span><span class="nx">I</span><span class="w"> </span><span class="kt">int</span><span class="w">
</span><span class="p">}</span></pre></div><div class="step step-level-1" step="72" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="182500" data-y="0" data-z="0"><h1 id="what-s-padding">What's padding?</h1><pre class="highlight code go"><span class="nx">x</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">XXX</span><span class="p">{}</span><span class="w">         </span><span class="c1">// measured with Go 1.20!</span><span class="w">
</span><span class="nx">s</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">unsafe</span><span class="p">.</span><span class="nx">Sizeof</span><span class="w"> </span><span class="c1">//</span><span class="w">
</span><span class="nb">println</span><span class="p">(</span><span class="nx">s</span><span class="p">(</span><span class="nx">x</span><span class="p">.</span><span class="nx">A</span><span class="p">))</span><span class="w">    </span><span class="c1">// 8 int64</span><span class="w">
</span><span class="nb">println</span><span class="p">(</span><span class="nx">s</span><span class="p">(</span><span class="nx">x</span><span class="p">.</span><span class="nx">B</span><span class="p">))</span><span class="w">    </span><span class="c1">// 4 uint32</span><span class="w">
</span><span class="nb">println</span><span class="p">(</span><span class="nx">s</span><span class="p">(</span><span class="nx">x</span><span class="p">.</span><span class="nx">C</span><span class="p">))</span><span class="w">    </span><span class="c1">// 1 byte</span><span class="w">
</span><span class="nb">println</span><span class="p">(</span><span class="nx">s</span><span class="p">(</span><span class="nx">x</span><span class="p">.</span><span class="nx">D</span><span class="p">))</span><span class="w">    </span><span class="c1">// 1 bool</span><span class="w">
                   </span><span class="c1">// +2 padding</span><span class="w">
</span><span class="nb">println</span><span class="p">(</span><span class="nx">s</span><span class="p">(</span><span class="nx">x</span><span class="p">.</span><span class="nx">E</span><span class="p">))</span><span class="w">    </span><span class="c1">// 16 string (ptr+len)</span><span class="w">
</span><span class="nb">println</span><span class="p">(</span><span class="nx">s</span><span class="p">(</span><span class="nx">x</span><span class="p">.</span><span class="nx">F</span><span class="p">))</span><span class="w">    </span><span class="c1">// 24 slice (ptr+len+cap)</span><span class="w">
</span><span class="nb">println</span><span class="p">(</span><span class="nx">s</span><span class="p">(</span><span class="nx">x</span><span class="p">.</span><span class="nx">G</span><span class="p">))</span><span class="w">    </span><span class="c1">// 8 map (ptr)</span><span class="w">
</span><span class="nb">println</span><span class="p">(</span><span class="nx">s</span><span class="p">(</span><span class="nx">x</span><span class="p">.</span><span class="nx">H</span><span class="p">))</span><span class="w">    </span><span class="c1">// 16 iface (ptr+typ)</span><span class="w">
</span><span class="nb">println</span><span class="p">(</span><span class="nx">s</span><span class="p">(</span><span class="nx">x</span><span class="p">.</span><span class="nx">I</span><span class="p">))</span><span class="w">    </span><span class="c1">// 8 int</span><span class="w">
                   </span><span class="c1">// Sum: 86</span><span class="w">
</span><span class="nb">println</span><span class="p">(</span><span class="nx">s</span><span class="p">(</span><span class="nx">x</span><span class="p">))</span><span class="w">      </span><span class="c1">// 88 (not 86!)</span></pre><div class="notes"><p>If a struct is bigger than a cache line, then accessing .A and .I
would cause the CPU to always require to get a new cache line!</p><p>Keep your structures under 64 bytes at max. Even less is better,
aim to stay under 32 byte.</p><p>Some more background info regarding why the value needs to be padded
can be found here (i.e. instructions require proper alignment):</p><p><a href="https://go101.org/article/memory-layout.html">https://go101.org/article/memory-layout.html</a></p></div></div><div class="step step-level-1" step="73" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="185000" data-y="0" data-z="0"><pre class="highlight code go"><span class="kd">var</span><span class="w"> </span><span class="nx">data</span><span class="w"> </span><span class="p">[</span><span class="mi">100</span><span class="p">]</span><span class="nx">XXX</span><span class="w">
</span><span class="k">for</span><span class="w"> </span><span class="nx">_</span><span class="p">,</span><span class="w"> </span><span class="nx">elem</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="k">range</span><span class="w"> </span><span class="nx">data</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="c1">// 100 cache misses (at least)!</span><span class="w">
    </span><span class="nx">fmt</span><span class="p">.</span><span class="nx">Println</span><span class="p">(</span><span class="nx">elem</span><span class="p">.</span><span class="nx">A</span><span class="p">,</span><span class="w"> </span><span class="nx">elem</span><span class="p">.</span><span class="nx">I</span><span class="p">)</span><span class="w">
</span><span class="p">}</span></pre><pre class="highlight code bash"><span class="c1"># How big is a cache line?
</span>$<span class="w"> </span>lscpu<span class="w"> </span>--caches</pre><div class="notes"><p>Good article with a slightly different (and more realworld) example:</p><p><a href="https://www.ardanlabs.com/blog/2023/07/getting-friendly-with-cpu-caches.html">https://www.ardanlabs.com/blog/2023/07/getting-friendly-with-cpu-caches.html</a></p></div></div><div class="step step-level-1" step="74" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="187500" data-y="0" data-z="0"><h1 id="binary-size-matters">(Binary) size matters!</h1><ul><li>More debug symbols, functions, lookup tables and instructions make the binary bigger.</li><li>A process needs <em>at least</em> as much memory as the binary size (<em>Caveat:</em> only the first one)</li><li>The bigger the binary, the longer the startup time. Important for shortlived processes/bootup (scripts!)</li><li>CPUs have separate caches for code instructions. If your program is so fat that that the caches get evicted while jumping
between two functions, then you pay with performance.</li></ul><p class="small">&#xBB;<em>Yo binary is so fat, you see it on Google Earth!</em> &#x1F30D;&#xAB;</p><div class="notes"><p>Binaries can be compressed with UPX, but that does make start up time faster - contrary to that.</p><p>Also, in the embedded world the binary size is way more important, but 30M binaries seem excessive
even on servers. Go is doing a bad job here while Rust produces tiny outputs.</p></div></div><div class="step step-level-1" step="75" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="190000" data-y="0" data-z="0"><h1 id="binary-sizes-per-language">Binary sizes per language</h1><p>(for a &#xBB;<em>Hello world!</em>&#xAB;)</p><img src="images/binary_sizes.png" width="100%"></img><div class="notes"><p>Source: <a href="https://drewdevault.com/2020/01/04/Slow.html">https://drewdevault.com/2020/01/04/Slow.html</a></p></div></div><div class="step step-level-1" step="76" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="192500" data-y="0" data-z="0"><h1 id="perf">perf</h1><pre class="highlight code bash"><span class="c1"># Like `time` but much better.
</span>$<span class="w"> </span>perf<span class="w"> </span>stat<span class="w"> </span>-a<span class="w"> </span>&lt;command&gt;<span class="w">
</span>$<span class="w"> </span>perf<span class="w"> </span>stat<span class="w"> </span>-a<span class="w"> </span>-p<span class="w"> </span>&lt;PID&gt;<span class="w">

</span><span class="c1"># See where the system spends time now:
</span>$<span class="w"> </span>perf<span class="w"> </span>top<span class="w">

</span><span class="c1"># Detailed report about memory access / misses
</span>$<span class="w"> </span>perf<span class="w"> </span>mem<span class="w"> </span>record<span class="w"> </span>-a<span class="w"> </span>./counter<span class="w"> </span>atomic<span class="w">
</span>$<span class="w"> </span>perf<span class="w"> </span>mem<span class="w"> </span>-t<span class="w"> </span>load<span class="w"> </span>report<span class="w"> </span>--sort<span class="o">=</span>mem<span class="w">

</span><span class="c1"># Can find false sharing (see next chapter)
</span>$<span class="w"> </span>perf<span class="w"> </span>c2c</pre></div><div class="step step-level-1" step="77" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="195000" data-y="0" data-z="0"><h1 id="pprof">pprof</h1><img src="images/dashboard_pprof_preview.png" width="100%"></img><pre class="highlight code bash"><span class="c1"># simple:
</span>$<span class="w"> </span>go<span class="w"> </span><span class="nb">test</span><span class="w"> </span>-v<span class="w"> </span>-cpuprofile<span class="o">=</span>cpu.pprof<span class="w"> </span>-memprofile<span class="o">=</span>mem.pprof<span class="w">
</span>$<span class="w"> </span>go<span class="w"> </span>tool<span class="w"> </span>pprof<span class="w"> </span>./cpu.pprof</pre><pre class="highlight code go"><span class="c1">// permanently:</span><span class="w">
</span><span class="kn">import</span><span class="w"> </span><span class="nx">_</span><span class="w"> </span><span class="s">"net/http/pprof"</span><span class="w">
</span><span class="k">go</span><span class="w"> </span><span class="nx">http</span><span class="p">.</span><span class="nx">ListenAndServe</span><span class="p">(</span><span class="s">"localhost:3000"</span><span class="p">,</span><span class="w"> </span><span class="kc">nil</span><span class="p">)</span></pre><pre class="highlight code bash">$<span class="w"> </span>go<span class="w"> </span>tool<span class="w"> </span>pprof<span class="w"> </span>localhost:3000/debug/pprof/profile<span class="w">
</span>$<span class="w"> </span>go<span class="w"> </span>tool<span class="w"> </span>pprof<span class="w"> </span>localhost:3000/debug/pprof/heap</pre><p><a href="https://raw.githubusercontent.com/sahib/misc/master/performance/images/analytics_profile_10min.png">Profile of firmware's analytics service</a></p><div class="notes"><p>Look at images/dashboard_pprof.svg here.</p><p>Pprof is also available for Python, but not as well integrated:
<a href="https://github.com/timpalpant/pypprof">https://github.com/timpalpant/pypprof</a></p></div></div><div class="step step-level-1" step="78" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="197500" data-y="0" data-z="0"><h1 id="flame-graphs">Flame graphs</h1><img src="images/brig_flamegraph.png" width="80%"></img><pre class="highlight code go"><span class="nx">f</span><span class="p">,</span><span class="w"> </span><span class="nx">_</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">os</span><span class="p">.</span><span class="nx">Create</span><span class="p">(</span><span class="s">"/tmp/cpu.pprof"</span><span class="p">)</span><span class="w">
</span><span class="nx">pprof</span><span class="p">.</span><span class="nx">StartCPUProfile</span><span class="p">(</span><span class="nx">f</span><span class="p">)</span><span class="w">
</span><span class="k">defer</span><span class="w"> </span><span class="nx">pprof</span><span class="p">.</span><span class="nx">StopCPUProfile</span><span class="p">()</span></pre><pre class="highlight code bash">$<span class="w"> </span>go<span class="w"> </span>tool<span class="w"> </span>pprof<span class="w"> </span>-http<span class="o">=</span><span class="s2">":8000"</span><span class="w"> </span>&lt;binary&gt;<span class="w"> </span>/tmp/cpu.prof</pre><div class="notes"><p>Alternative for short lived programs:
make pprof record a profile.</p><p>See images/brig_flamegraph.png
See images/brig_flamegraph.html</p><p>Perfect to see what time is spend in in what symbol.
Available for:</p><ul><li>CPU</li><li>Memory Allocations (although I like pprof more here)</li><li>Off-CPU (i.e. I/O)</li></ul></div></div><div class="step step-level-1" step="79" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="200000" data-y="0" data-z="0"><h1 id="false-sharing">False sharing</h1><ul><li><strong>Problem:</strong> Unrelated data in the same cache line gets modified and thus cache line gets evicted.</li><li><strong>Solution:</strong> Add some padding!</li></ul><p class="example">Example code/counter (4)</p><div class="notes"><p>If a program modifies data, the responding cache line needs
to be evicted (unless the modification resulted from the currently
running program). This is called "cache eviction" in short.</p><p>If it happens because the data in the cache line was actually
changed, then all is good. Data needs to be fetched again from memory
which costs a bit of time.</p><p>But what if two data points just happen to be in the same cache line?
Imagine two int64 counters that get incremented by two separate threads.
They do not talk to each other and should be influenced by each other.
However, each increment evicts the cache line and causes a slowdown.
We can use padding to force each counter into a separate cache line.</p></div></div><div class="step step-level-1" step="80" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="202500" data-y="0" data-z="0"><h1 id="true-sharing">True sharing</h1><ul><li><strong>Situation:</strong> Closely related data lands in the same cache line.</li><li><strong>Effect:</strong> Less jumping, less memory loads, higher throughput.</li><li><strong>Trick:</strong> Structs &lt; 64 byte and being cache friendly.</li></ul><p class="example">Example: code/striding</p><div class="notes"><p>This is when the idea of introducing caches between CPU and memory works out.
Good news: Can be controlled by:</p><ul><li>Limiting struct sizes to 64 bytes</li><li>Grouping often accessed data together.
(arrays of data, not array of structs of data)</li></ul><p>A bad example of this are linked lists. The next node is usually somewhere else
in memory and the size of a single node is below 64 bytes. This results in cache lines
that are mostly loaded for no reason. One solution would be to design cache-friendly linked
list by packing more data into the node itself.</p></div></div><div class="step step-level-1" step="81" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="205000" data-y="0" data-z="0"><h1 id="typical-access-patterns">Typical Access patterns</h1><img src="images/access_patterns.png" width="100%"></img><div class="line-block"><br></br></div><div class="notes"><p><em>Learning:</em> Group data in a CPU friendly way. Prefer <em>Struct of Arrays</em> over
<em>Array of Structs</em> if you require a performance boost.</p></div></div><div class="step step-level-1" step="82" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="207500" data-y="0" data-z="0"><h1 id="aos-vs-soa">AoS vs SoA</h1><pre class="highlight code go"><span class="kd">var</span><span class="w"> </span><span class="nx">AoS</span><span class="w"> </span><span class="p">[]</span><span class="kd">struct</span><span class="p">{</span><span class="w"> </span><span class="c1">// ArrayOfStructures</span><span class="w">
    </span><span class="nx">A</span><span class="w"> </span><span class="kt">int</span><span class="w">
    </span><span class="nx">B</span><span class="w"> </span><span class="kt">int</span><span class="w">
</span><span class="p">}</span><span class="w">

</span><span class="kd">var</span><span class="w"> </span><span class="nx">SoA</span><span class="w"> </span><span class="kd">struct</span><span class="p">{</span><span class="w">   </span><span class="c1">// StructureOfArrays</span><span class="w">
    </span><span class="nx">A</span><span class="w"> </span><span class="p">[]</span><span class="kt">int</span><span class="w">
    </span><span class="nx">B</span><span class="w"> </span><span class="p">[]</span><span class="kt">int</span><span class="w">
</span><span class="p">}</span></pre><img src="images/struct_of_slices.png" width="90%"></img></div><div class="step step-level-1" step="83" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="210000" data-y="0" data-z="0"><h1 id="dataoriented-programming">Dataoriented programming</h1><p>The science of designing programs in a CPU friendly way.</p><img src="images/dop_book.png" width="50%"></img><div class="notes"><p>DOP is often mentioned as contrast to OOP, but both concepts can complement each other.</p><p>Object oriented program is designing the program in a way that is friendly to humans.</p><p>It does by encapsulating data and methods together. By coincidence, this is not exactly
helpful to the machine your program runs on. Why?</p><ul><li>global state (i.e. impure functions) make branch/cache predictions way harder.</li><li>hurts cache locality.</li></ul></div></div><div class="step step-level-1" step="84" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="212500" data-y="0" data-z="0"><h1 id="quiz-matrix-traversal">Quiz: Matrix Traversal</h1><pre class="highlight code c"><span class="kt">int</span><span class="w"> </span><span class="o">*</span><span class="n">m</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">malloc</span><span class="p">(</span><span class="n">N_ROWS</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">N_COLS</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">));</span></pre><img src="images/matrix_traversal.png" width="100%"></img><p class="example">Example: code/matrix</p><div class="notes"><p>What is faster? Traversing <tt>m</tt>...</p><ol><li>...row by row?</li><li>...column by column?</li></ol><p>Good picture source: <a href="https://medium.com/mirum-budapest/introduction-to-data-oriented-programming-85b51b99572d">https://medium.com/mirum-budapest/introduction-to-data-oriented-programming-85b51b99572d</a></p></div></div><div class="step step-level-1" step="85" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="215000" data-y="0" data-z="0"><h1 id="recursion-vs-iteration">Recursion vs Iteration</h1><ul><li>Recursion is elegant but can be expensive.</li><li>Make the recursive call the last thing in your function.</li><li>(&#xBB;Tailcall optimization&#xAB;)</li></ul><pre class="highlight code">BenchmarkSum/normal    286.7 ns/op
BenchmarkSum/tailcall  242.1 ns/op
BenchmarkSum/iterative  71.1 ns/op</pre><p class="example">Example: code/tailcall</p><div class="notes"><p>Recursive algorithms like quicksort or merge sort are relatively elegant
when writing as recursive function. Sadly, this results in some performance impact.</p><p>Why? Because function call have a certain overhead, as we will see in the next chapter.
This function overhead can be reduced if we place the recursive function call at the end
of the function. This allows a smart compiler to save some instructions. An even smarter
compiler (clang or gcc) might even able to convert the recursion function into its
iterative equivalent.</p><p>This is called "Tail call optimization": <a href="https://de.wikipedia.org/wiki/Endrekursion">https://de.wikipedia.org/wiki/Endrekursion</a></p></div></div><div class="step step-level-1" step="86" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="217500" data-y="0" data-z="0"><h1 id="virtual-funcs-interfaces">Virtual funcs &amp; Interfaces</h1><img src="images/interface.svg" width="60%"></img><p class="example">Example: code/cost_of_interface</p><div class="notes"><p>Interface calls have between 2x to 10x as much overhead as direct calls in Go.</p><p>Interfaces also have a space cost. A variable of type interface is basically
a pointer with 16 byte (Space!). It consists of two actual pointers: type
(pointing to a struct describing the type for reflection) and a pointer to
the actual data the interface points to. This is one indirection more, one
more cycle for the GC.</p><p>Also, interfaces are opaque to the compiler. It cannot reason about what
they could do, so they can not use inlining or do proper escape analysis and
instead allocate on the heap always.</p><p>More info: <a href="https://syslog.ravelin.com/go-interfaces-but-at-what-cost-961e0f58a07b">https://syslog.ravelin.com/go-interfaces-but-at-what-cost-961e0f58a07b</a></p><p>Now you could say: Ah, I don't use Go, all good. Well, pretty much all
languages that support OOP are affected by this kind of behaviour. Virtual
methods in C++ or Java have their price too: They need to lookup a vtable,
which adds some more instructions but most importantly hinders the compiler
to optimize further.</p><p>Since especially Java uses Getters and Setters a lot - which are just one-line
functions in most cases - they pay quite a penalty regarding performance.</p><p>Python is especially wild, since they just might do tons of dictionary lookups
if you use classes with a lot of inheritance.</p></div></div><div class="step step-level-1" step="87" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="220000" data-y="0" data-z="0"><h1 id="boundcheck-elimination">Boundcheck Elimination</h1><p>Help the compiler!</p><p class="example">Example: code/boundscheck</p><div class="notes"><p>In a memory-safe language all access to slices are checked
(and if out-of-bound, an language panic/exception is produced)</p><p>This is a very small price to pay for the safety, but it costs
a few instructions.</p><p>More infos can be found here:
<a href="https://go101.org/article/bounds-check-elimination.html">https://go101.org/article/bounds-check-elimination.html</a></p></div></div><div class="step step-level-1" step="88" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="222500" data-y="0" data-z="0"><h1 id="process-scheduler">Process scheduler</h1><p><strong>Context switch:</strong></p><ul><li><em>Before execution:</em> Load register state from RAM.</li><li><em>After execution:</em> Store register state in RAM.</li></ul><img src="images/process_states.webp" width="50%"></img><div class="notes"><p>We're not alone on a system. Every process get assigned a share of time that it may execute.</p><p>-&gt; Expensive. Switching too often is expensive.</p><ul><li>scheduler types (O(n), O(1), CFS, BFS)</li><li>scheduler is determined at compile time.</li><li>there are some knobs to tune the scheduler, but not that interesting.</li><li>Show process states with ps a.</li></ul></div></div><div class="step step-level-1" step="89" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="225000" data-y="0" data-z="0"><h1 id="process-load">Process load</h1><p><strong>Load:</strong> Count of processes currently in running or waiting state.</p><p><span class="math ">\(load_{now} = \begin{cases}N_{count} = 0\:\:\:\:\:\:\:\:\:\iff\textrm{Idle}\\N_{count} &lt; N_{cores}\iff\textrm{Normal}\\N_{count}\ge N_{cores}\iff\textrm{Overload}\end{cases}\)</span></p><div class="notes"><p>The load metric makes most sense if averaged over some time.</p><p>Those are the load5/load10/load15 params.
Use load5 for graphs, load15 for quick judgmenet.</p><p>You can use the "uptime" command to check the load.</p></div></div><div class="step step-level-1" step="90" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="227500" data-y="0" data-z="0"><h1 id="process-niceness">Process niceness</h1><p><em>Niceness</em> is the scheduling priority.</p><ul><li>Ranges from <span class="math ">\(-20\)</span> to <span class="math ">\(+19\)</span>; <span class="math ">\(0\)</span> is default.</li><li><span class="math ">\(-20\)</span> gives the process more time to execute.</li><li><span class="math ">\(+19\)</span> gives the process way less to execute.</li></ul><pre class="highlight code bash"><span class="c1"># for new processes: sleep with high prio
</span>$<span class="w"> </span>nice<span class="w"> </span>-n<span class="w"> </span>-20<span class="w"> </span>sleep<span class="w"> </span>5s<span class="w">

</span><span class="c1"># for running processes: change to unimportant
</span>$<span class="w"> </span>renice<span class="w"> </span>-n<span class="w"> </span>+19<span class="w"> </span><span class="k">$(</span>pgrep<span class="w"> </span>docker<span class="k">)</span></pre><div class="notes"><p>Disclaimer: Exact behaviour depends on scheduler (scheduling frequency vs
time slice size)</p></div></div><div class="step step-level-1" step="91" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="230000" data-y="0" data-z="0"><h1 id="rough-rules-to-take-away">Rough Rules to take away</h1><ol><li>Watch out for cache misses.</li><li>Keep your structs small (&lt; 64B).</li><li>Check if you need padding (false sharing).</li><li>Place commonly accessed data close (true sharing).</li><li>Design your access patterns cache friendly.</li><li>Avoid virtual methods and inheritance.</li><li>Do not overuse pointers over values.</li><li>Trust your compiler, but check what it did.</li><li>Use SIMD if you have to; or leave it to others.</li></ol><div class="notes"><p>Go even warns about too structures (if they are used as values):
gocritic hugeParam: cfg is heavy (240 bytes); consider passing it by pointer</p><p>A good and very quick summary is also in this article
(although you need background info to understand the tips):</p><p><a href="https://medium.com/scum-gazeta/golang-simple-optimization-notes-70bc64673980">https://medium.com/scum-gazeta/golang-simple-optimization-notes-70bc64673980</a></p></div></div><div class="step step-level-1" step="92" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="232500" data-y="0" data-z="0"><h1 id="fynn-1">Fynn!</h1><div class="line-block"><br></br></div><p class="big-text">&#x1F3C1;</p><div class="line-block"><br></br></div><p class="next-link"><strong>Next:</strong> <a href="../3_memory/index.html">Memory</a>: Bookkeeping is hard &#x1F4DD;</p></div><div class="step step-level-1" step="93" data-x="235000" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-y="0" data-z="0"><p class="chapter">Memory</p><p>Bookkeeping is hard &#x1F4DD;</p><div class="notes"><p>Most stuff in this session is related to this PDF:</p><p><a href="https://people.freebsd.org/~lstewart/articles/cpumemory.pdf">https://people.freebsd.org/~lstewart/articles/cpumemory.pdf</a></p></div></div><div class="step step-level-1" step="94" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="237500" data-y="0" data-z="0"><h1 id="agenda-2">Agenda</h1><ul><li>How does memory work as hardware?</li><li>How does Linux manage memory?</li><li>How can we measure &amp; profile memory usage?</li><li>How can we allocate less and faser?</li></ul><img src="images/ram.png" width="50%"></img></div><div class="step step-level-1" step="95" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="240000" data-y="0" data-z="0"><h1 id="rama-lama-ding-dong">RAMa Lama Ding Dong &#x1F3BA;</h1><ul><li><strong>RAM</strong> = Random Access Memory</li><li>Huge, sequential line of individual memory cells.</li><li>Usually can only be addressed in 4K pages.</li><li>Memory controller that handles the actual interaction between Bus and CPU.</li></ul><p>Two major types in use today:</p><ul><li><em>Static RAM</em> (SRAM)</li><li><em>Dynamic RAM</em> (DRAM)</li></ul><div class="notes"><p>SDRAM = Synchronous DRAM</p><p>DDR-SDRAM = Double Data Rate SDRAM</p><p>(and DDR2 doubles that and so on)</p></div></div><div class="step step-level-1" step="96" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="242500" data-y="0" data-z="0"><h1 id="dram-one-bit-please">DRAM - one bit, please</h1><img src="images/dram.png" width="100%" align="center"></img><div class="notes"><p>Dynamic sounds good, doesn't it? Well, it isn't...</p><p>Pros:</p><ul><li>Very simple and cheap to produce.</li><li>High density (many cells per area)</li></ul><p>Cons:</p><ul><li>Needs to be refreshed constantly (64ns or so)</li><li>Makes logic in controller way more complicated.</li><li>Relatively slow.</li><li>Enables security issues like ROWHAMMER.</li></ul></div></div><div class="step step-level-1" step="97" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="245000" data-y="0" data-z="0"><h1 id="sram-one-bit-please">SRAM - one bit, please</h1><img src="images/sram.png" width="100%" align="center"></img><div class="notes"><ul><li>Very fast. (10x or more)</li><li>Used in L1/L2/L3 caches in the CPU.</li><li>No refresh required.</li><li>Low power consumption</li><li>Expensive, not so high density</li></ul></div></div><div class="step step-level-1" step="98" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="247500" data-y="0" data-z="0"><h1 id="why-use-dram-at-all">Why use DRAM at all?</h1><ul><li>Because it's cheap, and we need tons of it.</li><li>Main memory is all DRAM.</li><li>Caches (L1-L3) are SRAM.</li><li>A lightbulb is maybe OSRAM (Sorry.) &#x1F4A1;</li></ul><div class="notes"><p>So basically...</p><p>again, hardware is at fault
and instead of fixing it with some Pfiffikus
we software devs have to cope with slow main memory.</p></div></div><div class="step step-level-1" step="99" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="250000" data-y="0" data-z="0"><h1 id="rowhammer">ROWHAMMER &#x1F528;</h1><img src="images/rowhamer.webp" width="100%"></img><div class="notes"><p>Fun fact: DRAM enables a hardware-based security attack: ROWHAMMER.
Changing a row of DRAM cells can, if done very often, switch a
nearby row. This can be used to change data like "userIsLoggedIn".</p></div></div><div class="step step-level-1" step="100" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="252500" data-y="0" data-z="0"><h1 id="ecc-memory">ECC Memory</h1><ul><li>Radiation or damage can flip bits</li><li>ECC RAM protects against such errors.</li><li>Use of parity bits or Hamming code.</li><li>Slightly slower than normal RAM.</li></ul><img src="images/ecc.png" width="100%"></img><div class="notes"><p>ECC comes with a price &amp; performance tag.</p></div></div><div class="step step-level-1" step="101" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="255000" data-y="0" data-z="0"><h1 id="numa-multiple-cpus">NUMA - multiple CPUs</h1><p><strong>NUMA</strong> = <em>Non Uniform Memory Architecture</em></p><p>Is the access to all memory equally fast?</p><ul><li>Not if you have more than one CPU!</li><li>Every CPU gets 1/nth of the memory.</li><li>Every CPU can access the complete memory.</li><li>Non-local access is costly.</li></ul><div class="notes"><p>NUMA is a term you might come across.</p><p>Linux is NUMA capable and that's why it's such a popular server and
superomputer operating system. Or one of the reasons at least.</p></div></div><div class="step step-level-1" step="102" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="257500" data-y="0" data-z="0"><h1 id="how-is-memory-managed">How is memory managed?</h1><img src="diagrams/3_os_allocations.svg" width="100%"></img><div class="notes"><p>The large sequential slab of memory needs to be
distributed to all programs that require it.</p><ul><li>Usage is not known in advance.</li><li>programs need to allocate based on their need.</li><li>OS needs to make memory allocations inexpensive</li></ul><p>Understandin how the kernel and processes manage their memory
makes it possible to use less of it and make more efficient use of it.</p><p>For this we need to start at the basics...</p></div></div><div class="step step-level-1" step="103" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="260000" data-y="0" data-z="0"><h1 id="inside-a-process">Inside a process</h1><ul><li>Each process may allocate certain amounts of memory on-demand.</li><li>Memory inside the process can be managed in three ways: <em>Data</em>, <em>Stack</em>, <em>Heap.</em></li><li><em>Data:</em> Readonly data known at compile time.</li><li><em>Stack:</em> For short-lived memory &amp; automatic.</li><li><em>Heap:</em> For long-lived memory &amp; manual.</li></ul></div><div class="step step-level-1" step="104" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="262500" data-y="0" data-z="0"><h1 id="the-stack-growth">The stack: Growth</h1><pre class="highlight code go"><span class="kd">func</span><span class="w"> </span><span class="nx">recursive</span><span class="p">(</span><span class="nx">depth</span><span class="w"> </span><span class="kt">int</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="k">if</span><span class="w"> </span><span class="nx">depth</span><span class="w"> </span><span class="o">&lt;=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="p">}</span><span class="w">

    </span><span class="kd">var</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="kt">int</span><span class="w">
    </span><span class="nx">fmt</span><span class="p">.</span><span class="nx">Printf</span><span class="p">(</span><span class="s">"%p\n"</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="nx">a</span><span class="p">)</span><span class="w">
    </span><span class="nx">recursive</span><span class="p">(</span><span class="nx">depth</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w">
</span><span class="p">}</span><span class="w">
</span><span class="c1">// ...</span><span class="w">
</span><span class="nx">recursive</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span><span class="w">

</span><span class="c1">// Output:</span><span class="w">
</span><span class="mh">0xc000070e70</span><span class="w"> </span><span class="o">-</span><span class="p">&gt;</span><span class="w"> </span><span class="nx">diff</span><span class="p">:</span><span class="w"> </span><span class="mi">80</span><span class="w"> </span><span class="nx">bytes</span><span class="w"> </span><span class="nx">due</span><span class="w"> </span><span class="nx">to</span><span class="p">:</span><span class="w">
</span><span class="mh">0xc000070e20</span><span class="w"> </span><span class="o">-</span><span class="p">&gt;</span><span class="w"> </span><span class="nx">stack</span><span class="w"> </span><span class="nx">pointer</span><span class="p">,</span><span class="w"> </span><span class="nx">frame</span><span class="w"> </span><span class="nx">pointer</span><span class="w">
</span><span class="mh">0xc000070dd0</span><span class="w"> </span><span class="o">-</span><span class="p">&gt;</span><span class="w"> </span><span class="nx">registers</span><span class="p">,</span><span class="w"> </span><span class="nx">params</span><span class="p">,</span><span class="w"> </span><span class="nx">local</span><span class="w"> </span><span class="nx">vars</span><span class="w">
</span><span class="o">...</span></pre><div class="notes"><p>More details on calling a function:</p><p><a href="https://eli.thegreenplace.net/2011/09/06/stack-frame-layout-on-x86-64">https://eli.thegreenplace.net/2011/09/06/stack-frame-layout-on-x86-64</a></p><p>Stack grow direction: Depends on OS / your programming language.</p></div></div><div class="step step-level-1" step="105" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="265000" data-y="0" data-z="0"><h1 id="the-stack-lifo-layout">The stack: LIFO Layout</h1><img src="images/stack_layout.svg" width="120%"></img><div class="notes"><p>Every function calls causes a so called "frame" to be placed on
top of the stack. Each frame contains parameters, local variables,
but also a return link to the previous frame.</p><p>The program knows in which frame we are by looking at a thing
called "base pointer". A special pointer kept in a fixed register
and changed on each function call. There's also a pointer that points
to the current end of the stack, so using this we know how big a frame
is. When a function returns we can simply change those two registers
using the "return link". The current stack frame is then deallocated
and will be overwritten when another function is called.</p><p>Malicious software sometimes is able to overwrite the return link to somewhere
else, e.g. using a buffer overflow. This leads the program to execute some other
other data as code, potentially doing evil things.</p><p>Registers:</p><p>ebp: Base pointer. Points to start of function. Cell at adress contains "return link to last function" (i.e. pointer to instruction offset)
esp: Initially the base pointer, but grows with each variable put on the stack.
eip: Pointer that points to current instruction (not on the stack, but your code is somewhere else in memory)</p><p>Stack origin:  ebp.
Stack pointer: esp.</p><p><a href="https://en.wikipedia.org/wiki/Stack-based_memory_allocation">https://en.wikipedia.org/wiki/Stack-based_memory_allocation</a></p><p>Good explanation here too: <a href="https://people.cs.rutgers.edu/~pxk/419/notes/frames.html">https://people.cs.rutgers.edu/~pxk/419/notes/frames.html</a></p></div></div><div class="step step-level-1" step="106" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="267500" data-y="0" data-z="0"><h1 id="the-stack-overflow">The stack: Overflow</h1><p>Why not use the Stack for everything?</p><ol><li>Stack size is limited to 8MB (default on Linux).</li><li>Memory is bound to your call hierarchy.</li><li>The memory lives only until your function returns.</li><li>Stack is per-thread, sharing requires heap.</li></ol><div class="notes"><dl><dt>1: Reason for this are security mostly. Recursion happens on the stack, so</dt><dd><p>endless recursive programs cannot break everything. Also running over the
extents of a buffer in C (Security issue!) will overwrite parts of the
stack, so limiting it makes sense.</p></dd></dl><ol><li>Stack is a LIFO. You cannot free objects down in the stack without
freeing everything in between.</li><li>Every thread (and in Go every goroutine) has their own stack.</li></ol></div><p class="example">Example: code/stackoverflow</p></div><div class="step step-level-1" step="107" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="270000" data-y="0" data-z="0"><h1 id="the-stack-summary">The stack: Summary</h1><ul><li>...cleaned up automatically on return.</li><li>...bound to a function call.</li><li>...low overhead and should be preferred.</li><li>...can be reasoned about during compile time.</li><li>...good for small amounts of data.</li></ul></div><div class="step step-level-1" step="108" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="272500" data-y="0" data-z="0"><h1 id="the-heap-allocations">The Heap: Allocations</h1><pre class="highlight code go"><span class="c1">//go:noinline</span><span class="w">
</span><span class="kd">func</span><span class="w"> </span><span class="nx">f</span><span class="p">()</span><span class="w"> </span><span class="o">*</span><span class="kt">int</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nx">v</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="mi">3</span><span class="w">
    </span><span class="k">return</span><span class="w"> </span><span class="o">&amp;</span><span class="nx">v</span><span class="w">
</span><span class="p">}</span><span class="w">

</span><span class="kd">func</span><span class="w"> </span><span class="nx">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="c1">// Two for the stack:</span><span class="w">
    </span><span class="c1">// a=0xc00009aef8 b=0xc00009aef0</span><span class="w">
    </span><span class="nx">a</span><span class="p">,</span><span class="w"> </span><span class="nx">b</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="mi">23</span><span class="p">,</span><span class="w"> </span><span class="mi">42</span><span class="w">

    </span><span class="c1">// Two for the heap:</span><span class="w">
    </span><span class="c1">// c=0xc0000b2000 d=0xc0000b2008</span><span class="w">
    </span><span class="nx">c</span><span class="p">,</span><span class="w"> </span><span class="nx">d</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">f</span><span class="p">(),</span><span class="w"> </span><span class="nx">f</span><span class="p">()</span><span class="w">
</span><span class="p">}</span></pre><div class="notes"><p>Contrary to the stack, the memory is not bound to the function
and therefore will survive the return of a function. The downside
is that the memory needs to be freed</p><p>Languages like Go allocate automatically on the heap if they
have to - they do this either when the compiler cannot prove that
the value does not escape the function stack or when the allocation
is too big for the stack. More on this later. Thanks to the GC
memory is freed automatically after it's used. Having a GC is often
understood as "I don't need to think about memory" though, which is not
the case. You can help the GC to run faster and avoid memory leaks
that can arise through edge cases.</p><p>Languages like Python allocate everything on the heap. They almost
never use stack based memory for anything. Most interpreted languages
use a combination of reference counting and garbage collection.
Very convenient but also the slowest way to go.</p><p>Languages like C (and partly Rust) pass the duty of memory management
to the programmer. While this make it possible to be clever, it also
opens up ways to fuck up tremendously by creating memory leaks, double
frees, forgotten allocations or use-after-free scenarios.</p><p>Heap memory must be cleaned up after use. Go does this with a GC.</p></div></div><div class="step step-level-1" step="109" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="275000" data-y="0" data-z="0"><h1 id="the-heap-malloc">The Heap: <tt>malloc()</tt></h1><pre class="highlight code c"><span class="kt">int</span><span class="w"> </span><span class="o">*</span><span class="n">ptrs</span><span class="p">[</span><span class="mi">100</span><span class="p">];</span><span class="w">
</span><span class="k">for</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">100</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="n">ptrs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">malloc</span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">));</span><span class="w">
</span><span class="p">}</span><span class="w">
</span><span class="c1">// ... use memory ...
</span><span class="k">for</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">100</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="n">free</span><span class="p">(</span><span class="n">ptrs</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span><span class="w">
</span><span class="p">}</span></pre><div class="notes"><p>malloc() is a function that returns N bytes of memory, if available. It uses
a syscall of the kernel (sbrk()), but most of the logic is a library in
userspace.</p><p>malloc() manages internally a pool of memory internally, from which it
slices of the requested portions. Whenever the pool runs out of fresh
memory, the malloc implementation will ask the kernel for a new chunk
of memory. The exact mechanism is either over sbrk(2) or mmap()
(we will see mmap later)</p><p>As malloc() needs to cater objects of many different sizes (as seen in the
example above) it is prone to fragmentation.</p><p>malloc() can fail! Originally it was supposed to fail if there is no memory
left, but on Linux there is "infinite" virtual memory and overcommitting (we come to
those later), which is why it does not fail for this reasons. It will fail however
if you ask it a too large block, have some memory restrictions on your processes (cgroups)
or other administration reasons.</p></div></div><div class="step step-level-1" step="110" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="277500" data-y="0" data-z="0"><h1 id="the-heap-freelist">The Heap: Freelist</h1><img src="images/heap_freelist.png" width="70%"></img><div class="notes"><p>As mentioned above, the memory allocated from the pool
needs to be freed, so it can be re-used. This is done by the free() call.</p><p>malloc() needs to track which parts of its pool are in-use and which can
be issued on the next call. It does by the use of free-lists. Each block
returned by malloc() has a small header (violet) that points to the next block.
The memory returned by malloc() is just behind this small header.</p><p>Once allocated, a free block is taken out of the list and added to the "allocated"
list. This means that every allocation has a small space and time overhead.</p><p>On free(), the opposite happens: The block is put back into the freelist and
out of the "allocated" list. (i.e. an allocation is O(log n), instead of
O(1) as with the stack)</p><p>It is interesting to note that there are different implementations of this,
with different advantages and drawbacks. One very high performant implementatin
is jemalloc.</p><p>Useful Links:</p><ul><li><a href="https://azeria-labs.com/heap-exploitation-part-1-understanding-the-glibc-heap-implementation">https://azeria-labs.com/heap-exploitation-part-1-understanding-the-glibc-heap-implementation</a> (More details)</li><li><a href="https://sourceware.org/git/?p=glibc.git;a=blob;f=malloc/malloc.c;h=05e65a2d54f9b3850fa0c4d2c7dfaae3dfd94dac;hb=HEAD#l54">https://sourceware.org/git/?p=glibc.git;a=blob;f=malloc/malloc.c;h=05e65a2d54f9b3850fa0c4d2c7dfaae3dfd94dac;hb=HEAD#l54</a></li><li><a href="https://sourceware.org/git/?p=glibc.git;a=blob;f=malloc/malloc.c;h=05e65a2d54f9b3850fa0c4d2c7dfaae3dfd94dac;hb=HEAD#l102">https://sourceware.org/git/?p=glibc.git;a=blob;f=malloc/malloc.c;h=05e65a2d54f9b3850fa0c4d2c7dfaae3dfd94dac;hb=HEAD#l102</a>:</li></ul></div></div><div class="step step-level-1" step="111" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="280000" data-y="0" data-z="0"><h1 id="the-heap-leaks">The Heap: Leaks</h1><pre class="highlight code c"><span class="c1">// In C:
</span><span class="kt">char</span><span class="w"> </span><span class="o">*</span><span class="n">s</span><span class="p">;</span><span class="w">
</span><span class="n">s</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">malloc</span><span class="p">(</span><span class="mi">20</span><span class="p">);</span><span class="w">
</span><span class="n">s</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">malloc</span><span class="p">(</span><span class="mi">30</span><span class="p">);</span><span class="w"> </span><span class="c1">// leak: 20 bytes.</span></pre><pre class="highlight code go"><span class="c1">// In Go:</span><span class="w">
</span><span class="kd">var</span><span class="w"> </span><span class="nx">m</span><span class="w"> </span><span class="kd">map</span><span class="p">[</span><span class="kt">string</span><span class="p">][]</span><span class="kt">byte</span><span class="p">{}</span><span class="w">
</span><span class="kd">func</span><span class="w"> </span><span class="nx">f</span><span class="p">(</span><span class="nx">v</span><span class="w"> </span><span class="kt">int</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="c1">// the slice will be still referenced after</span><span class="w">
    </span><span class="c1">// the function returned, if not delete()'d</span><span class="w">
    </span><span class="nx">m</span><span class="p">[</span><span class="s">"blub"</span><span class="p">]</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nb">make</span><span class="p">([]</span><span class="kt">byte</span><span class="p">,</span><span class="w"> </span><span class="mi">100</span><span class="p">)</span><span class="w">
    </span><span class="k">return</span><span class="w"> </span><span class="nx">v</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nx">v</span><span class="w">
</span><span class="p">}</span></pre><div class="notes"><p>Other sources of memory leaks:</p><ul><li>Go routines blocking forever.</li><li>Assigning a small slice of a big array to a variable
(causing the whole array to be still referenced)</li></ul><p>Use pprof to find memory leaks in Go.</p><p>In C it's very easy to forget a free(), therefore quite
some impressive tooling developed over the years. The most prominent
example is valgrind: <a href="https://valgrind.org">https://valgrind.org</a></p><p>Python: Also has memory leaks, finding them is much harder
since the tooling is not great (at least when I looked last time).
Also: Memory leaks can happen on the C-side or in the python code
itself. If they happen in a C-module you're pretty much fuc.. lost.</p></div></div><div class="step step-level-1" step="112" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="282500" data-y="0" data-z="0"><h1 id="the-heap-summary">The Heap: Summary</h1><p><strong>Heap</strong></p><ul><li>...needs to be explicitly requested.</li><li>...needs to be explititly cleaned up.</li><li>...can be used until freed. Will crash otherwise.</li><li>...required for big data chunks or long-lived data.</li><li>...has a small, but noticeable, overhead.</li></ul><div class="notes"><p>Heap requires some implementation of malloc(). There are many different implementations
of it in C, using different strategies to perform well under certain load.
Choosing the right kind of allocator is a science in itself. More info can be obtained here:</p><p><a href="https://en.wikipedia.org/wiki/Memory_management#Implementations">https://en.wikipedia.org/wiki/Memory_management#Implementations</a></p><p>In languages like Go you don't have a choice which memory allocator you get. The Go runtime
provides one for you. This makes sense as it is coupled very tightly with the garbage collector.
Go uses a similar implementation, but is more sophisticated. Main difference:
it keeps pre-allocated arenas for differently sized objects. i.e. 4, 8, 16,
32, 64 and so on.</p><p>The grow direction of the heap and stack is not really important and you
should keep in mind that every thread/goroutine has their own stack and
there might be even more than one heap area, possibly backed by different
malloc() implementations.</p></div></div><div class="step step-level-1" step="113" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="285000" data-y="0" data-z="0"><h1 id="ways-to-optimize">Ways to optimize</h1><ul><li>Allocate less bytes.</li><li>Allocate less often.</li><li>Prefer cheap stack over heap, if possible.</li><li>Make the life of your GC easier.</li></ul></div><div class="step step-level-1" step="114" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="287500" data-y="0" data-z="0"><h1 id="garbage-collector-gc">Garbage collector (GC)</h1><img src="images/gc.png" width="100%"></img><div class="notes"><p>GC is a utility that remembers allocation and scans the memory used by the program
for referenes to the allocations. If no references are found it automatically cleans
up the associated memory.</p><p>This is very ergonomic for the programmer, but comes with a peformance impact. The
GC needs to run regularly and has, at least for a very small amount of time, stop
the execution of the program.</p><p>Good reference for the Go GC: <a href="https://tip.golang.org/doc/gc-guide">https://tip.golang.org/doc/gc-guide</a></p></div></div><div class="step step-level-1" step="115" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="290000" data-y="0" data-z="0"><h1 id="gc-pressure">GC: Pressure</h1><pre class="highlight code go"><span class="c1">// Prefer this...</span><span class="w">
</span><span class="nx">m</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nb">make</span><span class="p">(</span><span class="kd">map</span><span class="p">[</span><span class="kt">string</span><span class="p">]</span><span class="nx">someStruct</span><span class="p">)</span><span class="w">

</span><span class="c1">// ...over this:</span><span class="w">
</span><span class="nx">m</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nb">make</span><span class="p">(</span><span class="kd">map</span><span class="p">[</span><span class="kt">string</span><span class="p">]</span><span class="o">*</span><span class="nx">someStruct</span><span class="p">)</span></pre><p class="example">Example: code/allocs</p><pre class="highlight code bash"><span class="c1"># counting words with a map:
</span>$<span class="w"> </span>go<span class="w"> </span><span class="nb">test</span><span class="w"> </span>-v<span class="w"> </span>-bench<span class="o">=</span>.<span class="w"> </span>-benchmem<span class="w">
</span>noptr<span class="w">  </span><span class="m">577</span>.7<span class="w"> </span>ns/op<span class="w">   </span><span class="m">336</span><span class="w"> </span>B/op<span class="w">   </span><span class="m">2</span><span class="w"> </span>allocs/op<span class="w">
</span>ptr<span class="w">    </span><span class="m">761</span>.4<span class="w"> </span>ns/op<span class="w">   </span><span class="m">384</span><span class="w"> </span>B/op<span class="w">  </span><span class="m">10</span><span class="w"> </span>allocs/op</pre><div class="notes"><p>"GC Pressure" describes the amount of load a garbage collector currently has.
The more small objects it has to track, the higher the load. You can help it
by reducing the amount of different objects and making use of sync.Pools (see later)</p><p>One way to less use memory is to use less pointers:</p><ul><li>Way less memory in total (one cell less for the pointer)</li><li>Data is packed together (good for the CPU cache!)</li><li>Less work for the GC and the allocator to do</li><li>Pointers give you more potential to fuck up (they can be nil...)</li></ul><p>The "10" will increase with input size!
Longer runs will cause more GC for the ptr case.</p></div></div><div class="step step-level-1" step="116" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="292500" data-y="0" data-z="0"><h1 id="gc-escape-analysis">GC: Escape Analysis</h1><img src="images/escape_analysis.jpg" width="100%"></img><pre class="highlight code bash">$<span class="w"> </span>go<span class="w"> </span>build<span class="w"> </span>-gcflags<span class="o">=</span><span class="s2">"-m"</span><span class="w"> </span>.<span class="w">
</span>./main.go:5:2:<span class="w"> </span>moved<span class="w"> </span>to<span class="w"> </span>heap:<span class="w"> </span>x</pre><div class="notes"><p>Only heap allocated data is managed by the garbage collector.
The more you allocate on the heap, the more pressure you put on the
memory bookkeeping and the garbage collector.</p><ul><li>Avoid using pointers and refactor to make it allocate-able on the stack.</li><li>Prefer pass &amp; return by value if value is small (&lt; 64 byte ~= cache line)</li><li>Use sync.Pool to save allocations.</li><li>Sometimes inlining can make stack allocations possible.</li><li>Sometimes use of interfaces can force heap allocations (cost_of_interface example!)</li></ul><p>Good guide for the details: <a href="https://tip.golang.org/doc/gc-guide#Eliminating_heap_allocations">https://tip.golang.org/doc/gc-guide#Eliminating_heap_allocations</a></p><p>Picture source: <a href="https://dev.to/karankumarshreds/memory-allocations-in-go-1bpa">https://dev.to/karankumarshreds/memory-allocations-in-go-1bpa</a></p></div></div><div class="step step-level-1" step="117" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="295000" data-y="0" data-z="0"><h1 id="gc-pre-allocate">GC: Pre-Allocate</h1><pre class="highlight code go"><span class="nx">s</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nb">make</span><span class="p">([]</span><span class="kt">int</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="nb">len</span><span class="p">(</span><span class="nx">input</span><span class="p">))</span><span class="w">
</span><span class="nx">m</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nb">make</span><span class="p">(</span><span class="kd">map</span><span class="p">[</span><span class="kt">string</span><span class="p">]</span><span class="kt">int</span><span class="p">,</span><span class="w"> </span><span class="mi">20</span><span class="p">)</span><span class="w">
</span><span class="c1">// ...</span><span class="w">

</span><span class="c1">// If you need to concatenate many strings:</span><span class="w">
</span><span class="kd">var</span><span class="w"> </span><span class="nx">b</span><span class="w"> </span><span class="nx">strings</span><span class="p">.</span><span class="nx">Builder</span><span class="w">
</span><span class="nx">b</span><span class="p">.</span><span class="nx">Grow</span><span class="p">(</span><span class="mi">100</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">13</span><span class="p">)</span><span class="w">
</span><span class="k">for</span><span class="w"> </span><span class="nx">idx</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="nx">idx</span><span class="w"> </span><span class="p">&lt;</span><span class="w"> </span><span class="mi">100</span><span class="p">;</span><span class="w"> </span><span class="nx">idx</span><span class="o">++</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nx">b</span><span class="p">.</span><span class="nx">WriteString</span><span class="p">(</span><span class="s">"Hello World!\n"</span><span class="p">)</span><span class="w">
</span><span class="p">}</span><span class="w">
</span><span class="nx">fmt</span><span class="p">.</span><span class="nx">Println</span><span class="p">(</span><span class="nx">b</span><span class="p">.</span><span class="nx">String</span><span class="p">())</span></pre><p class="example">Example: code/prealloc</p><div class="notes"><p>This helps the GC in two ways:</p><ul><li>Slices by default only plan with a very small additional space
regarding allocations. If a slice grows it has to repeatedly copied
to a bigger memory chunk. Old chunks have then to be cleaned up.
Same goes with map, just a little more complicated.</li><li>It is easier to clean up one big memory slice instead of many small ones.</li></ul></div></div><div class="step step-level-1" step="118" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="297500" data-y="0" data-z="0"><h1 id="gc-re-use">GC: Re-use</h1><pre class="highlight code go"><span class="nx">s</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nb">make</span><span class="p">([]</span><span class="kt">int</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="nx">maxLineLen</span><span class="p">)</span><span class="w">
</span><span class="k">for</span><span class="w"> </span><span class="nx">_</span><span class="p">,</span><span class="w"> </span><span class="nx">line</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="k">range</span><span class="w"> </span><span class="nx">lines</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nx">s</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">s</span><span class="p">[:</span><span class="mi">0</span><span class="p">]</span><span class="w">  </span><span class="c1">// re-use underlying array!</span><span class="w">

    </span><span class="c1">// do something with s and line here</span><span class="w">
</span><span class="p">}</span></pre></div><div class="step step-level-1" step="119" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="300000" data-y="0" data-z="0"><h1 id="gc-pooling">GC: Pooling</h1><pre class="highlight code go"><span class="c1">// avoid expensive allocations by pooling:</span><span class="w">
</span><span class="kd">var</span><span class="w"> </span><span class="nx">writerGzipPool</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">sync</span><span class="p">.</span><span class="nx">Pool</span><span class="p">{</span><span class="w">
    </span><span class="c1">// other good candidates: bytes.Buffer{},</span><span class="w">
    </span><span class="c1">// big slices, empty objects used for unmarshal</span><span class="w">
    </span><span class="nx">New</span><span class="p">:</span><span class="w"> </span><span class="kd">func</span><span class="p">()</span><span class="w"> </span><span class="kt">any</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="k">return</span><span class="w"> </span><span class="nx">gzip</span><span class="p">.</span><span class="nx">NewWriter</span><span class="p">(</span><span class="nx">ioutil</span><span class="p">.</span><span class="nx">Discard</span><span class="p">)</span><span class="w">
    </span><span class="p">},</span><span class="w">
</span><span class="p">}</span><span class="w">

</span><span class="nx">w</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">writerGzipPool</span><span class="p">.</span><span class="nx">Get</span><span class="p">().(</span><span class="o">*</span><span class="nx">gzip</span><span class="p">.</span><span class="nx">Writer</span><span class="p">)</span><span class="w">
</span><span class="c1">// ... use w ...</span><span class="w">
</span><span class="nx">writerGzipPool</span><span class="p">.</span><span class="nx">Put</span><span class="p">(</span><span class="nx">w</span><span class="p">)</span></pre><p class="example">Example: code/mempool</p><div class="notes"><p>Pooling is the general technique of keeping a set of objects that are expensive object,
if they can be re-used. Typical examples would be thread pools that keep running threads
around, instead of firing up a new one for every task. Same can be done for memory objects
that are expensive to allocate (or have long-running init code like gzip.Writer).</p><p>Pools can be easily implemented using an array (or similar) and a mutex.
sync.Pool is a Go-specific solution that has some knowledge of the garbage collector
which would be not available to normal programs otherwise. It keeps a set of objects
around until they would be garbage collected anyways. I.e. the objects in the pool
get automatically freed after one or two GC runs.</p></div></div><div class="step step-level-1" step="120" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="302500" data-y="0" data-z="0"><h1 id="gc-internment-1">GC: Internment #1</h1><pre class="highlight code go"><span class="c1">// type StringHeader struct { Data uintptr, Len  int }</span><span class="w">
</span><span class="kd">func</span><span class="w"> </span><span class="nx">stringptr</span><span class="p">(</span><span class="nx">s</span><span class="w"> </span><span class="kt">string</span><span class="p">)</span><span class="w"> </span><span class="kt">uintptr</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="k">return</span><span class="w"> </span><span class="p">(</span><span class="o">*</span><span class="nx">reflect</span><span class="p">.</span><span class="nx">StringHeader</span><span class="p">)(</span><span class="nx">unsafe</span><span class="p">.</span><span class="nx">Pointer</span><span class="p">(</span><span class="o">&amp;</span><span class="nx">s</span><span class="p">)).</span><span class="nx">Data</span><span class="w">
</span><span class="p">}</span><span class="w">

</span><span class="kd">func</span><span class="w"> </span><span class="nx">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nx">s1</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="s">"123"</span><span class="w">
    </span><span class="nx">s2</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">s1</span><span class="w">
    </span><span class="nx">s3</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="s">"1"</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="s">"2"</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="s">"3"</span><span class="w">
    </span><span class="nx">s4</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="s">"12"</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nx">strconv</span><span class="p">.</span><span class="nx">FormatInt</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="mi">10</span><span class="p">)</span><span class="w">
    </span><span class="nx">s5</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="s">"12"</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nx">strconv</span><span class="p">.</span><span class="nx">FormatInt</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="mi">10</span><span class="p">)</span><span class="w">
    </span><span class="nx">fmt</span><span class="p">.</span><span class="nx">Printf</span><span class="p">(</span><span class="s">"0x%x 0x%x 0x%x 0x%x\n"</span><span class="p">,</span><span class="w">
        </span><span class="nx">stringptr</span><span class="p">(</span><span class="nx">s1</span><span class="p">),</span><span class="w"> </span><span class="c1">// 0x000049a4c2</span><span class="w">
        </span><span class="nx">stringptr</span><span class="p">(</span><span class="nx">s2</span><span class="p">),</span><span class="w"> </span><span class="c1">// 0x000049a4c2</span><span class="w">
        </span><span class="nx">stringptr</span><span class="p">(</span><span class="nx">s3</span><span class="p">),</span><span class="w"> </span><span class="c1">// 0x000049a4c2</span><span class="w">
        </span><span class="nx">stringptr</span><span class="p">(</span><span class="nx">s4</span><span class="p">),</span><span class="w"> </span><span class="c1">// 0xc000074ed0</span><span class="w">
        </span><span class="nx">stringptr</span><span class="p">(</span><span class="nx">s5</span><span class="p">),</span><span class="w"> </span><span class="c1">// 0xc000074f80</span><span class="w">
    </span><span class="p">)</span><span class="w">
</span><span class="p">}</span></pre></div><div class="step step-level-1" step="121" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="305000" data-y="0" data-z="0"><h1 id="gc-internment-2">GC: Internment #2</h1><pre class="highlight code go"><span class="kd">type</span><span class="w"> </span><span class="nx">stringInterner</span><span class="w"> </span><span class="kd">map</span><span class="p">[</span><span class="kt">string</span><span class="p">]</span><span class="kt">string</span><span class="w">

</span><span class="kd">func</span><span class="w"> </span><span class="p">(</span><span class="nx">si</span><span class="w"> </span><span class="nx">stringInterner</span><span class="p">)</span><span class="w"> </span><span class="nx">Intern</span><span class="p">(</span><span class="nx">s</span><span class="w"> </span><span class="kt">string</span><span class="p">)</span><span class="w"> </span><span class="kt">string</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="k">if</span><span class="w"> </span><span class="nx">interned</span><span class="p">,</span><span class="w"> </span><span class="nx">ok</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">si</span><span class="p">[</span><span class="nx">s</span><span class="p">];</span><span class="w"> </span><span class="nx">ok</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="k">return</span><span class="w"> </span><span class="nx">interned</span><span class="w">
    </span><span class="p">}</span><span class="w">
    </span><span class="nx">si</span><span class="p">[</span><span class="nx">s</span><span class="p">]</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">s</span><span class="w">
    </span><span class="k">return</span><span class="w"> </span><span class="nx">s</span><span class="w">
</span><span class="p">}</span><span class="w">

</span><span class="kd">func</span><span class="w"> </span><span class="nx">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nx">si</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">stringInterner</span><span class="p">{}</span><span class="w">
    </span><span class="nx">s1</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">si</span><span class="p">.</span><span class="nx">Intern</span><span class="p">(</span><span class="s">"123"</span><span class="p">)</span><span class="w">
    </span><span class="nx">s2</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">si</span><span class="p">.</span><span class="nx">Intern</span><span class="p">(</span><span class="nx">strconv</span><span class="p">.</span><span class="nx">Itoa</span><span class="p">(</span><span class="mi">123</span><span class="p">))</span><span class="w">
    </span><span class="nx">fmt</span><span class="p">.</span><span class="nx">Println</span><span class="p">(</span><span class="nx">stringptr</span><span class="p">(</span><span class="nx">s1</span><span class="p">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="nx">stringptr</span><span class="p">(</span><span class="nx">s2</span><span class="p">))</span><span class="w"> </span><span class="c1">// true</span><span class="w">
</span><span class="p">}</span></pre><div class="notes"><p>Advantage:</p><ul><li>Strings can be compared by the compiler by ptr equality.</li><li>Less memory is used.</li></ul><p>Further examples and the full impressive benchmark can be found here:</p><p><a href="https://artem.krylysov.com/blog/2018/12/12/string-interning-in-go">https://artem.krylysov.com/blog/2018/12/12/string-interning-in-go</a></p></div></div><div class="step step-level-1" step="122" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="307500" data-y="0" data-z="0"><h1 id="gc-internment-3">GC: Internment #3</h1><pre class="highlight code go"><span class="c1">// Measuring speed of string comparisons:</span><span class="w">
</span><span class="nx">BenchmarkStringCompare1</span><span class="o">-</span><span class="mi">4</span><span class="w">         </span><span class="mf">1.873</span><span class="w"> </span><span class="nx">ns</span><span class="o">/</span><span class="nx">op</span><span class="w">
</span><span class="nx">BenchmarkStringCompare10</span><span class="o">-</span><span class="mi">4</span><span class="w">        </span><span class="mf">4.816</span><span class="w"> </span><span class="nx">ns</span><span class="o">/</span><span class="nx">op</span><span class="w">
</span><span class="nx">BenchmarkStringCompare100</span><span class="o">-</span><span class="mi">4</span><span class="w">       </span><span class="mf">9.481</span><span class="w"> </span><span class="nx">ns</span><span class="o">/</span><span class="nx">op</span><span class="w">
</span><span class="nx">BenchmarkStringCompareIntern1</span><span class="o">-</span><span class="mi">4</span><span class="w">   </span><span class="mf">1.830</span><span class="w"> </span><span class="nx">ns</span><span class="o">/</span><span class="nx">op</span><span class="w">
</span><span class="nx">BenchmarkStringCompareIntern10</span><span class="o">-</span><span class="mi">4</span><span class="w">  </span><span class="mf">1.868</span><span class="w"> </span><span class="nx">ns</span><span class="o">/</span><span class="nx">op</span><span class="w">
</span><span class="nx">BenchmarkStringCompareIntern100</span><span class="o">-</span><span class="mi">4</span><span class="w"> </span><span class="mf">1.965</span><span class="w"> </span><span class="nx">ns</span><span class="o">/</span><span class="nx">op</span></pre><p class="example">Example: code/internment</p><div class="notes"><p>Internment scales incredibly well.</p><p>Good usecases:</p><ul><li>Reading words of natural language.</li><li>Enum-like strings like country names.</li><li>Interning keys of json objects.</li></ul><p>Bad usecases:</p><ul><li>Internment for input that is very long
and cannot be predicted (tweets e.g.)</li></ul></div></div><div class="step step-level-1" step="123" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="310000" data-y="0" data-z="0"><h1 id="gc-memory-limit">GC: Memory Limit</h1><pre class="highlight code bash">$<span class="w"> </span><span class="nv">GOMEMLIMIT</span><span class="o">=</span>2000M<span class="w"> </span>go<span class="w"> </span>run<span class="w"> </span>app.go</pre><img src="images/deephealth_mem.png" width="100%"></img><img src="images/gomemlimit.png" width="100%"></img><div class="notes"><p>Linux only supports setting a max amount of memory that a process (or cgroup)
may consume. If the limit is exceeded, then the process (or cgroup) is killed.
This makes the limit a hard limit, which is seldomly useful.</p><p>What is more useful is to have a soft limit, that makes the application attempt
to free memory before it reaches the limit. As the garbage collector normally
has a backlog of short-lived (i.e. memory on the heap that gets regularly freed)
it could peak over a hard limit (6G in the diagram) for a short moment of time.
By setting a GOMEMLIMIT we can tell the GC to run the</p><p>More Info:
<a href="https://weaviate.io/blog/gomemlimit-a-game-changer-for-high-memory-applications">https://weaviate.io/blog/gomemlimit-a-game-changer-for-high-memory-applications</a></p></div></div><div class="step step-level-1" step="124" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="312500" data-y="0" data-z="0"><h1 id="exercise-optimized-copy">Exercise: Optimized Copy</h1><pre class="highlight code go"><span class="kd">type</span><span class="w"> </span><span class="nx">Item</span><span class="w"> </span><span class="kd">struct</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nx">Key</span><span class="w"> </span><span class="kt">int64</span><span class="w">
    </span><span class="nx">Blob</span><span class="w"> </span><span class="p">[]</span><span class="kt">byte</span><span class="w">
</span><span class="p">}</span><span class="w">
</span><span class="kd">func</span><span class="w"> </span><span class="p">(</span><span class="nx">i</span><span class="w"> </span><span class="nx">Item</span><span class="p">)</span><span class="w"> </span><span class="nx">Copy</span><span class="p">()</span><span class="w"> </span><span class="nx">Item</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nx">blob</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nb">make</span><span class="p">([]</span><span class="kt">byte</span><span class="p">,</span><span class="w"> </span><span class="nb">len</span><span class="p">(</span><span class="nx">i</span><span class="p">.</span><span class="nx">Blob</span><span class="p">))</span><span class="w">
    </span><span class="nb">copy</span><span class="p">(</span><span class="nx">blob</span><span class="p">,</span><span class="w"> </span><span class="nx">i</span><span class="p">.</span><span class="nx">Blob</span><span class="p">)</span><span class="w">
    </span><span class="k">return</span><span class="w"> </span><span class="nx">Item</span><span class="p">{</span><span class="nx">Key</span><span class="p">:</span><span class="w"> </span><span class="nx">i</span><span class="p">.</span><span class="nx">Key</span><span class="p">,</span><span class="w"> </span><span class="nx">Blob</span><span class="p">:</span><span class="w"> </span><span class="nx">blob</span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">

</span><span class="kd">type</span><span class="w"> </span><span class="nx">Items</span><span class="w"> </span><span class="p">[]</span><span class="nx">Item</span><span class="w">
</span><span class="kd">func</span><span class="w"> </span><span class="p">(</span><span class="nx">items</span><span class="w"> </span><span class="nx">Items</span><span class="p">)</span><span class="w"> </span><span class="nx">Copy</span><span class="p">()</span><span class="w"> </span><span class="nx">Items</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nx">copyItems</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">Items</span><span class="p">{}</span><span class="w">
    </span><span class="k">for</span><span class="w"> </span><span class="nx">_</span><span class="p">,</span><span class="w"> </span><span class="nx">item</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="k">range</span><span class="w"> </span><span class="nx">items</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="nx">copyItems</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nb">append</span><span class="p">(</span><span class="nx">copyItems</span><span class="p">,</span><span class="w"> </span><span class="nx">item</span><span class="p">.</span><span class="nx">Copy</span><span class="p">())</span><span class="w">
    </span><span class="p">}</span><span class="w">
    </span><span class="k">return</span><span class="w"> </span><span class="nx">copyItems</span><span class="w">
</span><span class="p">}</span></pre><div class="notes"><p>Steps:</p><ol><li>Write a benchmark.</li><li>Pre-allocate copyItems.</li><li>Pass already-allocated items from outside ("Copy(dst Items) Items")</li><li>Allocate one big buffer and slice from that.</li></ol><p>Example taken from timeq: <a href="https://github.com/sahib/timeq/blob/main/item/item.go#L80">https://github.com/sahib/timeq/blob/main/item/item.go#L80</a></p></div></div><div class="step step-level-1" step="125" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="315000" data-y="0" data-z="0"><h1 id="virtual-memory-vm">Virtual memory (VM)</h1><img src="images/elephant_in_the_room.jpg" width="100%"></img><div class="notes"><p>Let's talk about the elephant in the room: The adress of a value
is not the adress in physical memory. How can we proof it?</p></div></div><div class="step step-level-1" step="126" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="317500" data-y="0" data-z="0"><h1 id="vm-the-mapping">VM: The mapping</h1><img src="images/virtual_memory.png" width="80%"></img><div class="notes"><ul><li>The physical memory of a system is splitted up into 4k pages.</li><li>Each process maintains a virtual memory mapping table, mapping
from the virtual range of memory to physical memory.</li><li>Address translation is handled efficiently by the MMU</li></ul><p>Wait, those addresses I saw earlier... are those the addrs in RAM?
Hopefully not, because otherwise you could somehow find out where the OpenSSH
server lives in memory and steal it's keys. For security reasons it must look
for each process like he's completely alone on the system. What you saw above
are virtual memory addresses and they stay very similar on each run.</p><p>The concept how this achieved is called "virtual memory" and it's probably one of
the more clever things we did in computer science.</p></div></div><div class="step step-level-1" step="127" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="320000" data-y="0" data-z="0"><h1 id="vm-implementation">VM: Implementation</h1><pre class="highlight code bash">$<span class="w"> </span>cat<span class="w"> </span>/proc/&lt;pid&gt;/maps<span class="w">
</span>55eab7237000-55eab7258000<span class="w"> </span>rw-p<span class="w">  </span><span class="o">[</span>heap<span class="o">]</span><span class="w">
</span>...<span class="w">
</span>7f54a1c18000-7f54a1c3a000<span class="w"> </span>r--p<span class="w">  </span>/usr/lib/libc.so.6<span class="w">
</span>...<span class="w">
</span>7ffe78a26000-7ffe78a47000<span class="w"> </span>rw-p<span class="w">  </span><span class="o">[</span>stack<span class="o">]</span></pre><p>Each process has a &#xBB;<em>Page Table</em>&#xAB; mapping virtual to physical memory.</p><div class="notes"><p>On process start this table is filled with a few default kilobytes of mapped pages
(the first few pages are not mapped, so dereferencing a NULL pointer will always crash)</p><p>When the program first accesses those addresses the CPU will generate a page fault, indicating
that there is no such mapping. The OS receives this and will find a free physical page, map
it and retry execution. If another page fault occurs the OS will kill the process with SIGSEGV.</p></div></div><div class="step step-level-1" step="128" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="322500" data-y="0" data-z="0"><h1 id="residual-memory-versus-virtual-memory">Residual memory <em>versus</em> Virtual memory</h1><img src="images/res_vs_virtual.png" width="100%"></img><div class="notes"><p>Picture above showing htop on my rather old laptop
with a normal workload. The amount of virtual memory for some programs
like signal-desktop is HUGE and only a tiny portion is actually used.</p><p>Fun fact: The program I was actively using was gimp, but the actual
performance hogs were all browser-based applications. Brave new world.</p><p>If you want to flex: Use btop for even prettier screens.</p></div></div><div class="step step-level-1" step="129" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="325000" data-y="0" data-z="0"><h1 id="vm-advantages">VM: Advantages</h1><ul><li>Pages can be mapped only once used (CoW)</li><li>Several processes can share the same pages</li><li>Pages do not need to be mapped to physical memory: Disk, DMA or even network is possible!</li><li>Processes are isolated from each other.</li><li>Processes consume only as much physical (<em>&#xBB;residual&#xAB;</em>) memory as really needed.</li><li>Programs get easier to write because they can just assume that the memory is not fragmented.</li><li>Pages can be swapped to disk by the OS without the process even noticing</li><li>The kernel can give away more memory than there is on the system (overcommiting)</li><li>Pages with the same content can be deduplicated</li><li>Kernel may steal pages of inactive processes</li></ul></div><div class="step step-level-1" step="130" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="327500" data-y="0" data-z="0"><h1 id="vm-swapping">VM: Swapping</h1><pre class="highlight code bash"><span class="c1"># Create some space for swapping:
</span>$<span class="w"> </span>dd<span class="w"> </span><span class="k">if</span><span class="o">=</span>/dev/zero<span class="w"> </span><span class="nv">of</span><span class="o">=</span>swapfile<span class="w"> </span><span class="nv">count</span><span class="o">=</span><span class="m">1024</span><span class="w"> </span><span class="nv">bs</span><span class="o">=</span>1M<span class="w">
</span>$<span class="w"> </span>swapon<span class="w"> </span>./swapfile<span class="w">

</span><span class="c1"># Check how eager the system is to use the swap
# with a value between 0-100. This is the percentage
# of RAM that is left before swapping starts.
</span>$<span class="w"> </span>cat<span class="w"> </span>/proc/sys/vm/swappiness<span class="w">
</span><span class="o">(</span>a<span class="w"> </span>value<span class="w"> </span>between<span class="w"> </span><span class="m">0</span>-100<span class="o">)</span><span class="w">

</span><span class="c1">#   0 = only swap if OOM would hit otherwise.
# 100 = swap everything not actively used.
#  60 = default for most desktops.
# &lt;10 = good setting for database servers</span></pre><div class="notes"><p>Linux can use swap space as second-prio memory if main memory runs low.
Swap is already used before memory goes low. Inactive processes and stale IO pages
get put to swap so that memory management can make use of that space to provide less
fragmented memory regions.</p><p>How aggressive this happens can be set using vm.swappiness. A value between 0 and 100.</p><p>Rules:</p><ul><li>If you want to hibernate (i.e. powerless suspend) then you need as much swap as RAM.</li><li>Otherwise about half of RAM is a good rule of thumb.</li><li>Systems that rely on low latency (i.e. anything that goes in the direction of realtime) should not swap.</li></ul></div></div><div class="step step-level-1" step="131" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="330000" data-y="0" data-z="0"><h1 id="profiling-quick-dirty">Profiling: Quick &amp; Dirty</h1><pre class="highlight code bash"><span class="c1"># Show the peak residual memory usage:
</span>$<span class="w"> </span>/usr/bin/time<span class="w"> </span>-v<span class="w"> </span>&lt;command&gt;<span class="w">
</span>...<span class="w">
</span>Maximum<span class="w"> </span>resident<span class="w"> </span><span class="nb">set</span><span class="w"> </span>size<span class="w"> </span><span class="o">(</span>kbytes<span class="o">)</span>:<span class="w"> </span><span class="m">16192</span><span class="w">
</span>...</pre><div class="line-block"><br></br><br></br></div><p class="example">Example: code/virtualmem</p><div class="notes"><p>Start ./virt and observe in htop how the virtual memory is immediately there
and the residual memory slowly increases second by second. The program will
crash if you wait long enough.</p><p>Start with '/usr/bin/time -v ./virt' and interrupt at any time.</p><p>Note that time only samples in a certain interval. If you're unlucky
you might miss the actual peak. So don't rely on values to be exact here.</p></div></div><div class="step step-level-1" step="132" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="332500" data-y="0" data-z="0"><h1 id="profiling-pprof">Profiling: <tt>pprof</tt></h1><img src="images/pprof_heap.png" width="100%"></img><div class="notes"><p>Works similar to the CPU profile and gives us a good overview.
The little cubes mean "x memory was allocated in y size batches".</p><p>The pprof output is also available as flamegraph if you prefer
this kind of presentation.</p></div></div><div class="step step-level-1" step="133" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="335000" data-y="0" data-z="0"><h1 id="profiling-monitoring">Profiling: Monitoring</h1><img src="images/memleak_grafana.png" width="100%"></img><div class="notes"><p>No way around it. Profiling and benchmarking leave a gap:
long running applications where you do not expect performance issues.
In that case you should always monitor resource usage so you can check
when and how fast memory usage increased (and maybe correlate with load)</p><p>When you notice issues you can do profiling via pprof.</p></div></div><div class="step step-level-1" step="134" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="337500" data-y="0" data-z="0"><h1 id="profiling-pyroscope">Profiling: Pyroscope</h1><img src="images/pyroscope.png" width="100%"></img><p><a href="https://pyroscope.io/docs/golang">Pyroscope</a></p><div class="notes"><p>Especially long-running memory leaks are hard to debug
(i.e. when memory accumulates over the course of several days e.g.)</p><p>In this it can help to combine monitoring and profiling. This is sometimes
called "continuous profiling" therefore. Pyroscope is one of those tools.</p><p>A short article on how to integrate this with Go services:
<a href="https://grafana.com/blog/2023/04/19/how-to-troubleshoot-memory-leaks-in-go-with-grafana-pyroscope/">https://grafana.com/blog/2023/04/19/how-to-troubleshoot-memory-leaks-in-go-with-grafana-pyroscope/</a></p><p>Demo for Go:
<a href="https://demo.pyroscope.io/?query=rideshare-app-golang.cpu%7B%7D&amp;from=1682450314&amp;until=1682450316">https://demo.pyroscope.io/?query=rideshare-app-golang.cpu%7B%7D&amp;from=1682450314&amp;until=1682450316</a></p><p>Another way to enable continuous profiling is to use eBPF - small programs that run in kernel space.
That's more lowlevel, but is pretty much the most powerful ever and I expect many tools to emerge in
the next year that make use of it.</p></div></div><div class="step step-level-1" step="135" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="340000" data-y="0" data-z="0"><h1 id="the-oom-killer">The OOM Killer</h1><img src="images/oom.jpg" width="50%"></img><div class="notes"><ul><li>Kicks in if system almost completely ran out of RAM.</li><li>Selects a process based on a scoring system and kills it.</li><li>Processes can be given a priority in advance.</li><li>Last resort mechanism.</li><li>Reports in dmesg.</li><li>Sometimes comes too late and is not able to operate anymore.</li></ul><p>Alternatives:</p><ul><li>earlyoom</li><li>systemd-oomd</li></ul><p>Userspace-Daemons that monitor memory usage and kill processes
in a very configurable way. Well suited for server systems.</p></div></div><div class="step step-level-1" step="136" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="342500" data-y="0" data-z="0"><h1 id="fynn-2">Fynn!</h1><div class="line-block"><br></br></div><p class="big-text">&#x1F3C1;</p><div class="line-block"><br></br></div><p class="next-link"><strong>Next:</strong> <a href="../4_io/index.html">I/O &amp; Syscalls</a>: Speaking with the kernel &#x1F427;</p><div class="notes"><p>IMPROVEMENT IDEA:</p><p>Maybe cut a bit content and do more of the useful stuff. Do more examples,
maybe one big example that grows with time?</p></div></div><div class="step step-level-1" step="137" data-x="345000" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-y="0" data-z="0"><p class="chapter">I/O &amp; Syscalls</p><p>Speaking with the kernel &#x1F427;</p></div><div class="step step-level-1" step="138" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="347500" data-y="0" data-z="0"><h1 id="agenda-3">Agenda</h1><ul><li>How to store bits?</li><li>How does the kernel talk to the storage?</li><li>How can we do I/O over syscalls?</li><li>Profiling and benchmarking.</li><li>Some performance tips.</li></ul><img src="images/waterpipe.png" width="50%"></img></div><div class="step step-level-1" step="139" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="350000" data-y="0" data-z="0"><h1 id="typical-terms">Typical terms</h1><ul><li><em>Latency:</em> Time until the first drop of water arrives.</li><li><em>Throughput:</em> Current volume of water per time.</li><li><em>Bandwidth:</em> Maximum throughput. (liter/time)</li></ul><div class="line-block"><br></br><br></br></div><table cellpadding="0" cellspacing="0" class="colwidths-given"><thead><tr><th><p>Examples:</p></th><th><p><em>Low latency</em></p></th><th><p><em>High latency</em></p></th></tr></thead><tbody><tr><td><p><em>Low throughput</em></p></td><td><p><strong>SDCards</strong></p></td><td><p><strong>SSHFS</strong></p></td></tr><tr><td><p><em>High throughput</em></p></td><td><p><strong>SSD</strong></p></td><td><p><strong>HDD</strong></p></td></tr></tbody></table><div class="notes"><p>Fun fact: An extreme example of high latency with high throughput is IPoAC
(IP over Avian Carrier), i.e. sticking an USB stick on a homing pidgeon.
This was even standardized (jokingly):
<a href="https://en.wikipedia.org/wiki/IP_over_Avian_Carriers">https://en.wikipedia.org/wiki/IP_over_Avian_Carriers</a></p></div></div><div class="step step-level-1" step="140" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="352500" data-y="0" data-z="0"><h1 id="hardware-hdds">Hardware: HDDs</h1><img src="images/hdd.jpg" width="60%"></img><div class="line-block"><br></br></div><ul><li>Rotational, stacked disks with reading head.</li><li>Reading head needs to seek to the right position.</li><li>Elevator algorithm for ordering seeks.</li><li>Performance loss at high or low temperature.</li><li>Does not work if moved - bad for laptops.</li><li>Dying, but battled tested &amp; still widely used.</li></ul><div class="notes"><p>Big advantage: You could debug issues with too many seeks by audio!</p></div></div><div class="step step-level-1" step="141" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="355000" data-y="0" data-z="0"><h1 id="hardware-ssds">Hardware: SSDs</h1><img src="images/ssd.jpg" width="70%"></img><div class="line-block"><br></br></div><ul><li>NAND Flash technology (like USB sticks)</li><li>No expensive seek necessary.</li><li>Limited number of write cycles.</li><li>Becoming cheaper and better every year.</li></ul><div class="notes"><p>Write software for SSDs. There were some crazy tricks like FIEMAP to make
applications re-order their reads in the order of how they are placed on disk.
(Huge speedup on HDD, small speedup on SSD), but those will become pointless
more and more.</p></div></div><div class="step step-level-1" step="142" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="357500" data-y="0" data-z="0"><h1 id="ssd-write-amplification">SSD Write amplification</h1><img src="images/ssd_write_amplification.png" width="100%"></img><div class="notes"><p>Source: <a href="http://databasearchitects.blogspot.com/2021/06/what-every-programmer-should-know-about.html?m=1">http://databasearchitects.blogspot.com/2021/06/what-every-programmer-should-know-about.html?m=1</a></p><p>SSDs are divided into blocks (512kb), which are divided into pages (often 4K).
Pages can be read or overwritten. Pages cannot be erased, only blocks can be. Updates of a pages are written to new blocks.
If space runs out, old blocks with many stale pages are erased and can be re-used.
The number of physical writes is therefore higher than the number of logical writes.
The more space is used, the higher the write amplication factor though.</p><p>What we can do about it: Buy bigger SSDs than you need. Also avoid rewriting pages if possible.
Secret: SSD have some spare space to keep working they don't tell you about.</p><p>Also enable TRIM support if your OS did not yet, but nowadways always enabled.
This makes it possible for the OS to tell the SSD additional blocks that are not needed anymore.</p></div></div><div class="step step-level-1" step="143" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="360000" data-y="0" data-z="0"><h1 id="kill-it-with-hardware-raid0">Kill it with Hardware: RAID0</h1><img src="images/raid0.png" width="50%"></img><div class="notes"><p>Let's be honest: I/O is one of the cases where it's the easiest to kill the problem
by throwing a lot of hardware on it. The easiest way to increase the available bandwidth
is using a RAID0, i.e. coupling several disk to build one logical unit out of them.
Depending on your usecase you can of course use other raid levels:</p><p><a href="https://en.wikipedia.org/wiki/Standard_RAID_levels">https://en.wikipedia.org/wiki/Standard_RAID_levels</a></p><p>But that's not the point of this workshop. The point is how you can increase the throughput
of your applications so you're able to reach this bandwidth (and maybe also on how you can
defer having to buy more hard disks).</p></div></div><div class="step step-level-1" step="144" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="362500" data-y="0" data-z="0"><h1 id="everything-is-a-file">Everything is a file</h1><img src="images/everything-is-afile.webp" width="100%"></img><div class="notes"><p>Even memory is a file: /dev/mem
Or a complete usb stick: /dev/sda
Or randomnes: /dev/urandom</p></div></div><div class="step step-level-1" step="145" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="365000" data-y="0" data-z="0"><h1 id="virtual-file-system">Virtual File System</h1><img src="images/vfs.webp" width="100%"></img><div class="notes"><p>Below device drivers: hardware controllers - beyond this talk.
They can also re-order writes and are mostly concerned with durability,
i.e. a SSD controller will try to distribute the blocks he used to make sure
they have a similar amount of write cycles.</p></div></div><div class="step step-level-1" step="146" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="367500" data-y="0" data-z="0"><h1 id="how-do-syscalls-work">How do syscalls work?</h1><pre class="highlight code c"><span class="c1">// Example: writing to a file
// as documented in glibc:
// ssize_t write(
//     int fd,           // file descriptor
//     const void buf[], // data
//     size_t count      // size of data
// );
</span><span class="n">write</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="s">"Hello world!</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span><span class="w"> </span><span class="mi">12</span><span class="p">);</span></pre></div><div class="step step-level-1" step="147" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="370000" data-y="0" data-z="0"><p><strong>Compiled:</strong></p><pre class="highlight code asm"><span class="c1">; use the `write` system call (1)
</span><span class="nf">movl</span><span class="w"> </span><span class="no">rax</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="w">
</span><span class="c1">; write to stdout (1) - 1st arg
</span><span class="nf">movl</span><span class="w"> </span><span class="no">rbx</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="w">
</span><span class="c1">; use string "Hello World" - 2nd arg
; (0x1234 is the addr of the "Hello World!\0")
</span><span class="nf">movl</span><span class="w"> </span><span class="no">rcx</span><span class="p">,</span><span class="w"> </span><span class="mi">0x1234</span><span class="w">
</span><span class="c1">; write 12 characters - 3rd arg
</span><span class="nf">movl</span><span class="w"> </span><span class="no">rdx</span><span class="p">,</span><span class="w"> </span><span class="mi">12</span><span class="w">
</span><span class="c1">; make system call via special instruction
</span><span class="nf">syscall</span><span class="w">
</span><span class="c1">; The return code is now in the RAX register.</span></pre><div class="notes"><p>Disclaimer: The 'syscall' instruction is not the only instruction and kind of deprecated
in favor of another one. But it's similar enough and better to explain.</p><p>All available syscalls and their ids are here: <a href="https://filippo.io/linux-syscall-table/">https://filippo.io/linux-syscall-table/</a></p><p>Only method of userspace to talk to kernel. How to call is ISA specific.</p><p>The syscall instruction performs a context switch: This means the current
state of the process (i.e. the state of all registers in the CPU) is saved
away, so it can be restored later. Once done, the kernel sets the register
to its needs, does whatever is required to serve the system call. When
finished, the process state is restored and execution continues.</p><p>Context switches also happen when you're not calling any syscalls.
Simply when the scheduler decide this process is done with execution.</p></div></div><div class="step step-level-1" step="148" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="372500" data-y="0" data-z="0"><h1 id="typical-syscalls">Typical syscalls</h1><ul><li>IO: <tt>read</tt>, <tt>write</tt>, <tt>close</tt></li><li>Files: <tt>stat</tt>, <tt>chmod</tt>, <tt>mkdir</tt></li><li>Memory: <tt>sbrk</tt>, <tt>mmap</tt></li><li>Processes: <tt>fork</tt>, <tt>kill</tt>, <tt>wait</tt></li><li>Network: <tt>listen</tt>, <tt>connect</tt>, <tt>epoll</tt></li><li>Mysterious: <tt>ioctl</tt>, <tt>chroot</tt>, <tt>mount</tt></li></ul><div class="notes"><p>There is a syscall for every single thing that userspace cannot do without the kernel's help.</p><p>Luckily for us, glibc and Go provide us nice names and interfaces to make those system calls.
They usually provide thin wrappers that also do some basic error checking. Watch out: <tt>fread</tt>
is doing buffering in userspace!</p><p>Can anyone think of another syscall not in the list above? exit! chdir ...
(There are about 300 of them)</p><p>Also, what things are no syscalls? Math, random numbers, cryptography, ...
i.e. everything that can be done without any side effects or hardware.</p></div></div><div class="step step-level-1" step="149" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="375000" data-y="0" data-z="0"><h1 id="use-the-man-luke">Use the man, Luke!</h1><pre class="highlight code bash">$<span class="w"> </span>man<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="nb">read</span></pre><div class="notes"><p>Every man page in section refers to a system call.</p></div></div><div class="step step-level-1" step="150" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="377500" data-y="0" data-z="0"><h1 id="prayer-of-syscalls">Prayer of Syscalls</h1><p>&#xBB;Reduce the number of syscalls and thou shalt be blessed!&#xAB;</p></div><div class="step step-level-1" step="151" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="380000" data-y="0" data-z="0"><h1 id="typical-read-i-o">Typical read I/O</h1><pre class="highlight code c"><span class="kt">char</span><span class="w"> </span><span class="n">buf</span><span class="p">[</span><span class="mi">1024</span><span class="p">];</span><span class="w">
</span><span class="kt">int</span><span class="w"> </span><span class="n">fd</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">open</span><span class="p">(</span><span class="s">"/some/path"</span><span class="p">,</span><span class="w"> </span><span class="n">O_CREAT</span><span class="o">|</span><span class="n">O_RDONLY</span><span class="o">|</span><span class="n">O_TRUNC</span><span class="p">);</span><span class="w">
</span><span class="kt">size_t</span><span class="w"> </span><span class="n">bytes_read</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w">
</span><span class="k">while</span><span class="p">((</span><span class="n">bytes_read</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">read</span><span class="p">(</span><span class="n">fd</span><span class="p">,</span><span class="w"> </span><span class="n">buf</span><span class="p">,</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="n">buf</span><span class="p">)))</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="cm">/* do something with buf[:bytes_read] */</span><span class="w">
</span><span class="p">}</span><span class="w">
</span><span class="n">close</span><span class="p">(</span><span class="n">fd</span><span class="p">);</span></pre><div class="notes"><p>There are two costs here: Copying the data and context switching.</p><p>Looks fairly straightforward and most of you might have written something like that already.
Maybe even for sockets or other streams. BUT here's the thing: every read needs one syscall
and all bytes from the file are copied to a userspace-supplied buffer. This model is flexible,
but costs performance. With mmap() and io_uring we will see options that can, sometimes,
work with zero copies.</p><p>Sidenote: Always be nice and close your file descriptors.
That has two reasons:</p><ul><li>You are only allowed a certain maximum of file descriptors per process.
(check with  ulimit -a for soft limits and ulimit -aH for hard limits)</li><li>If you write something to a file close will also flush file contents
that are not written to disk yet.</li></ul></div></div><div class="step step-level-1" step="152" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="382500" data-y="0" data-z="0"><h1 id="typical-write-i-o">Typical write I/O</h1><pre class="highlight code c"><span class="kt">char</span><span class="w"> </span><span class="n">buf</span><span class="p">[</span><span class="mi">1024</span><span class="p">];</span><span class="w">
</span><span class="kt">size_t</span><span class="w"> </span><span class="n">bytes_in_buf</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w">
</span><span class="kt">int</span><span class="w"> </span><span class="n">fd</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">open</span><span class="p">(</span><span class="s">"/some/path"</span><span class="p">,</span><span class="w"> </span><span class="n">O_CREAT</span><span class="o">|</span><span class="n">O_WRONLY</span><span class="o">|</span><span class="n">O_TRUNC</span><span class="p">);</span><span class="w">
</span><span class="k">do</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="cm">/* fill buf somehow with data you'd like to write,
     * set bytes_in_buf accordingly.
     */</span><span class="w">
</span><span class="p">}</span><span class="w"> </span><span class="k">while</span><span class="p">(</span><span class="n">write</span><span class="p">(</span><span class="n">fd</span><span class="p">,</span><span class="w"> </span><span class="n">buf</span><span class="p">,</span><span class="w"> </span><span class="n">bytes_in_buf</span><span class="p">)</span><span class="w"> </span><span class="o">&gt;=</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w">
</span><span class="n">fsync</span><span class="p">(</span><span class="n">fd</span><span class="p">);</span><span class="w">
</span><span class="n">close</span><span class="p">(</span><span class="n">fd</span><span class="p">);</span></pre><div class="notes"><p>Q1: Does this mean that the data is available to read() when write() returned?
Q2: Is the data saved on disk after write() returns?</p><dl><dt>A1: Mostly. There might be exotic edge cases with non-POSIX filesystems,</dt><dd><p>but you should mostly be able to assume this.</p></dd><dt>A2: No. You should call fsync() to ensure that and even than, it is</dt><dd><p>sadly not guaranteed depending on the storage driver and hardware.
(Kernel has to rely on the hardware to acknowledge received data)</p></dd></dl><p>---</p><p>There is a bug here though:</p><p>write() returns the number of written bytes. It might be less than bytes_in_buf
and this is not counted as an error. The write call might have simply been
interrupted and we expect that it is called another time with the remaining data.
This only happens if your program uses POSIX signals that were not registed with
the SA_RESTART flag (see man 7 signal). Since it's default, it's mostly not an
issue in C.</p><p>Go hides this edgecase for you in normal likes fd.Write() or io.ReadAll().
However, the Go runtime uses plenty of signals and if you use the syscalls
package for some reason, then you might be hit by this kind of bug.
This does not affect only write() but also read() and many other syscalls.</p><p>Also please note: There is some error handling missing here.</p></div></div><div class="step step-level-1" step="153" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="385000" data-y="0" data-z="0"><h1 id="sidenote-apis-are-important">Sidenote: APIs are important</h1><pre class="highlight code go"><span class="c1">// Don't: No pre-allocation possible</span><span class="w">
</span><span class="kd">func</span><span class="w"> </span><span class="nx">ReadEntry</span><span class="p">()</span><span class="w"> </span><span class="p">([]</span><span class="kt">byte</span><span class="p">,</span><span class="w"> </span><span class="kt">error</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="c1">// allocate buffer, fill and return it.</span><span class="w">
</span><span class="p">}</span></pre><pre class="highlight code go"><span class="c1">// Better: buf can be pre-allocated.</span><span class="w">
</span><span class="kd">func</span><span class="w"> </span><span class="nx">ReadEntry</span><span class="p">(</span><span class="nx">buf</span><span class="w"> </span><span class="p">[]</span><span class="kt">byte</span><span class="p">)</span><span class="w"> </span><span class="kt">error</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="c1">// use buf, append to it.</span><span class="w">
</span><span class="p">}</span></pre><pre class="highlight code go"><span class="c1">// Do: Open the reader only once to</span><span class="w">
</span><span class="c1">// reduce number of syscalls</span><span class="w">
</span><span class="kd">func</span><span class="w"> </span><span class="nx">ReadEntry</span><span class="p">(</span><span class="nx">r</span><span class="w"> </span><span class="nx">io</span><span class="p">.</span><span class="nx">Reader</span><span class="p">,</span><span class="w"> </span><span class="nx">buf</span><span class="w"> </span><span class="p">[]</span><span class="kt">byte</span><span class="p">)</span><span class="w"> </span><span class="kt">error</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="c1">// use buf, append to it.</span><span class="w">
</span><span class="p">}</span></pre><div class="notes"><p>This is a reminder to the last session. Many Read()-like functions
get passed a buffer in, instead of allocating one. This is good practice,
as it allows calling ReadEntry() in a loop and re-using a buffer during that.
Even better is of course no copying the data at all, but that's a different story.</p></div></div><div class="step step-level-1" step="154" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="387500" data-y="0" data-z="0"><h1 id="buffered-i-o">&#xBB;Buffered&#xAB; I/O</h1><ul><li>Almost all I/O is buffered, but some is double buffered.</li><li><tt>fread()</tt>: Does buffering in userspace; calls <tt>read()</tt>.</li><li><tt>bufio.Reader</tt>: Same thing in Go.</li></ul><p><strong>Usecases:</strong></p><ul><li>You need to read byte by byte.</li><li>You need to "unread" some bytes frequently.</li><li>You need to read easily line by line.</li><li>You have logic that does small reads.</li></ul><p><em>Otherwise:</em> Prefer the simpler version.</p><div class="notes"><p>Userspace buffered functions. No real advantage, but limiting and confusing
API. Has some extra features like printf-style formatting. Since it imposes
another copy from its internal buffer to your buffer and since it uses
dynamic allocation for the FILE structure I tend to avoid it.</p><p>In Go the normal read/write is using the syscall directly,
bufio is roughly equivalent to f{read,write} etc.
fsync() is a syscall, not part of that even though it starts with "f"</p></div></div><div class="step step-level-1" step="155" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="390000" data-y="0" data-z="0"><h1 id="syscalls-are-expensive">Syscalls are expensive</h1><pre class="highlight code bash">$<span class="w"> </span>dd<span class="w"> </span><span class="k">if</span><span class="o">=</span>/dev/urandom<span class="w"> </span><span class="nv">of</span><span class="o">=</span>./x<span class="w"> </span><span class="nv">bs</span><span class="o">=</span>1M<span class="w"> </span><span class="nv">count</span><span class="o">=</span><span class="m">1024</span><span class="w">
</span>$<span class="w"> </span>dd<span class="w"> </span><span class="k">if</span><span class="o">=</span>x<span class="w"> </span><span class="nv">of</span><span class="o">=</span>/dev/null<span class="w"> </span><span class="nv">bs</span><span class="o">=</span>1b<span class="w">
</span><span class="m">4</span>,07281<span class="w"> </span>s,<span class="w"> </span><span class="m">264</span><span class="w"> </span>MB/s<span class="w">
</span>$<span class="w"> </span>dd<span class="w"> </span><span class="k">if</span><span class="o">=</span>x<span class="w"> </span><span class="nv">of</span><span class="o">=</span>/dev/null<span class="w"> </span><span class="nv">bs</span><span class="o">=</span>32b<span class="w">
</span><span class="m">0</span>,255229<span class="w"> </span>s,<span class="w"> </span><span class="m">4</span>,2<span class="w"> </span>GB/s<span class="w">
</span>$<span class="w"> </span>dd<span class="w"> </span><span class="k">if</span><span class="o">=</span>x<span class="w"> </span><span class="nv">of</span><span class="o">=</span>/dev/null<span class="w"> </span><span class="nv">bs</span><span class="o">=</span>1024b<span class="w">
</span><span class="m">0</span>,136717<span class="w"> </span>s,<span class="w"> </span><span class="m">7</span>,9<span class="w"> </span>GB/s<span class="w">
</span>$<span class="w"> </span>dd<span class="w"> </span><span class="k">if</span><span class="o">=</span>x<span class="w"> </span><span class="nv">of</span><span class="o">=</span>/dev/null<span class="w"> </span><span class="nv">bs</span><span class="o">=</span>32M<span class="w">
</span><span class="m">0</span>,206027<span class="w"> </span>s,<span class="w"> </span><span class="m">5</span>,2<span class="w"> </span>GB/s</pre><p>Good buffer sizes: <span class="math ">\(1k - 32k\)</span></p><div class="notes"><p>Each syscall needs to store away the state of all registers in the CPU
and restore it after it finished. This is called "context switch".</p><p>Many syscalls vs a few big ones.</p><p>Try to reduce the number of syscalls,
but too big buffers hurt too.</p></div></div><div class="step step-level-1" step="156" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="392500" data-y="0" data-z="0"><h1 id="making-syscalls-visible">Making syscalls visible</h1><pre class="highlight code bash"><span class="c1"># (Unimportant output skipped)
</span>$<span class="w"> </span>strace<span class="w"> </span>ls<span class="w"> </span>-l<span class="w"> </span>/tmp<span class="w">
</span>openat<span class="o">(</span>AT_FDCWD,<span class="w"> </span><span class="s2">"/tmp"</span>,<span class="w"> </span>...<span class="o">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">4</span><span class="w">
</span>getdents64<span class="o">(</span><span class="m">4</span>,<span class="w"> </span>/*<span class="w"> </span><span class="m">47</span><span class="w"> </span>entries<span class="w"> </span>*/,<span class="w"> </span><span class="m">32768</span><span class="o">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2256</span><span class="w">
</span>...<span class="w">
</span>statx<span class="o">(</span>AT_FDCWD,<span class="w"> </span><span class="s2">"/tmp/file"</span>,<span class="w"> </span>...<span class="o">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="w">
</span>getxattr<span class="o">(</span><span class="s2">"/tmp/file"</span>,<span class="w"> </span>...<span class="o">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>-1<span class="w"> </span>ENODATA<span class="w">
</span>...<span class="w">
</span>write<span class="o">(</span><span class="m">1</span>,<span class="w"> </span><span class="s2">"r-- 8 sahib /tmp/file"</span>,<span class="w"> </span>...<span class="o">)</span></pre><div class="notes"><p>Insanely useful tool to debug hanging tools
or tools that crash without a proper error message.
Usually the last syscall they do gives a hint.</p><p>Important options:</p><p>-C: count syscalls and stats at the end.</p><p>-f: follow also subprocesses.</p><p>-e: Trace only specific syscalls.</p></div></div><div class="step step-level-1" step="157" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="395000" data-y="0" data-z="0"><h1 id="page-cache">Page cache</h1><img src="images/page-cache.png" width="100%"></img><div class="notes"><ul><li>All I/O access is cached using the page cache (dir + inode)</li><li>Free pages are used to store recently accessed file contents.</li><li>Performance impact can be huge.</li><li>Writes are asynchronous, i.e. synced later</li></ul><p>Good overview and more details here:
<a href="https://biriukov.dev/docs/page-cache/2-essential-page-cache-theory/">https://biriukov.dev/docs/page-cache/2-essential-page-cache-theory/</a></p></div></div><div class="step step-level-1" step="158" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="397500" data-y="0" data-z="0"><h1 id="caveat-writes-are-buffered">Caveat: Writes are buffered!</h1><pre class="highlight code bash"><span class="c1"># wait for ALL buffers to be flushed:
</span>$<span class="w"> </span>sync<span class="w">
</span><span class="c1"># pending data is now safely stored.</span></pre><pre class="highlight code c"><span class="c1">// wait for specific file to be flushed:
</span><span class="k">if</span><span class="p">(</span><span class="n">fsync</span><span class="p">(</span><span class="n">fd</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
     </span><span class="c1">// error handling
</span><span class="p">}</span><span class="w">
</span><span class="c1">// pending data is now safely stored.</span></pre><div class="notes"><p>That's why we have the sync command before the drop_cache command.</p></div></div><div class="step step-level-1" step="159" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="400000" data-y="0" data-z="0"><h1 id="clearing-the-cache">Clearing the cache</h1><p>For I/O benchmarks <em>always</em> clear caches:</p><pre class="highlight code bash"><span class="c1"># 1: Clear page cache only.
# 2: Clear inodes/direntries cache.
# 3: Clear both.
</span>sync<span class="p">;</span><span class="w"> </span><span class="nb">echo</span><span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>sudo<span class="w"> </span>tee<span class="w"> </span>/proc/sys/vm/drop_caches</pre><div class="line-block"><br></br></div><p class="example">Example: code/io_cache</p></div><div class="step step-level-1" step="160" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="402500" data-y="0" data-z="0"><h1 id="alternative-to-fsync">Alternative to <tt>fsync()</tt></h1><pre class="highlight code bash"><span class="c1"># Move is atomic!
</span>$<span class="w"> </span>cp<span class="w"> </span>/src/bigfile<span class="w"> </span>/dst/bigfile.tmp<span class="w">
</span>$<span class="w"> </span>mv<span class="w"> </span>/dst/bigfile.tmp<span class="w"> </span>/dst/bigfile</pre><div class="notes"><p>This only works obviously if you're not constantly updating the file,
i.e. for files that are written just once.</p></div></div><div class="step step-level-1" step="161" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="405000" data-y="0" data-z="0"><h1 id="detour-filesystems">Detour: Filesystems</h1><p>Defines layout of files on disk:</p><ul><li><strong>ext2/3/4</strong>: good, stable &amp; fast choice.</li><li><strong>fat8/16/32</strong>: simple, but legacy; avoid</li><li><strong>NTFS</strong>: slow and only for compatibility.</li><li><strong>XFS</strong>: good with big files.</li><li><strong>btrfs</strong>: feature-rich, can do CoW &amp; snapshots.</li><li><strong>ZFS</strong>: highly scalable and very complex.</li><li><strong>sshfs</strong>: remote access over FUSE</li><li>...</li></ul><div class="notes"><p>Do you know what filesystems you use? What filesystems you know?</p><p>Actual implementation of read/write/etc. for a single
filesystem like FAT, ext4, btrfs. There are different ways
to layout and maintain data on disk, depending on your use case.</p><p>Syscalls all work the same, but some filesystems have
better performance regarding writes/reads/syncs or
are more targeted at large files or many files.</p><p>Most differences are admin related (i.e. integrity, backups,
snapshots etc.) and not so much performance related. But if you
need things like snapshots and don't want external tools then
btrfs of ZFS are incredibly fast.</p></div></div><div class="step step-level-1" step="162" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="407500" data-y="0" data-z="0"><h1 id="detour-fragmentation">Detour: Fragmentation</h1><img src="images/windows_fragmentation.jpg" width="100%"></img><div class="notes"><p>What OS do you think of when you hear "defragmentation"? Right, Windows.
Why? Because NTFS used to suffer from it quite heavily.
FAT suffered even more from this.</p><p>Fragmentation means that the content of a file is not stored as one
continuous block, but in several blocks that might be scattered all over
the place, possibly even out-of-order (Block B before Block A). With
rotational disk this was in issue since the reading head had to jump all
over the place to read a single file. This caused noticeable pauses.</p><p>Thing is: Linux filesystems rarely require defragmentation and if
you are in need of defragmentation you are probably using an exotic enough
setup that you know why.</p><p>Most Linux filesystems have strategies to actively, defragment files (i.e.
bringing the parts of the file closer together) during writes to that file.
In practice, it does not matter anymore today.</p></div></div><div class="step step-level-1" step="163" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="410000" data-y="0" data-z="0"><h1 id="detour-tweaking">Detour: Tweaking</h1><ul><li>Do not fill up your filesystem.</li><li>Do not stack layers (<tt>overlayfs</tt>, <tt>luks</tt>, <tt>mdadm</tt>)</li><li>Do not enable <tt>atime</tt> (Access time, <tt>noatime</tt>)</li><li>Disable journaling if you like to live risky.</li></ul><div class="notes"><p>Performance is not linear. The fuller the FS is the,
more it will be busy with background processes cleaning
things up.</p><p>Stacking filesystems (like with using encryption) can slow things
down. Often this without alternatives though. Only with RAID you
have the option to choose hardware RAID.</p><p>Journaling filesystems like ext4 use something like a WAL. They write the
metadata and/or data to a log before integrating it into the actual
data structure (which is more complex and takes longer to commit).
Data is written twice therefore with the advantage of being able to
recover it on crash or power loss. Disabling it speeds things up
at the risk of data loss (which might be okay on some servers).</p></div></div><div class="step step-level-1" step="164" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="412500" data-y="0" data-z="0"><h1 id="detour-fuse">Detour: FUSE</h1><img src="images/fuse.png" width="100%"></img><div class="notes"><p>Examples of FUSE filesystems:</p><ul><li>s3fs</li><li>sshfs</li><li>ipfs / brig</li></ul><p>FUSE gives you very decent performance,
as most of the logic still runs in kernel space.</p></div></div><div class="step step-level-1" step="165" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="415000" data-y="0" data-z="0"><h1 id="mmap"><tt>mmap()</tt></h1><pre class="highlight code c"><span class="c1">// Handle files like arrays:
</span><span class="kt">int</span><span class="w"> </span><span class="n">fd</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">open</span><span class="p">(</span><span class="s">"/var/tmp/file1.db"</span><span class="p">)</span><span class="w">
</span><span class="kt">char</span><span class="w"> </span><span class="o">*</span><span class="n">map</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mmap</span><span class="p">(</span><span class="w">
    </span><span class="nb">NULL</span><span class="p">,</span><span class="w">                 </span><span class="c1">// addr
</span><span class="w">    </span><span class="mi">1024</span><span class="w">                  </span><span class="c1">// map size
</span><span class="w">    </span><span class="n">PROT_READ</span><span class="o">|</span><span class="n">PROT_WRITE</span><span class="p">,</span><span class="w"> </span><span class="c1">// acess flags
</span><span class="w">    </span><span class="n">MAP_SHARED</span><span class="w">            </span><span class="c1">// private or shared
</span><span class="w">    </span><span class="n">fd</span><span class="p">,</span><span class="w">                   </span><span class="c1">// file descriptor
</span><span class="w">    </span><span class="mi">0</span><span class="w">                     </span><span class="c1">// offset
</span><span class="p">);</span><span class="w">

</span><span class="c1">// copy string to file with offset
</span><span class="n">map</span><span class="p">[</span><span class="mi">20</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="sc">'H'</span><span class="p">;</span><span class="w"> </span><span class="n">map</span><span class="p">[</span><span class="mi">21</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="sc">'e'</span><span class="p">;</span><span class="w"> </span><span class="n">map</span><span class="p">[</span><span class="mi">22</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="sc">'l'</span><span class="p">;</span><span class="w"> </span><span class="n">map</span><span class="p">[</span><span class="mi">23</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="sc">';'</span><span class="p">;</span><span class="w">
</span><span class="n">map</span><span class="p">[</span><span class="mi">24</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="sc">'W'</span><span class="p">;</span><span class="w"> </span><span class="n">map</span><span class="p">[</span><span class="mi">25</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="sc">'o'</span><span class="p">;</span><span class="w"> </span><span class="n">map</span><span class="p">[</span><span class="mi">26</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="sc">'r'</span><span class="p">;</span><span class="w"> </span><span class="n">map</span><span class="p">[</span><span class="mi">27</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="sc">'d'</span><span class="p">;</span></pre><p class="example">Example: code/mmap</p><div class="notes"><p>Benchmarking IO is especially hard: Often you just benchmark the speed of your page cache
for reading/writing. Always clear your cache and use fsync() during benchmarking extensivey!</p></div></div><div class="step step-level-1" step="166" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="417500" data-y="0" data-z="0"><img src="images/mmap.png" width="80%"></img><div class="notes"><p>Maybe one of the most mysterious and powerful features we have on Linux.</p><p>Typical open/read/write/close APIs see files as streams. They are awkward to
use if you need to jump around a lot in the file itself (like some datbases do).</p><p>With mmap() we can handle files as arrays and let the kernel manage
reading/writing the required data from us magically on access. See m[17] above,
it does not require reading the respective part of the file explicitly.</p><p>Good mmap use cases:</p><ul><li>Reading large files (+ telling the OS how to read)</li><li>Jumping back and forth in big files.</li><li>Sharing the file data with several processes in a very efficient way.</li><li>Zero copy during reading! No buffering needed.</li><li>Ease-of-use. No buffers, no file handles, just arrays.</li></ul><p>Image source:</p><p><a href="https://biriukov.dev/docs/page-cache/5-more-about-mmap-file-access/">https://biriukov.dev/docs/page-cache/5-more-about-mmap-file-access/</a></p></div></div><div class="step step-level-1" step="167" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="420000" data-y="0" data-z="0"><h1 id="mmap-controversy"><tt>mmap()</tt> controversy</h1><img src="images/mmap_for_db.png" width="42%"></img><div class="line-block"><br></br></div><ul><li>Some databases use <tt>mmap()</tt> (<em>Influx, sqlite3, ...</em>)</li><li>Some people <a href="https://db.cs.cmu.edu/mmap-cidr2022">advise vehemently against it</a>. &#x1F4A9;</li><li>For good reasons, but it's complicated.</li><li>Main argument: Not enough control &amp; safety.</li><li>For some usecases <tt>mmap()</tt> is fine for databases.</li></ul></div><div class="step step-level-1" step="168" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="422500" data-y="0" data-z="0"><h1 id="to-sync-or-to-async">To sync or to async? &#x1F914;</h1><img src="images/sync_async.jpg" width="90%"></img><div class="notes"><p><a href="https://unixism.net/loti/async_intro.html">https://unixism.net/loti/async_intro.html</a></p><p>The image below can be achieved using special system calls like epoll(), poll() or select():
They "multiplex" between several files. Basically they work all the same: You given them
a list of files and once invoked epoll() waits until one of the files are ready to be read from.
This minimizes polling on userspace side and keeps the wait between I/O as low as possible.</p><p>This is however only possible for network I/O - normal files cannot be polled.
Beyond the scope of this talk however.</p></div></div><div class="step step-level-1" step="169" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="425000" data-y="0" data-z="0"><h1 id="io-uring"><tt>io_uring</tt></h1><img src="images/iouring.png" width="100%"></img><div class="notes"><p>A technique to introduce polling mechanisms to files too and benefit from it.</p><p>SQ: Submission Queue: Commands like read file 123 at offset 42.
CQ: Completion Queue: Here is the dat aof file 123 at offset 42.</p><p>Advantage: Does only need syscalls during the setup of the interface, but not
during operation as the data transfer is done via a memory mapping that has been
set up during the setup phase.</p></div></div><div class="step step-level-1" step="170" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="427500" data-y="0" data-z="0"><h1 id="myth-o-direct">Myth: <tt>O_DIRECT</tt> &#x1F44E;</h1><pre class="highlight code c"><span class="c1">// Skip the page cache; see `man 2 open`
</span><span class="kt">int</span><span class="w"> </span><span class="n">fd</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">open</span><span class="p">(</span><span class="s">"/some/file"</span><span class="p">,</span><span class="w"> </span><span class="n">O_DIRECT</span><span class="o">|</span><span class="n">O_RDONLY</span><span class="p">);</span><span class="w">

</span><span class="c1">// No use of the page cache here:
</span><span class="kt">char</span><span class="w"> </span><span class="n">buf</span><span class="p">[</span><span class="mi">1024</span><span class="p">];</span><span class="w">
</span><span class="n">read</span><span class="p">(</span><span class="n">fd</span><span class="p">,</span><span class="w"> </span><span class="n">buf</span><span class="p">,</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="n">buf</span><span class="p">));</span></pre><div class="notes"><p>This flag can be passed to the open() call.
It disables the page cache for this specific file handle.</p><p>Some people on the internet claim this would be faster,
but this is 90% wrong. There are 2 main use cases where O_DIRECT
has its use:</p><ul><li>Avoiding cache pollution: You know that you will not access the pages of
a specific file again and not want the page cache to remember those
files. This is a micro optimization and is probably not worth it. More or
less the same effect can be safely achieved by fadvise() with
FADV_DONTNEED.</li><li>Implementing your own "page cache" in userspace. Many databases use this,
since they have a better idea of what pages they need to cache and which
should be re-read.</li></ul></div></div><div class="step step-level-1" step="171" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="430000" data-y="0" data-z="0"><h1 id="myth-i-o-scheduler">Myth: I/O scheduler &#x1F44E;</h1><img src="images/io_scheduler_perf.svg" width="100%"></img><p><a href="https://www.phoronix.com/review/linux-56-nvme">Full benchmark</a></p><div class="notes"><p>Re-orders read and write requests for performance.</p><ul><li><tt>none</tt>: Does no reordering.</li><li><tt>bfq</tt>: Complex, designed for desktops.</li><li><tt>mq-deadline</tt>, <tt>kyber</tt>: Simpler, good allround schedulers.</li></ul><p>In the age of SSDs we can use dumber schedulers.
In the age of HDDs schedulers were vital.</p></div></div><div class="step step-level-1" step="172" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="432500" data-y="0" data-z="0"><h1 id="myth-ionice">Myth: <tt>ionice</tt> &#x1F44E;</h1><pre class="highlight code c"><span class="cp"># Default level is 4. Lower is higher.
</span><span class="n">$</span><span class="w"> </span><span class="n">ionice</span><span class="w"> </span><span class="o">-</span><span class="n">c</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="o">-</span><span class="n">n</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="o">&lt;</span><span class="n">some</span><span class="o">-</span><span class="n">pid</span><span class="o">&gt;</span></pre><div class="notes"><p>Well, you can probably guess what it does.</p></div></div><div class="step step-level-1" step="173" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="435000" data-y="0" data-z="0"><h1 id="madvise-fadvise"><tt>madvise()</tt> &amp; <tt>fadvise()</tt></h1><img src="images/fadvise_bench.png" width="100%"></img><p class="example">Example: code/fadvise</p><p class="example">Example: code/madvise</p><div class="notes"><p>fadvise() and madvise() can be used to give the page cache hints on what
pages are going to be used next and in what order. This can make a big difference
for complex use cases like rsync or tar, where the program knows that it needs
to read a bunch of files in a certain order. In this case advises can be given
to the kernel quite a bit before the program starts reading the file.</p><p>The linked examples try to simulate this by clearing the cache, giving a advise,
waiting a bit and then reading the file in a specific order.</p><p>The examples also contain some noteable things:</p><ul><li>Reading random is much slower than reading forward.</li><li>Reading backwards is the end boss and really much, much slower.</li><li>hyperfine is a nice tool to automate little benchmarks like these.</li><li>Complex orders (like heaps or tree traversal) cannot be requested.</li><li>mmap does not suffer from the read order much and is much faster
for this kind of no-copy-needed workload.</li></ul></div></div><div class="step step-level-1" step="174" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="437500" data-y="0" data-z="0"><h1 id="why-is-cp-faster">Why is cp faster?</h1><pre class="highlight code go"><span class="kn">package</span><span class="w"> </span><span class="nx">main</span><span class="w">

</span><span class="kn">import</span><span class="p">(</span><span class="w">
    </span><span class="s">"os"</span><span class="w">
    </span><span class="s">"io"</span><span class="w">
</span><span class="p">)</span><span class="w">

</span><span class="c1">// Very simple `cp` in Go:</span><span class="w">
</span><span class="kd">func</span><span class="w"> </span><span class="nx">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nx">src</span><span class="p">,</span><span class="w"> </span><span class="nx">_</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">os</span><span class="p">.</span><span class="nx">Open</span><span class="p">(</span><span class="nx">os</span><span class="p">.</span><span class="nx">Args</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="w">
    </span><span class="nx">dst</span><span class="p">,</span><span class="w"> </span><span class="nx">_</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">os</span><span class="p">.</span><span class="nx">Create</span><span class="p">(</span><span class="nx">os</span><span class="p">.</span><span class="nx">Args</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span><span class="w">
    </span><span class="nx">io</span><span class="p">.</span><span class="nx">Copy</span><span class="p">(</span><span class="nx">dst</span><span class="p">,</span><span class="w"> </span><span class="nx">src</span><span class="p">)</span><span class="w">
</span><span class="p">}</span></pre><div class="notes"><p>cp is not faster because it copies data faster, but
because it avoids copies to user space by using specialized calls like:</p><ul><li>ioctl(5, BTRFS_IOC_CLONE or FICLONE, 4) = 0 (on btrfs)</li><li>copy_file_range() - performs in-kernel copy, sometimes even using DMA</li></ul><p>Find out using strace cp src dst.
If no trick is possible it falls back to normal buffered read/write.</p></div></div><div class="step step-level-1" step="175" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="440000" data-y="0" data-z="0"><h1 id="find-resource-hogs">Find resource hogs &#x1F437;</h1><pre class="highlight code bash"><span class="c1"># Show programs with most throughput:
</span>$<span class="w"> </span>iotop</pre><p>Finding max throughput:</p><pre class="highlight code bash"><span class="c1"># Write:
</span>$<span class="w"> </span>dd<span class="w"> </span><span class="k">if</span><span class="o">=</span>/dev/zero<span class="w"> </span><span class="nv">of</span><span class="o">=</span>./file<span class="w"> </span><span class="nv">bs</span><span class="o">=</span>32k<span class="w"> </span><span class="nv">count</span><span class="o">=</span><span class="m">10000</span><span class="w">
</span><span class="c1"># Read:
</span>$<span class="w"> </span>sync<span class="p">;</span><span class="w"> </span><span class="nb">echo</span><span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>sudo<span class="w"> </span>tee<span class="w"> </span>/proc/sys/vm/drop_caches<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="se">\
</span><span class="w">     </span>dd<span class="w"> </span><span class="k">if</span><span class="o">=</span>./file<span class="w"> </span><span class="nv">of</span><span class="o">=</span>/dev/null<span class="w"> </span><span class="nv">bs</span><span class="o">=</span>32k</pre><div class="notes"><p>NOTE: dd can be nicely used to benchmark the throughput of your disk!
Just dd from /dev/zero for write perf and to /dev/null for read perf.
But you have to use conv=fdatasync for both and clear the page cache (see below)
in case.</p></div></div><div class="step step-level-1" step="176" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="442500" data-y="0" data-z="0"><h1 id="reduce-number-of-copies">Reduce number of copies</h1><ul><li>Do not copy buffers too often (&#x1F921;)</li><li>Use <tt>readv()</tt> to splice existing buffers to one.</li><li>Use hardlinks if possible</li><li>Use CoW reflinks if possible.</li><li><tt>sendfile()</tt> to copy files to Network.</li><li><tt>copy_file_range()</tt> to copy between files.</li></ul><div class="notes"><p>Not copying: using mmap, io_uring. If using read() file API
then try to minimize copying in your application.</p></div></div><div class="step step-level-1" step="177" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="445000" data-y="0" data-z="0"><h1 id="good-abstractions">Good abstractions</h1><pre class="highlight code go"><span class="kd">type</span><span class="w"> </span><span class="nx">ReaderFrom</span><span class="w"> </span><span class="kd">interface</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nx">ReadFrom</span><span class="p">(</span><span class="nx">r</span><span class="w"> </span><span class="nx">Reader</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="nx">n</span><span class="w"> </span><span class="kt">int64</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="kt">error</span><span class="p">)</span><span class="w">
</span><span class="p">}</span><span class="w">

</span><span class="kd">type</span><span class="w"> </span><span class="nx">WriterTo</span><span class="w"> </span><span class="kd">interface</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nx">WriteTo</span><span class="p">(</span><span class="nx">w</span><span class="w"> </span><span class="nx">Writer</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="nx">n</span><span class="w"> </span><span class="kt">int64</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="kt">error</span><span class="p">)</span><span class="w">
</span><span class="p">}</span></pre><div class="notes"><p>You might have heard that abstractions are costly from a performance point
of view and this partly true. Please do not take this an excuse for not adding
any abstractions to your code in fear of performance hits.</p><p>Most bad rap of abstractions come from interfaces that are not general
enough and cannot be extended when performance needs arise.</p><p>Example: io.Reader/io.Writer/io.Seeker are very general and hardly specific.
From performance point of view they tend to introduce some extra allocations
and also some extra copying that a more specialized implementation might get
rid of if it would know how it's used.</p><p>For example, a io.Reader that has to read a compressed stream needs to read
big chunks of compressed data since compression formats work block
oriented. Even if the caller only needs a single byte, it still needs to
decompress a whole block. If the API user needs another byte a few KB away,
the reader might have to throw away the curent block and allocate space for
a new one, while seeking in the underlying stream. This is costly.</p><p>Luckily, special cases can be optimized. What if the reader knows that the whole
stream is read in one go? Like FADV_SEQUENTIAL basically. This is what WriteTo()
is for. A io.Reader can implement this function to dump its complete content to
the writer specified by w. The knowledge that no seeking is required allows
the decompression reader to make some optimizations: i.e. use one big buffer,
no need to re-allocate, parallelize reading/decompression and avoid seek calls.</p><p>So remember: Keep your abstractions general, check if there are specific
patterns on how your API is called and offer optimizations for that.</p></div></div><div class="step step-level-1" step="178" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="447500" data-y="0" data-z="0"><h1 id="i-o-performance-checklist-the-sane-part">I/O performance checklist: <em>The sane part</em></h1><ol><li>Avoid I/O. (&#x1F921;)</li><li>Reduce the number of system calls.</li><li>Use a sane buffer size with <tt>read()</tt>/<tt>write()</tt>.</li><li>Use append only writes if possible.</li><li>Read files sequential, avoid seeking.</li><li>Batch small writes, as they evict caches.</li><li>Avoid creating too many small files.</li><li>Make use of <tt>mmap()</tt> where applicable.</li><li>Reduce copying (<tt>mmap</tt>, <tt>sendfile</tt>, <tt>splice</tt>).</li><li>Compress data if you can spare the CPU cycles.</li></ol><div class="notes"><ol><li>In many cases I/O can be avoided by doing more things in memory
or avoiding duplicate work.</li><li>If you write/read a file several times, then do not open and close it every time.</li><li>Anything between 1 and 32k is mostly fine. Exact size depends
on your system and might vary a little. Benchmark to find out.</li><li>Appending to a file is a heavily optimized flow in Linux. Benefit
from this by designing your software accordingly.</li><li>Reading a file backwards is much much slower than reading it
sequentially in forward direction. This is also a heavily optimized
case. Avoid excessive seeking, even for SSDs (syscall overhead +
page cache has a harder time what you will read next)</li><li>Small writes of even a single byte will mark a complete page
from the page cache as dirty, i.e. it needs to be written.
If done for many pages this will have an impact.</li><li>Every file is stored with metadata and some overhead. Prefer to
join small files to bigger ones by application logic.</li><li>mmap() can be very useful, especially in seek-heavy applications.
It can also be used to share the same file over several processes
and it has a zero-copy overhead.</li><li>Specialized calls can help to avoid copying data to userspace and
do a lot of syscalls by shifting the work to the kernel. In general,
try to avoid copying data in your application as much as possible.</li><li>If you have really slow storage (i.e. SD-cards) but a fast CPU,
then compressing data might be an option using a fast compression
algorithm like lz4 or snappy.</li></ol></div></div><div class="step step-level-1" step="179" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="450000" data-y="0" data-z="0"><h1 id="i-o-performance-checklist-the-deseperate-part">I/O performance checklist: <em>The deseperate part</em></h1><ol><li>Use <tt>io_uring</tt>, if applicable.</li><li>Buy faster/specialized hardware (<tt>RAID 0</tt>).</li><li>Use no I/O scheduler (<tt>none</tt>).</li><li>Tweak your filesystems settings (noatime).</li><li>Use a different filesystem (<tt>tmpfs</tt>)</li><li>Slightly crazy: <tt>fadvise()</tt> for cache warmup.</li><li>Maybe crazy: use <tt>O_DIRECT</tt></li><li>Likely crazy: skip <tt>fsync()/msync()</tt>)</li><li>Do not fill up your FS/SSD fully.</li></ol><div class="notes"><ol><li>io_uring can offer huge benefits, especially when dealing
with many files and parallel processing of them. It is definitely
the most complex of the 3 APIs of read+write / mmap / io_uring
and its usage most be warranted.</li><li>Always a good option and often the cheapest one. RAID 0 can,
in theory, speed up throughput almost indefinitely, although
you'll hit limits with processing speeds quite fast.</li><li>Mostly standard now. I/O schedulers were important in the age
of HDDs. Today, it's best to skip scheduling (to avoid overhead)
by using the none scheduler.</li><li>If raw performance is needed, then you might tweak some filesystem
settings, as seen before.</li><li>Some filesystems are optimized for scaling and write workloads (XFS),
while others are more optimized for desktop workloads (ext4). Choose
wisely. The pros and cons go beyond the scope of this workshop.
If you're happy with memory, you can of course <tt>tmpfs</tt> which is
the fastest available FS - because it just does not use the disk.</li><li>fadvise() can help in workloads that include a lot of files.
The correct usage is rather tricky though.</li><li>Some databases use direct access without page cache to implement
their own buffer pools. Since they know better when to keep a page
and when to read it from disk again.</li><li>If you do not care for lost data, then do not use fsync() to ensure that
data was written.</li><li>Full SSDs (and filesystem) suffer more from write amplification and
finding more free extents becomes increasingly challenging.</li></ol></div></div><div class="step step-level-1" step="180" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="452500" data-y="0" data-z="0"><h1 id="fynn-3">Fynn!</h1><div class="line-block"><br></br></div><p class="big-text">&#x1F3C1;</p><div class="line-block"><br></br></div><p class="next-link"><strong>Next:</strong> <a href="../5_concurrent/index.html">Concurrency</a>: Make things confusing fast &#x1F9F5;</p></div><div class="step step-level-1" step="181" data-x="455000" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-y="0" data-z="0"><p class="chapter">Concurrency</p><p>Make things confusing fast &#x1F9F5;</p></div><div class="step step-level-1" step="182" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="457500" data-y="0" data-z="0"><h1 id="agenda-4">Agenda</h1><ul><li>Intro</li><li>Parallel primitives</li><li>Parallel problems</li><li>Parallel patterns</li><li>A hard problem</li></ul><div class="notes"><p>Examples in this workshop will be in Go. Reason: It's rather simple there. C
requires pthreads, which is a bit of an arcane library. Python has threads,
but they suck greatly (GIL). Other languages like Javascript are single threaded
by nature (well, there are web workers, but that's embarassing). Parallel
programming in bash would be fun, but you might not share my sense of humor.</p><p>We will not talk about GPU Programming, which is something that is also part of parallel
programming but it's seldom enough that you have to do that yourself.</p></div><img src="images/thread.jpg" width="40%"></img></div><div class="step step-level-1" step="183" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="460000" data-y="0" data-z="0"><h1 id="parallel-programming">Parallel programming</h1><p><em>&#xBB;The art of distributing work to maximize
the use of resources with minimal overhead.&#xAB;</em></p><p class="small-text">(while not shooting yourself in the knee by getting confused with the mindboggling behemoth you created)</p><div class="notes"><p>It really is an art, since there are no easy guidelines.</p><p>There are two ways to be comfortable writing parallel code:</p><ul><li>Being very experienced and having made a lot of mistakes.</li><li>Being fearless and not be aware of the possible problems.</li></ul></div></div><div class="step step-level-1" step="184" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="462500" data-y="0" data-z="0"><h1 id="there-s-always-a-xkcd">There's always a xkcd</h1><img src="images/xkcd_parallel.webp" width="100%"></img></div><div class="step step-level-1" step="185" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="465000" data-y="0" data-z="0"><h1 id="rule-of-thumb">Rule of thumb &#x1F44D;</h1><div class="line-block"><br></br></div><p class="big-text"><strong>Don't.</strong> &#xB9;</p><p class="small-text">&#xB9; Unless you really, really need the performance and you proved this by benchmarks.</p><div class="notes"><p>Just to repeat: Concurrency hurts readability, will almost inevitably cause bugs
and eat your hamster. Proceed at your own risk.</p></div></div><div class="step step-level-1" step="186" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="467500" data-y="0" data-z="0"><p class="quote">If you write the code as cleverly as possible, you are,
by definition, not smart enough to debug it.</p><div class="line-block">- <strong>Brian Kernighan</strong><br></br></div><div class="notes"><p>Especially true for parallel programming as our brain is really not build
to think this way. So our mind's horizon is never far away when doing
parallel programming.</p></div></div><div class="step step-level-1" step="187" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="470000" data-y="0" data-z="0"><h1 id="concurrent-vs-parallel">Concurrent vs Parallel</h1><p>What's the difference again?</p><div class="notes"><p>Concurrent = execution might be interrupted at an time.
Parallel = several instructions get executed at the same time.</p><p>All parallel programs are also concurrent.</p></div></div><div class="step step-level-1" step="188" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="472500" data-y="0" data-z="0"><h1 id="what-are-processes">What are processes?</h1><ul><li>Processes are a lightweight way to schedule work over all available cpu cores.</li><li>Processes get started by <tt>fork()</tt> (except PID 1)</li><li>Processes focus on memory isolation - memory can only be shared via IPC (unix sockets, pipes, shared memory, network...)</li><li>Processes have their own ID (PID)</li></ul></div><div class="step step-level-1" step="189" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="475000" data-y="0" data-z="0"><h1 id="what-are-threads">What are threads?</h1><ul><li>Threads are lightweight processes (again?)</li><li>Threads get started by <tt>pthread_create()</tt> (except first thread, which exists implicitly)</li><li>Threads share the heap of the process but have each their own stack</li><li>Threads have their own ID (TID)</li></ul><div class="notes"><p>Threads are scheduled like processes by the kernel. No real difference is made between
processes and threads in that regard.</p></div></div><div class="step step-level-1" step="190" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="477500" data-y="0" data-z="0"><h1 id="what-are-coroutines">What are coroutines?</h1><ul><li>Coroutines are lightweight threads (oh come on)</li><li>Coroutines are implemented completely in user space using a scheduler</li><li>Every detail depends on the individual programming languages' implementation</li><li>&#xBB;Goroutines&#xAB; are one example of a coroutine implementation. &#xBB;Fibers&#xAB; are another often used term.</li><li>Not a kernel concept, kernel scheduler does not care.</li></ul><div class="notes"><p>Good example of software evolution. Old concepts are never cleaned up. Just new concepts
get added that enhance (in the best case) the old concepts. I call this toilet paper development:
If it stinks, put another layer over it.</p><p>In case of Go, there is a scheduler that is started inside every program written in Go. It starts
a number of threads (see GOMAXPROCS) and schedules the set of go routines over the set of threads.</p></div></div><div class="step step-level-1" step="191" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="480000" data-y="0" data-z="0"><h1 id="cpu-perspective">CPU Perspective</h1><img src="images/time_sharing_threads.png"></img><div class="notes"><p>Note: Diagram is only for a single core.
Several cores of course can do the same.</p></div></div><div class="step step-level-1" step="192" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="482500" data-y="0" data-z="0"><h1 id="no-magic-bullets">No magic bullets &#x1F52B;</h1><img src="images/epoll_vs_othersz.png" width="120%"></img><div class="notes"><p>Benchmark of a webserver handling dummy requests:</p><ul><li>forking: Spawn a new process per request.</li><li>preforking: Use a pool of worker process.</li><li>threaded: Spawn a new thread per request.</li><li>prethreaded: Use a pool of woerk threads.</li><li>poll: Single threaded using the poll() syscall.</li><li>epoll: Single threaded using the epoll() syscall.</li></ul><p>"Concurrency" is the number of requests per sec thrown
at the server, y axis is the actual handled requests.</p><p>Just throwing multithreading on a problem makes it complex,
but does not necessarily solve it.</p><p>Source: <a href="https://unixism.net/loti/async_intro.html">https://unixism.net/loti/async_intro.html</a></p><p>More details on the individual benchmarks:</p><p><a href="https://unixism.net/2019/04/linux-applications-performance-introduction">https://unixism.net/2019/04/linux-applications-performance-introduction</a></p><p>Most of the time, when used in the right dose, multithreaded programming
can easily speed up things. That's why this part of the workshop focuses more
on the safe use of parallel programming instead of squeezing every last bit
of performance out of parallel programming. Multiple threads are basically
a single optimization and deserve their own chapter therefore.</p><p>The contents in this part of the workshop are best applied with the understanding
of the CPU and Memory chapters.</p></div></div><div class="step step-level-1" step="193" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="485000" data-y="0" data-z="0"><h1 id="preemption">Preemption</h1><img src="images/preemption.png" width="100%"></img><div class="notes"><p>Linux' scheduling is preemptive. This means that a high priority task
can be worked on by interrupting a task with lower priority.</p><p>Preemption points: The scheduler can interrupt a process at pretty much
any point in time. Normally this happens in any of those cases:</p><ul><li>Process used up their time share.</li><li>Process made a syscall. While execution happens in kernel, other
cores can work up on other tasks (especially for things like recv(),
or read() where the kernel also just waits on hardware)</li><li>When the process calls sched_yield() (or sleep())</li></ul></div></div><div class="step step-level-1" step="194" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="487500" data-y="0" data-z="0"><h1 id="synchronization">Synchronization</h1><img src="images/thread_shared_state.png" width="100%"></img><div class="notes"><p>As with humans that work on a project in paralle, parallel jobs
need to synchronize to be useful. There is a big toolbox to do so.</p><p>If you use processes you obviously need to synchronize too sometimes.
Potential ways can be to use filesystem locks or mlock() on shared memory.</p><p>Failure to synchronize leads to race conditions and other bugs that are
really not fun to find. Debuggers won't work and prints might change
timings so deadlocks or race conditions might not always occur.</p></div></div><div class="step step-level-1" step="195" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="490000" data-y="0" data-z="0"><h1 id="critical-section">Critical Section</h1><pre class="highlight code go"><span class="kd">var</span><span class="w"> </span><span class="nx">count</span><span class="w"> </span><span class="kt">int</span><span class="w">

</span><span class="kd">func</span><span class="w"> </span><span class="nx">inc</span><span class="p">()</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="k">for</span><span class="w"> </span><span class="nx">idx</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="nx">idx</span><span class="w"> </span><span class="p">&lt;</span><span class="w"> </span><span class="mi">100000</span><span class="p">;</span><span class="w"> </span><span class="nx">idx</span><span class="o">++</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="c1">// critical section start</span><span class="w">
        </span><span class="nx">count</span><span class="o">++</span><span class="w">
        </span><span class="c1">// critical section end</span><span class="w">
    </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">

</span><span class="kd">func</span><span class="w"> </span><span class="nx">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="k">go</span><span class="w"> </span><span class="nx">inc</span><span class="p">()</span><span class="w">
    </span><span class="k">go</span><span class="w"> </span><span class="nx">inc</span><span class="p">()</span><span class="w">
    </span><span class="nx">time</span><span class="p">.</span><span class="nx">Sleep</span><span class="p">(</span><span class="nx">time</span><span class="p">.</span><span class="nx">Second</span><span class="p">)</span><span class="w">
    </span><span class="nx">fmt</span><span class="p">.</span><span class="nx">Println</span><span class="p">(</span><span class="nx">count</span><span class="p">)</span><span class="w">
</span><span class="p">}</span></pre><div class="notes"><p>Keep critical sections as small as possible - for performance &amp; sanity.</p><p>Question for you: What synchronisation primitives do you know?</p><p>If you don't mention "sleep" then you're a little dishonest ;-)</p><p>Why does this not happen if we reduce the 100000 to e.g. 1000?</p></div></div><div class="step step-level-1" step="196" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="492500" data-y="0" data-z="0"><h1 id="parallel-code-smell-1">Parallel code smell #1 &#x1F443;</h1><p class="quote">Never start a goroutine/thread/process without knowing how it will stop.</p><div class="line-block">- <strong>Dave Cheney</strong><br></br></div><div class="notes"><p>The previous slide had a race condition: The program might have exited before
the go routine do anything. In general: You should think about how your goroutines
are terminated.</p><p>Why? Similar to memory leaks, the phenomen of goroutine leaks exist. Most of them
come from place where people think "Ah, I don't need to close that go routine".</p><p>More background:
<a href="https://dave.cheney.net/2016/12/22/never-start-a-goroutine-without-knowing-how-it-will-stop">https://dave.cheney.net/2016/12/22/never-start-a-goroutine-without-knowing-how-it-will-stop</a></p></div></div><div class="step step-level-1" step="197" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="495000" data-y="0" data-z="0"><h1 id="primitive-sleep">Primitive: <tt>sleep()</tt></h1><p>Just kidding. <strong>Don't!</strong></p><p class="small-text">Okay, let's be honest. Sometimes you have no other way and we all did it.</p><div class="notes"><p>In all seriousness: there are some edge cases where sleep() is the only
way to reach some sort of sync state. Especially when we have to wait
on some external process that we cannot control.</p><p>If you happen to have such an edge case, then do it in a loop:</p><p>for sync_condition_not_met() { sleep(small_amount_of_time) }</p><p>But often enough it's just a lazy way to wait until something has finished.
This turns out to be flaky and depends often on the execution speed of the machine.
Which is one reason why flaky unittests exist.</p><p>And yes, I'm very guilty of this myself.</p></div></div><div class="step step-level-1" step="198" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="497500" data-y="0" data-z="0"><h1 id="primitive-mutex">Primitive: Mutex</h1><p>A binary semaphore.</p><pre class="highlight code go"><span class="kd">var</span><span class="w"> </span><span class="nx">count</span><span class="w"> </span><span class="kt">int</span><span class="w">
</span><span class="kd">var</span><span class="w"> </span><span class="nx">mu</span><span class="w"> </span><span class="nx">sync</span><span class="p">.</span><span class="nx">Mutex</span><span class="w">

</span><span class="kd">func</span><span class="w"> </span><span class="nx">inc</span><span class="p">()</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="k">for</span><span class="w"> </span><span class="nx">idx</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="nx">idx</span><span class="w"> </span><span class="p">&lt;</span><span class="w"> </span><span class="mi">100000</span><span class="p">;</span><span class="w"> </span><span class="nx">idx</span><span class="o">++</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="nx">mu</span><span class="p">.</span><span class="nx">Lock</span><span class="p">()</span><span class="w">
        </span><span class="nx">count</span><span class="o">++</span><span class="w">
        </span><span class="nx">mu</span><span class="p">.</span><span class="nx">Unlock</span><span class="p">()</span><span class="w">
    </span><span class="p">}</span><span class="w">

    </span><span class="c1">// or better if a complete function is locked:</span><span class="w">
    </span><span class="c1">// mu.Lock()</span><span class="w">
    </span><span class="c1">// defer mu.Unlock()</span><span class="w">
</span><span class="p">}</span></pre><div class="notes"><p>Variants:</p><ul><li>recursive mutex: can be locked several times, if unlocked the same time.</li><li>rw-mutex: Allows one writer, but many readers.</li></ul></div></div><div class="step step-level-1" step="199" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="500000" data-y="0" data-z="0"><h1 id="primitive-channel">Primitive: Channel</h1><pre class="highlight code go"><span class="c1">// buffered channel with 10 items</span><span class="w">
</span><span class="nx">c1</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nb">make</span><span class="p">(</span><span class="kd">chan</span><span class="w"> </span><span class="kt">int</span><span class="p">,</span><span class="w"> </span><span class="mi">10</span><span class="p">)</span><span class="w">
</span><span class="nx">c1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="c1">// send</span><span class="w">
</span><span class="nx">fmt</span><span class="p">.</span><span class="nx">Println</span><span class="p">(</span><span class="o">&lt;-</span><span class="nx">c1</span><span class="p">)</span><span class="w"> </span><span class="c1">// recv</span><span class="w">

</span><span class="c1">// unbuffered channel:</span><span class="w">
</span><span class="nx">c2</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nb">make</span><span class="p">(</span><span class="kd">chan</span><span class="w"> </span><span class="kt">int</span><span class="p">)</span><span class="w">
</span><span class="nx">c2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="c1">// send</span><span class="w">
</span><span class="c1">// deadlock!</span></pre><div class="notes"><p>Might be called prioq, fiber or something in other languages.
Basically a slice or linked list protected with a mutex (in case of a buffered channel)
or a single data field (in case of unbuffered channel)</p><p>Channels can be buffered or unbuffered:</p><ul><li>unbuffered: reads and writes block until the other end is ready.</li><li>buffer: blocks only when channel is full.</li></ul><p>Channels can be closed, which can be used as signal to stop.
A send to a closed channel panics.
A recv from a closed channel blocks forever.</p><p>A nil channel panics when something is send.
A nil channel block forever on receiving.</p><p>We will see channels later in action.</p></div></div><div class="step step-level-1" step="200" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="502500" data-y="0" data-z="0"><h1 id="channel-rules">Channel rules</h1><pre class="highlight code go"><span class="nx">c1</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nb">make</span><span class="p">(</span><span class="kd">chan</span><span class="w"> </span><span class="kt">int</span><span class="p">)</span><span class="w"> </span><span class="c1">// unbuffered</span><span class="w">
</span><span class="nx">c2</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nb">make</span><span class="p">(</span><span class="kd">chan</span><span class="w"> </span><span class="kt">int</span><span class="p">,</span><span class="w"> </span><span class="mi">10</span><span class="p">)</span><span class="w"> </span><span class="c1">// buffered</span><span class="w">

</span><span class="c1">// A send on c1 would block until another go routine</span><span class="w">
</span><span class="c1">// will receive from it. On ch2 we can send 10 times</span><span class="w">
</span><span class="c1">// until the same happens.</span><span class="w">

</span><span class="c1">// channel are open or closed.</span><span class="w">
</span><span class="c1">// send on a closed channel panics</span><span class="w">
</span><span class="c1">// recv on a closed channel takes forever</span><span class="w">
</span><span class="nb">close</span><span class="p">(</span><span class="nx">c1</span><span class="p">)</span></pre></div><div class="step step-level-1" step="201" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="505000" data-y="0" data-z="0"><h1 id="primitive-semaphor">Primitive: Semaphor</h1><pre class="highlight code go"><span class="c1">// Init the semaphore:</span><span class="w">
</span><span class="nx">semaphore</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nb">make</span><span class="p">(</span><span class="kd">chan</span><span class="w"> </span><span class="kt">bool</span><span class="p">,</span><span class="w"> </span><span class="mi">10</span><span class="p">)</span><span class="w">
</span><span class="k">for</span><span class="w"> </span><span class="nx">i</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="nx">i</span><span class="w"> </span><span class="p">&lt;</span><span class="w"> </span><span class="nb">cap</span><span class="p">(</span><span class="nx">tokens</span><span class="p">);</span><span class="w"> </span><span class="nx">i</span><span class="o">++</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nx">semaphore</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nx">i</span><span class="w">
</span><span class="p">}</span><span class="w">

</span><span class="c1">// Limit number of jobs to 10 parallel jobs:</span><span class="w">
</span><span class="k">for</span><span class="w"> </span><span class="nx">_</span><span class="p">,</span><span class="w"> </span><span class="nx">job</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="k">range</span><span class="w"> </span><span class="nx">jobs</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="o">&lt;-</span><span class="nx">semaphore</span><span class="w">
    </span><span class="k">go</span><span class="w"> </span><span class="kd">func</span><span class="p">(</span><span class="nx">job</span><span class="w"> </span><span class="nx">Job</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="c1">// ... do work here ...</span><span class="w">
        </span><span class="nx">semaphore</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="kc">true</span><span class="w">
    </span><span class="p">}(</span><span class="nx">job</span><span class="p">)</span><span class="w">
</span><span class="p">}</span></pre><div class="notes"><p>Very easy way to limit the number of go routines.
Basically a lightweight pool - good for one-time jobs.</p><p>Metaphor: A bouncer before a club.</p><p>It's corona times and he knows that only 10 people are allowed in the club
(sad times) He counts up when he let's somebody in and counts down when
someone leaves. If the club is full new visitors have to wait. Whem somebody
leaves then a new person may enter the "critical section" (club).</p></div></div><div class="step step-level-1" step="202" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="507500" data-y="0" data-z="0"><h1 id="primitive-select">Primitive: Select</h1><pre class="highlight code go"><span class="k">select</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="k">case</span><span class="w"> </span><span class="o">&lt;-</span><span class="nx">c1</span><span class="p">:</span><span class="w">
        </span><span class="c1">// executed when c1 has</span><span class="w">
        </span><span class="c1">// incoming data.</span><span class="w">
    </span><span class="k">case</span><span class="w"> </span><span class="nx">result</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="o">&lt;-</span><span class="nx">c2</span><span class="p">:</span><span class="w">
        </span><span class="c1">// executed when c2 has</span><span class="w">
        </span><span class="c1">// incoming data.</span><span class="w">

    </span><span class="k">default</span><span class="p">:</span><span class="w">
        </span><span class="c1">// executed when nothing</span><span class="w">
        </span><span class="c1">// on both channels. If no</span><span class="w">
        </span><span class="c1">// 'default' given then</span><span class="w">
        </span><span class="c1">// select blocks.</span><span class="w">
        </span><span class="c1">// Without default, we block.</span><span class="w">
</span><span class="p">}</span></pre><div class="notes"><p>select exists to be multiplex between several channels
and to figure out if we way send or receive from a channel.</p><p>This feature does not exactly exist in most other languages.
Usually condition variables are used for this outside of Go
or something like await/asnyc in languages that have it.</p></div></div><div class="step step-level-1" step="203" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="510000" data-y="0" data-z="0"><h1 id="primitive-barrier">Primitive: Barrier</h1><pre class="highlight code go"><span class="nx">wg</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="o">&amp;</span><span class="nx">sync</span><span class="p">.</span><span class="nx">WaitGroup</span><span class="p">{}</span><span class="w">
</span><span class="k">for</span><span class="w"> </span><span class="nx">idx</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="nx">idx</span><span class="w"> </span><span class="p">&lt;</span><span class="w"> </span><span class="mi">10</span><span class="p">;</span><span class="w"> </span><span class="nx">idx</span><span class="o">++</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nx">wg</span><span class="p">.</span><span class="nx">Add</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="w">
    </span><span class="k">go</span><span class="w"> </span><span class="kd">func</span><span class="p">()</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="k">defer</span><span class="w"> </span><span class="nx">wg</span><span class="p">.</span><span class="nx">Done</span><span class="p">()</span><span class="w">
        </span><span class="nx">someJob</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">)</span><span class="w">
    </span><span class="p">}()</span><span class="w">
</span><span class="p">}</span><span class="w">

</span><span class="c1">// wait here for all jobs to finish:</span><span class="w">
</span><span class="nx">wg</span><span class="p">.</span><span class="nx">Wait</span><span class="p">()</span></pre><div class="notes"><p>A barrier is basically an inverted semaphore: Instead of counting up
until you hit a limit (which means that too many jobs at the same time),
you count down until you reach zero (which means that all jobs are done)
All threads have to arrive a certain point before any can continue.</p><p>Alternative names: Wait Groups, Latch.</p><p>Question: Would it still be correct if we move the wg.Add(1) to the go routine?
No! There's a chance that wg.Wait() would not wait yet, because no go routine
did start yet.</p></div></div><div class="step step-level-1" step="204" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="512500" data-y="0" data-z="0"><h1 id="primitive-cond-var">Primitive: Cond Var</h1><pre class="highlight code go"><span class="c1">// Init:</span><span class="w">
</span><span class="nx">m</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">sync</span><span class="p">.</span><span class="nx">Mutex</span><span class="p">{}</span><span class="w">
</span><span class="nx">c</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">sync</span><span class="p">.</span><span class="nx">NewCond</span><span class="p">(</span><span class="o">&amp;</span><span class="nx">m</span><span class="p">)</span><span class="w">
</span><span class="c1">// ...</span><span class="w">
</span><span class="c1">// Sender:</span><span class="w">
</span><span class="nx">c</span><span class="p">.</span><span class="nx">L</span><span class="p">.</span><span class="nx">Lock</span><span class="p">()</span><span class="w"> </span><span class="c1">// c.L == m</span><span class="w">
</span><span class="nx">newJobReceived</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="nx">c</span><span class="p">.</span><span class="nx">Broadcast</span><span class="p">()</span><span class="w"> </span><span class="c1">// or c.Signal() for a single go routine.</span><span class="w">
</span><span class="nx">c</span><span class="p">.</span><span class="nx">L</span><span class="p">.</span><span class="nx">Unlock</span><span class="p">()</span><span class="w">
</span><span class="c1">// ...</span><span class="w">
</span><span class="c1">// Receiver:</span><span class="w">
</span><span class="nx">c</span><span class="p">.</span><span class="nx">L</span><span class="p">.</span><span class="nx">Lock</span><span class="p">()</span><span class="w">
</span><span class="k">for</span><span class="w"> </span><span class="p">!</span><span class="nx">newJobReceived</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nx">c</span><span class="p">.</span><span class="nx">Wait</span><span class="p">()</span><span class="w">
</span><span class="p">}</span><span class="w">
</span><span class="c1">// Do something here.</span><span class="w">
</span><span class="nx">c</span><span class="p">.</span><span class="nx">L</span><span class="p">.</span><span class="nx">Unlock</span><span class="p">()</span></pre><div class="notes"><p>Probably the most brainfuck-y of the primitives.</p><ul><li>Broadcast or notify a single thread.</li><li>Seldomly used in Go, but has their use cases.</li><li>Use case: waiting on a condition without busy polling
and where the use of channels would be awkward (channels
suck if you have to wake up several go routines, as messages
are consumed)</li></ul><p>When to use:</p><p>Channels are a good replacement if you just need to wake up
a single go routine. If you need to wake up many go routines
at the same time (Broadcast()) then condition variables are
way more efficient.</p><p>Context is a pattern that can be used in a similar way
(although rather exclusively for cancellation)</p></div></div><div class="step step-level-1" step="205" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="515000" data-y="0" data-z="0"><h1 id="primitive-promises">Primitive: Promises</h1><pre class="highlight code go"><span class="kd">func</span><span class="w"> </span><span class="nx">fetchData</span><span class="p">(</span><span class="nx">url</span><span class="w"> </span><span class="kt">string</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;-</span><span class="kd">chan</span><span class="w"> </span><span class="nx">Result</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nx">ch</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nb">make</span><span class="p">(</span><span class="kd">chan</span><span class="w"> </span><span class="nx">Result</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w">
    </span><span class="k">go</span><span class="w"> </span><span class="kd">func</span><span class="p">()</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="c1">// use `url` somehow and do some potentially</span><span class="w">
        </span><span class="c1">// long running I/O work.</span><span class="w">
        </span><span class="nx">ch</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nx">Result</span><span class="p">{</span><span class="o">...</span><span class="p">}</span><span class="w">
    </span><span class="p">}()</span><span class="w">

    </span><span class="k">return</span><span class="w"> </span><span class="nx">ch</span><span class="w">
</span><span class="p">}</span><span class="w">

</span><span class="kd">func</span><span class="w"> </span><span class="nx">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nx">promise</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">longRunningTask</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span><span class="w">
    </span><span class="c1">// ...do something else...</span><span class="w">
    </span><span class="c1">// await the result:</span><span class="w">
    </span><span class="nx">fmt</span><span class="p">.</span><span class="nx">Println</span><span class="p">(</span><span class="o">&lt;-</span><span class="nx">promise</span><span class="p">)</span><span class="w">
</span><span class="p">}</span></pre><div class="notes"><p>Promises are a good way to make asynchronous code look like synchronous code.
A good example is fetching stuff via HTTP. While waiting for the response you can
potentially do something else.</p><p>You can also chain promises together. I.e. automatically do something
once the promise returns - by adding another go routine. This is called
promise chaining.</p><p>Other languages like Python/Javascript have first-class support
for async/await which kinda doing the same background. Go-routines
are however a more flexible concept and it's easy to write libraries
that emulate this behaviour (and others have done so)</p></div></div><div class="step step-level-1" step="206" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="517500" data-y="0" data-z="0"><h1 id="primitive-atomics">Primitive: Atomics</h1><pre class="highlight code go"><span class="kd">var</span><span class="w"> </span><span class="nx">n</span><span class="w"> </span><span class="nx">atomic</span><span class="p">.</span><span class="nx">Uint64</span><span class="p">{}</span><span class="w">
</span><span class="nx">n</span><span class="p">.</span><span class="nx">Store</span><span class="p">(</span><span class="nx">val</span><span class="w"> </span><span class="kt">int64</span><span class="p">)</span><span class="w">
</span><span class="nx">n</span><span class="p">.</span><span class="nx">Load</span><span class="p">()</span><span class="w"> </span><span class="p">(</span><span class="nx">curr</span><span class="w"> </span><span class="kt">int64</span><span class="p">)</span><span class="w">
</span><span class="nx">n</span><span class="p">.</span><span class="nx">Add</span><span class="p">(</span><span class="nx">delta</span><span class="w"> </span><span class="kt">int64</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="nx">new</span><span class="w"> </span><span class="kt">int64</span><span class="p">)</span><span class="w">
</span><span class="nx">n</span><span class="p">.</span><span class="nx">Swap</span><span class="p">(</span><span class="nx">val</span><span class="w"> </span><span class="kt">int64</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="nx">old</span><span class="w"> </span><span class="kt">int64</span><span class="p">)</span><span class="w">
</span><span class="nx">n</span><span class="p">.</span><span class="nx">CompareAndSwap</span><span class="p">(</span><span class="nx">old</span><span class="p">,</span><span class="w"> </span><span class="nx">new</span><span class="w"> </span><span class="kt">int64</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="nx">swapped</span><span class="w"> </span><span class="kt">bool</span><span class="p">)</span></pre><div class="notes"><p>Atomic: A thing that happens in one go. Either it fails completely and
leaves no trace or it work fully. Some operations can be executed on the
CPU atomically with guarantees of never being interrupted by another
thread, signal or ISR. Those are the above operations.</p><p>If you chain several atomic operations (e.g. Store+Load) they
are of course not atomic together!</p></div></div><div class="step step-level-1" step="207" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="520000" data-y="0" data-z="0"><h1 id="primitive-cas">Primitive: CAS</h1><pre class="highlight code go"><span class="kd">func</span><span class="w"> </span><span class="p">(</span><span class="nx">cd</span><span class="w"> </span><span class="nx">countdown</span><span class="p">)</span><span class="w"> </span><span class="nx">Stop</span><span class="p">()</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nx">cas</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">atomic</span><span class="p">.</span><span class="nx">CompareAndSwapInt32</span><span class="w">
    </span><span class="k">if</span><span class="w"> </span><span class="p">!</span><span class="nx">cas</span><span class="p">(</span><span class="o">&amp;</span><span class="nx">cd</span><span class="p">.</span><span class="nx">isStopped</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="c1">// do not stop code twice if</span><span class="w">
        </span><span class="c1">// Stop() called more than once.</span><span class="w">
        </span><span class="k">return</span><span class="w">
    </span><span class="p">}</span><span class="w">

    </span><span class="c1">// Do actual stopping here.</span><span class="w">
</span><span class="p">}</span></pre><div class="notes"><p>The most exotic looking is CompareAndSwap and surprisingly it's the one that
is the most important one. It is roughly comparable to this code:</p><pre class="highlight code go"><span class="k">if</span><span class="w"> </span><span class="o">*</span><span class="nx">n</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="nx">old</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="o">*</span><span class="nx">n</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">new</span><span class="w">
    </span><span class="k">return</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="p">}</span><span class="w">

</span><span class="k">return</span><span class="w"> </span><span class="kc">false</span></pre><p>It's main use is implementing lockfree datastructures that notice
when a value was modified behind their back.</p><p>Additional use: Making sure that we don't stop twice.
(actual code example in the firmare's ui)</p></div></div><div class="step step-level-1" step="208" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="522500" data-y="0" data-z="0"><h1 id="primitive-lockfree-data-structures">Primitive: Lockfree data structures</h1><pre class="highlight code go"><span class="kd">func</span><span class="w"> </span><span class="p">(</span><span class="nx">q</span><span class="w"> </span><span class="o">*</span><span class="nx">Queue</span><span class="p">)</span><span class="w"> </span><span class="nx">Pop</span><span class="p">()</span><span class="w"> </span><span class="o">*</span><span class="nx">Elem</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="k">for</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="nx">p</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">q</span><span class="p">.</span><span class="nx">head</span><span class="w">
        </span><span class="k">if</span><span class="w"> </span><span class="nx">p</span><span class="p">.</span><span class="nx">next</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span><span class="w">
            </span><span class="k">return</span><span class="w"> </span><span class="kc">nil</span><span class="w">
        </span><span class="p">}</span><span class="w">

        </span><span class="c1">// Is `p` still the value that</span><span class="w">
        </span><span class="c1">// we expect it to be?</span><span class="w">
        </span><span class="k">if</span><span class="w"> </span><span class="nx">cas</span><span class="p">(</span><span class="nx">q</span><span class="p">.</span><span class="nx">head</span><span class="p">,</span><span class="w"> </span><span class="nx">p</span><span class="p">,</span><span class="w"> </span><span class="nx">p</span><span class="p">.</span><span class="nx">next</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
            </span><span class="c1">// value was swapped!</span><span class="w">
            </span><span class="k">return</span><span class="w"> </span><span class="nx">p</span><span class="p">.</span><span class="nx">next</span><span class="p">.</span><span class="nx">elem</span><span class="w">
        </span><span class="p">}</span><span class="w">
    </span><span class="p">}</span><span class="w">
</span><span class="p">}</span></pre><div class="notes"><p>Let's recall what a Pop() actually does:</p><ol><li>Fetch the head of the queue so we can return it.</li><li>Make the node after the old head the new head.</li></ol><p>(this assumes that the queue is based on a linked list)</p><p>Those are two operations and they are not atomic together.
If two threads call Pop() at the same time, we might have the issue
that one thread overwrites the results of the other.</p><p>In a traditional implementation we could use a mutex to protect this.
Newer CPUs (i.e. &gt;Year 2000) have CAS instructions, so we can implement
it without locks at all.</p></div></div><div class="step step-level-1" step="209" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="525000" data-y="0" data-z="0"><h1 id="contention-starvation">Contention &amp; Starvation</h1><ul><li><em>Contention:</em> Threads competing for a common resource. Causes non-zero waiting.
Can often not be avoided, just minimized.</li></ul><div class="line-block"><br></br></div><ul><li><em>Starvation:</em> Special case of <em>Contention</em> where one &#xBB;victim&#xAB; thread is blocked
from resource access more often that the other &#xBB;greedy&#xAB; threads. Pathological
behavior that should be fixed.</li></ul><div class="notes"><p>OS-level contention: done by the process/thread scheduler.
Threads are waiting on sleep, syscalls or waiting on a mutex.</p><p>Program level contention: waiting on locks, busy polling, atomics.</p><p>Lock-free data structures are so popular because they allow
a little cheat here: No syscalls involved, so they do not get
scheduled away. Mutex locks involve a call to futex() in some
cases, which is a syscall.</p><p>How to find out which threads content others or which threads get starved?
You gonna need to find out with tracing tools!</p><p>Contention: 100 threads that operate on a database that allows at most 10 parallel connections.
90 threads have to wait while 10 do work. Minimizing means to give the 90 threads some meaningful
work while they wait.</p><p>Real world example for starvation: Telephone Hotline where some people call
the hotline all the time automatically, while some normal people don't come through anymore.
With the example above: Maybe some threads use an outdated db library that makes it more
unlikely to get a connection from the connection pool.</p></div></div><div class="step step-level-1" step="210" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="527500" data-y="0" data-z="0"><h1 id="patterns">Patterns</h1><p>Several primitives combined build a pattern.</p></div><div class="step step-level-1" step="211" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="530000" data-y="0" data-z="0"><h1 id="pattern-pool">Pattern: Pool</h1><p>Classical producer-consumer problem.</p><ol><li>Start a limited number of goroutines.</li><li>Pass each a shared channel.</li><li>Let each goroutine receive on the channel.</li><li>Producer sends jobs over the channel.</li><li>Tasks are distributed over the go routines.</li></ol><div class="notes"><p>Pools often use a queue (i.e. a channel or some other prioq). I.e. you can
produce more to some point than you consume. Can be a problem.</p></div><p class="example">Example: code/producer_consumer</p></div><div class="step step-level-1" step="212" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="532500" data-y="0" data-z="0"><h1 id="tracing">Tracing</h1><img src="images/tracer_goroutines.png" width="100%"></img><p class="example">Example: code/producer_consumer</p><div class="notes"><p>Features of a tracer:</p><ul><li>View trace: Detailed overview of which core ran which goroutine at what time
and what blocked the process (like waiting for syscalls).</li><li>Goroutine analysis: Show stats per goroutine - see screenshot above.
This is a good overview how "parallel" the go routine actually is.
Does it do actual work or does it wait to be scheduled or locks?</li><li>Syscall blocking profile: Overview of contention through syscalls.
Check this if you suspect that your program is spending time waiting
for input/output.</li><li>Scheduler latency profiler: Scheduling goroutines comes with an overhead.
This overhead is noticeable and the higher it gets the less time there is
for actually useful stuff. Sometimes go routines just play "yield ping pong".</li></ul><p>Usage:</p><ul><li>Import "runtime/trace"</li><li>Open a file descriptor to where you'd like your trace output.</li><li>Do trace.Start(fd)/trace.Stop() around the desired code portion.</li><li>Run your program so that it produces a trace output in the file you specified.</li><li>Run go tool trace &lt;path&gt; to start the web ui.</li></ul><p>A bit more background: <a href="https://blog.gopheracademy.com/advent-2017/go-execution-tracer">https://blog.gopheracademy.com/advent-2017/go-execution-tracer</a></p></div></div><div class="step step-level-1" step="213" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="535000" data-y="0" data-z="0"><h1 id="pattern-pipeline">Pattern: Pipeline</h1><p>Several pools connected over channels.</p><pre class="highlight code go"><span class="c1">// DO NOT:</span><span class="w">
</span><span class="kd">func</span><span class="w"> </span><span class="nx">work</span><span class="p">()</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nx">report</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">generateReport</span><span class="p">()</span><span class="w">
    </span><span class="nx">encoded</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">report</span><span class="p">.</span><span class="nx">Marshal</span><span class="p">()</span><span class="w">
    </span><span class="nx">compressed</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">compress</span><span class="p">(</span><span class="nx">encoded</span><span class="p">)</span><span class="w">
    </span><span class="nx">sendToNSA</span><span class="p">(</span><span class="nx">compressed</span><span class="p">)</span><span class="w">
</span><span class="p">}</span></pre><div class="notes"><p>Talk about the naive implementation where time of finish will
be influenced by a single long running job.</p></div></div><div class="step step-level-1" step="214" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="537500" data-y="0" data-z="0"><pre class="highlight code go"><span class="c1">// Instead:</span><span class="w">
</span><span class="nx">c1</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nb">make</span><span class="p">(</span><span class="kd">chan</span><span class="w"> </span><span class="nx">type1</span><span class="p">)</span><span class="w">
</span><span class="nx">c2</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nb">make</span><span class="p">(</span><span class="kd">chan</span><span class="w"> </span><span class="nx">type2</span><span class="p">)</span><span class="w">
</span><span class="nx">c3</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nb">make</span><span class="p">(</span><span class="kd">chan</span><span class="w"> </span><span class="nx">type3</span><span class="p">)</span><span class="w">

</span><span class="k">go</span><span class="w"> </span><span class="kd">func</span><span class="p">()</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="k">for</span><span class="w"> </span><span class="nx">job</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="k">range</span><span class="w"> </span><span class="nx">c1</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="nx">c2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nx">generateReport</span><span class="p">()</span><span class="w">
    </span><span class="p">}</span><span class="w">
</span><span class="p">}()</span><span class="w">
</span><span class="k">go</span><span class="w"> </span><span class="kd">func</span><span class="p">()</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="k">for</span><span class="w"> </span><span class="nx">report</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="k">range</span><span class="w"> </span><span class="nx">c2</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="nx">c3</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nx">report</span><span class="p">.</span><span class="nx">Marshal</span><span class="p">()</span><span class="w">
    </span><span class="p">}</span><span class="w">
</span><span class="p">}()</span><span class="w">

</span><span class="c1">// ...</span></pre><div class="notes"><p>This can also be easily combined with the pool pattern to start several go routines per pipeline step,
allowing us to easily balance out steps that take longer than others. Not shown here, take this as homework.</p></div></div><div class="step step-level-1" step="215" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="540000" data-y="0" data-z="0"><h1 id="pattern-parallel-iterator">Pattern: Parallel Iterator</h1><pre class="highlight code go"><span class="kd">func</span><span class="w"> </span><span class="nx">iter</span><span class="p">()</span><span class="w"> </span><span class="kd">chan</span><span class="w"> </span><span class="nx">Elem</span><span class="w"> </span><span class="p">{</span><span class="w">
     </span><span class="nx">ch</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nb">make</span><span class="p">(</span><span class="kd">chan</span><span class="w"> </span><span class="nx">Elem</span><span class="p">,</span><span class="w"> </span><span class="mi">10</span><span class="p">)</span><span class="w">
     </span><span class="k">go</span><span class="w"> </span><span class="kd">func</span><span class="p">()</span><span class="w"> </span><span class="p">{</span><span class="w">
         </span><span class="nx">a</span><span class="p">,</span><span class="w"> </span><span class="nx">b</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="w">
         </span><span class="k">for</span><span class="w"> </span><span class="p">{</span><span class="w">
             </span><span class="nx">ch</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nx">a</span><span class="w">
             </span><span class="nx">a</span><span class="p">,</span><span class="w"> </span><span class="nx">b</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">b</span><span class="p">,</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nx">b</span><span class="w">
         </span><span class="p">}</span><span class="w">
     </span><span class="p">}()</span><span class="w">
     </span><span class="k">return</span><span class="w"> </span><span class="nx">ch</span><span class="w">
</span><span class="p">}</span><span class="w">
</span><span class="k">for</span><span class="w"> </span><span class="nx">elem</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="k">range</span><span class="w"> </span><span class="nx">iter</span><span class="p">()</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="o">...</span><span class="w"> </span><span class="p">}</span></pre><div class="notes"><p>Problem: How to stop? Best to use context.Contex</p><p>Note: You should probably buffer a little here.</p></div></div><div class="step step-level-1" step="216" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="542500" data-y="0" data-z="0"><h1 id="problems">Problems</h1><p>What kind of problems do we need to solve with primitives and patterns?</p></div><div class="step step-level-1" step="217" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="545000" data-y="0" data-z="0"><h1 id="problem-race-conditions">Problem: Race conditions</h1><pre class="highlight code go"><span class="kd">var</span><span class="w"> </span><span class="nx">counter</span><span class="w"> </span><span class="kt">int</span><span class="w">
</span><span class="kd">func</span><span class="w"> </span><span class="nx">f</span><span class="p">()</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="k">for</span><span class="p">(</span><span class="nx">idx</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="nx">idx</span><span class="w"> </span><span class="p">&lt;</span><span class="w"> </span><span class="mi">10000</span><span class="p">;</span><span class="w"> </span><span class="nx">idx</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="nx">counter</span><span class="o">++</span><span class="w">
    </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">
</span><span class="c1">// ...</span><span class="w">
</span><span class="k">go</span><span class="w"> </span><span class="nx">f</span><span class="p">()</span><span class="w">
</span><span class="k">go</span><span class="w"> </span><span class="nx">f</span><span class="p">()</span></pre></div><div class="step step-level-1" step="218" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="547500" data-y="0" data-z="0"><h1 id="solution-race-conditions">Solution: Race conditions</h1><ul><li>Use synchronisation primitives.</li><li>Avoid shared state (no globals e.g.)</li><li>Prefer copies over references.</li><li>Use a race detector. (<tt>helgrind</tt>, <tt>go test -race</tt>)</li><li>Write tests that are multithreaded.</li><li>Use Rust. &#x1F61B;</li></ul><div class="notes"><p>Shared state / Copy over reference:</p><p>Channels copy data on send. Copies do have issues when being accessed by
several threads. There is a small overhead of course, but it is much
smaller than false sharing. This also means though: Do not send pointers
over channels, as the pointer value itself is copied but of course not the
value it points to.</p><p>Less scope is better. If a variable is only visible to a single thread
or goroutine, then it cannot have issues. Avoid global state anyways.</p><p>Proper synchronisation:</p><p>At some point you need to resort to sync primitives of course.
If you need to use too much of it, chances are you have an issue
in your architecture though.</p><p>Race detector / tests / rust:</p><p>Parallel code is complicated. Use whatever tools are available to
ensure correctness.</p></div></div><div class="step step-level-1" step="219" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="550000" data-y="0" data-z="0"><h1 id="tool-race-detector">Tool: Race detector</h1><pre class="highlight code bash"><span class="c1"># or for non-tests: go run -race main.go
</span>$<span class="w"> </span>go<span class="w"> </span><span class="nb">test</span><span class="w"> </span>-race<span class="w"> </span>./package<span class="w">
</span>WARNING:<span class="w"> </span>DATA<span class="w"> </span>RACE<span class="w">
</span>Read<span class="w"> </span>by<span class="w"> </span>goroutine<span class="w"> </span><span class="m">185</span>:<span class="w">
  </span>net.<span class="o">(</span>*pollServer<span class="o">)</span>.AddFD<span class="o">()</span><span class="w">
      </span>src/net/fd_unix.go:89<span class="w"> </span>+0x398<span class="w">
  </span>...<span class="w">

</span>Previous<span class="w"> </span>write<span class="w"> </span>by<span class="w"> </span>goroutine<span class="w"> </span><span class="m">184</span>:<span class="w">
  </span>net.setWriteDeadline<span class="o">()</span><span class="w">
      </span>src/net/sockopt_posix.go:135<span class="w"> </span>+0xdf<span class="w">
  </span>...</pre><div class="notes"><p>Herr Rittler likes this.</p><p>More info: <a href="https://go.dev/doc/articles/race_detector">https://go.dev/doc/articles/race_detector</a></p><p>Disadvantages:</p><ul><li>Slows down program a little. Sometimes races do not happen anymore
if -race is enabled.</li><li>It only sees race conditions that actually happen. If there's no test
for it, then you won't see a print.</li></ul><p>Still: You should probably enable it in your tests.</p></div></div><div class="step step-level-1" step="220" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="552500" data-y="0" data-z="0"><h1 id="problem-deadlock-1">Problem: Deadlock #1</h1><pre class="highlight code go"><span class="nx">ch</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nb">make</span><span class="p">(</span><span class="kd">chan</span><span class="w"> </span><span class="kt">int</span><span class="p">)</span><span class="w">

</span><span class="c1">// thread1:</span><span class="w">
</span><span class="nx">ch</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="mi">42</span><span class="w">

</span><span class="c1">// thread2:</span><span class="w">
</span><span class="k">if</span><span class="w"> </span><span class="nx">someCondition</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nx">result</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="o">&lt;-</span><span class="nx">ch</span><span class="w">
</span><span class="p">}</span></pre><div class="notes"><p>Unbuffered channels are prone to deadlocks.
In this example we will have a deadlock in thread1
if thread2 does not go into the if block.</p><p>For this we probably should have used a buffered channel.</p></div></div><div class="step step-level-1" step="221" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="555000" data-y="0" data-z="0"><h1 id="problem-deadlock-2">Problem: Deadlock #2</h1><pre class="highlight code go"><span class="kd">func</span><span class="w"> </span><span class="nx">foo</span><span class="p">()</span><span class="w"> </span><span class="kt">error</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nx">mu</span><span class="p">.</span><span class="nx">Lock</span><span class="p">()</span><span class="w">
    </span><span class="k">if</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">bar</span><span class="p">();</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="k">return</span><span class="w"> </span><span class="nx">err</span><span class="w">
    </span><span class="p">}</span><span class="w">

    </span><span class="nx">mu</span><span class="p">.</span><span class="nx">Unlock</span><span class="p">()</span><span class="w">
    </span><span class="k">return</span><span class="w"> </span><span class="kc">nil</span><span class="w">
</span><span class="p">}</span></pre><div class="notes"><p>Absolute classic. Forgetting to unlock in one error case.</p><p>Luckily, in Go we have the defer statement, so we can unlock
the mutex in all cases.</p></div></div><div class="step step-level-1" step="222" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="557500" data-y="0" data-z="0"><h1 id="problem-deadlock-3">Problem: Deadlock #3</h1><pre class="highlight code go"><span class="kd">func</span><span class="w"> </span><span class="nx">foo</span><span class="p">()</span><span class="w"> </span><span class="kt">error</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nx">mu1</span><span class="p">.</span><span class="nx">Lock</span><span class="p">()</span><span class="w">
    </span><span class="nx">mu2</span><span class="p">.</span><span class="nx">Lock</span><span class="p">()</span><span class="w">
    </span><span class="c1">// ...</span><span class="w">
    </span><span class="k">defer</span><span class="w"> </span><span class="nx">mu1</span><span class="p">.</span><span class="nx">Lock</span><span class="p">()</span><span class="w">
    </span><span class="k">defer</span><span class="w"> </span><span class="nx">mu2</span><span class="p">.</span><span class="nx">Lock</span><span class="p">()</span><span class="w">
</span><span class="p">}</span><span class="w">
</span><span class="kd">func</span><span class="w"> </span><span class="nx">bar</span><span class="p">()</span><span class="w"> </span><span class="kt">error</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nx">mu2</span><span class="p">.</span><span class="nx">Lock</span><span class="p">()</span><span class="w">
    </span><span class="nx">mu1</span><span class="p">.</span><span class="nx">Lock</span><span class="p">()</span><span class="w">
    </span><span class="c1">// ...</span><span class="w">
    </span><span class="k">defer</span><span class="w"> </span><span class="nx">mu2</span><span class="p">.</span><span class="nx">Lock</span><span class="p">()</span><span class="w">
    </span><span class="k">defer</span><span class="w"> </span><span class="nx">mu1</span><span class="p">.</span><span class="nx">Lock</span><span class="p">()</span><span class="w">
</span><span class="p">}</span></pre><div class="notes"><p>The lock hierarchy needs to be preserved. Otherwise
deadlocks might happen.</p></div></div><div class="step step-level-1" step="223" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="560000" data-y="0" data-z="0"><h1 id="solution-deadlocks">Solution: Deadlocks</h1><ul><li>Obtain a stacktrace if they happen. (<em>Ctrl-Backslash</em>)</li><li>Debugger (if deadlock is not timing sensitive)</li><li>Keep critical sections small.</li><li>Use defer for the <tt>Unlock</tt>.</li><li>Respect the lock hierarchy.</li><li>Double think if an unbuffered channel will work out.</li><li>Use unidirectional channels and <tt>select</tt> in Go.</li><li>Don't be <em>clever</em>.</li></ul><div class="notes"><p>Tip: In Go progamms you can press Ctrl+or send SIGABRT or SIGTERM
to the program to make it print a stack trace.
Or use a debugger.</p></div><p class="example">Example: code/deadlock</p></div><div class="step step-level-1" step="224" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="562500" data-y="0" data-z="0"><h1 id="problem-livelock">Problem: Livelock</h1><p>Example:</p><ul><li>Two persons walking in opposite directions,
trying to pass each other in a tight corridor.</li><li>When both persons move at the same time left and right
then hallway is still blocked.</li><li>If infinitely done, then it's a livelock.</li></ul><div class="notes"><p>A system that does not make any progress for prolonged times.
Relatively seldom, but can happen.</p><p>Usual cause: Too primitive retry mechanism.</p></div></div><div class="step step-level-1" step="225" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="565000" data-y="0" data-z="0"><h1 id="solution-livelock">Solution: Livelock</h1><ul><li>Avoid circular dependencies.</li><li>Use an arbitrator.</li><li>Use exponential backoff.</li></ul><div class="notes"><ul><li>Arbitrator: In the metaphor above somebody that has an overview of the situation and tells one person to move.</li><li>Exponential backoff: Proper retry mechanism with random jitter between retries.</li></ul><p>Real life example: Two processes trying to execute an SQL transaction that depend on each other.
SQL server will stop the transaction and make them retry - if the retry mechanism is the same, then
it might take a long time to resolve the situation.</p></div></div><div class="step step-level-1" step="226" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="567500" data-y="0" data-z="0"><h1 id="problem-cancellation">Problem: Cancellation</h1><pre class="highlight code go"><span class="nx">resultCh</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nb">make</span><span class="p">(</span><span class="kd">chan</span><span class="w"> </span><span class="kt">int</span><span class="p">)</span><span class="w">
</span><span class="k">go</span><span class="w"> </span><span class="nx">longRunningJob</span><span class="p">(</span><span class="nx">resultCh</span><span class="p">)</span><span class="w">

</span><span class="c1">// Give job 5 seconds to complete:</span><span class="w">
</span><span class="k">select</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="k">case</span><span class="w"> </span><span class="nx">result</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="o">&lt;-</span><span class="nx">resultCh</span><span class="p">:</span><span class="w">
        </span><span class="c1">// do something with `result`</span><span class="w">
    </span><span class="k">case</span><span class="w"> </span><span class="o">&lt;-</span><span class="nx">time</span><span class="p">.</span><span class="nx">After</span><span class="p">(</span><span class="mi">5</span><span class="o">*</span><span class="nx">time</span><span class="p">.</span><span class="nx">Second</span><span class="p">):</span><span class="w">
        </span><span class="nx">log</span><span class="p">.</span><span class="nx">Warnf</span><span class="p">(</span><span class="s">"Oh no! No result yet."</span><span class="p">)</span><span class="w">
        </span><span class="c1">// BUT: longRunningJob still running!</span><span class="w">
</span><span class="p">}</span></pre></div><div class="step step-level-1" step="227" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="570000" data-y="0" data-z="0"><h1 id="solution-context">Solution: Context</h1><pre class="highlight code go"><span class="c1">// Init:</span><span class="w">
</span><span class="nx">parentCtx</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">context</span><span class="p">.</span><span class="nx">Background</span><span class="p">()</span><span class="w">
</span><span class="nx">timeout</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="mi">5</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nx">time</span><span class="p">.</span><span class="nx">Second</span><span class="w">
</span><span class="nx">ctx</span><span class="p">,</span><span class="w"> </span><span class="nx">cancel</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">context</span><span class="p">.</span><span class="nx">WithTimeout</span><span class="p">(</span><span class="nx">parentCtx</span><span class="p">,</span><span class="w"> </span><span class="nx">timeout</span><span class="p">)</span><span class="w">

</span><span class="c1">// Check for cancellation:</span><span class="w">
</span><span class="k">select</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="k">case</span><span class="w"> </span><span class="o">&lt;-</span><span class="nx">ctx</span><span class="p">.</span><span class="nx">Done</span><span class="p">():</span><span class="w">
        </span><span class="k">return</span><span class="w"> </span><span class="nx">ctx</span><span class="p">.</span><span class="nx">Err</span><span class="p">()</span><span class="w">
    </span><span class="k">default</span><span class="p">:</span><span class="w">
        </span><span class="c1">// if not cancelled</span><span class="w">
        </span><span class="c1">// we land here.</span><span class="w">
</span><span class="p">}</span><span class="w">

</span><span class="c1">// Cancellation:</span><span class="w">
</span><span class="nx">cancel</span><span class="p">()</span></pre><div class="notes"><p>Especially useful for HTTP request handlers.
In Go, each of them has a context that is cancelled
when the request is not needed anymore.</p></div></div><div class="step step-level-1" step="228" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="572500" data-y="0" data-z="0"><h1 id="context-tree">Context Tree</h1><img src="images/context.avif" width="80%"></img></div><div class="step step-level-1" step="229" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="575000" data-y="0" data-z="0"><h1 id="takeaways">Takeaways</h1><ul><li>Benchmarks before committing your atrocities.</li><li>Always make sure to use proper synchronization.</li><li>Don't use more go routines than you need.</li><li>Avoid false sharing.</li><li>Avoid contention &amp; starvation.</li><li>Write tests that use several go routines (-race).</li><li>Don't be clever.</li></ul></div><div class="step step-level-1" step="230" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="577500" data-y="0" data-z="0"><p>You almost made it! Just one slide left!</p><p class="small-text">And it's a very easy one and won't take a lot of time at all!</p></div><div class="step step-level-1" step="231" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="580000" data-y="0" data-z="0"><h1 id="brainfuck-time">Brainfuck time &#x1F9E0;</h1><img src="images/philosophers.png" width="50%"></img><div class="line-block"><br></br></div><ul><li>Philosophers toggle between &#xBB;thinking&#xAB; and &#xBB;eating&#xAB;.</li><li>The &#xBB;eating&#xAB; phase has a fixed length.</li><li>The &#xBB;thinking&#xAB; phase has a random length.</li><li>During &#xBB;eating&#xAB; he requires two forks.</li><li>If only one fork is available, they wait until a second one is available.</li></ul><p><strong>Goal:</strong> No philosopher should starve.</p><div class="notes"><p>Bonus: If you can name all philosophers pictured above.</p><p>Two problems that can occur:</p><ul><li>Deadlock: Every philosopher took the left fork. None can pick the right fork.</li><li>Starvation: A single philspopher might be unlucky and never get two forks.</li></ul><p>Solution:</p><ul><li>Simple: Use a single mutex as "waiter" to stop concurrency.</li><li>Hard &amp; correct: Use global mutex pluse "hungry" state with semaphor per philosopher.</li><li>Easier: Give philosophers invdividual rights and priorities.</li><li>Weird: philosopher talk to each other if they need a fork (i.e. channels)</li></ul></div></div><div class="step step-level-1" step="232" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="582500" data-y="0" data-z="0"><h1 id="fynn-4">Fynn!</h1><div class="line-block"><br></br></div><p class="big-text">&#x1F3C1;</p><div class="line-block"><br></br></div><p class="next-link"><strong>Next:</strong> <a href="https://github.com/sahib/misc/tree/master/performance">Bookmark the GitHub repo!</a></p></div></div><div id="slide-number" class="slide-number">
         1
      </div><script type="text/javascript" src="js/impress.js"></script><script type="text/javascript" src="js/gotoSlide.js"></script><script type="text/javascript" src="js/hovercraft.js"></script><script type="text/javascript" src="hovercraft.js"></script><script type="text/javascript">
      document.getElementById("impress").addEventListener("impress:stepenter", update_slide_number, false);
    </script></body></html>