<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml"><head><title>Performance</title><meta charset="UTF-8"></meta><meta name="generator" content="Hovercraft! 1.0 http://regebro.github.com/hovercraft"></meta><link rel="stylesheet" href="css/hovercraft.css" media="all"></link><link rel="stylesheet" href="css/highlight.css" media="all"></link><link rel="stylesheet" href="hovercraft.css" media="screen,projection"></link><script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        showProcessingMessages: false,
        messageStyle: "none",
        TeX : { extensions : ['color.js'] }
      });
    </script></head><body class="impress-not-supported"><div id="impress-help"></div><div id="impress" data-transition-duration="1500"><div class="step step-level-1" step="0" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="0" data-y="0" data-z="0"><p>General idea:</p><ul><li>Teaching you about the internals of program execution.</li><li>Very neglected field - not much teached in studies, during day-to-day work "it worked (once)" is more important.</li><li>Maybe you heard of Moore's law?</li><li>Lemur's law: Software engineers get twice as incompentent every decade (only half ironic)</li><li>In the 90s we still squeezed every byte of memory out of game consoles</li><li>And last decade we invented things like Electron - I don't want you guys to invent something like Electron</li><li>If you think Electron is a good idea, then please stop doing anything related to software engineering</li><li>Maybe have a try with gardening, or do waterboarding in Guantanamo. Just do something less hurtful to mankind than Electron</li></ul><p>Disclaimer:</p><ul><li>We're working from low level to slightly higher level here. Don't expect tips like "use this data structure to make
stuff incredibly fast". I'll won't go over all possible performance tips for your language (there are better
lists on the internet). I also won't go over a lot of data structures - what I do show is to show you how to choose
a data structure.</li><li>The talk is loosely tied to the hardware: General intro, cpu, mem, io, parallel programming</li><li>Most code examples will be in Go and C, as most ideads require a compiled language.</li><li>Interpreted languages like Python/Typescript might take away a few concepts, but to be honest,
your language is fucked up and will never achieve solid performance.</li><li>For Python you can at least put performance criticals into C libraries, for the blistering cestpool
that web technology is... well, I guess your only hope is Webassembly.</li><li>In this talk you will learn why people invent things Webassembly - even though it's kinda sad.</li></ul><p>Intro to Complexity Notation</p><p>When to optimize ("does not matter how fast you return wrong results")</p><p><a href="https://go.dev/doc/diagnostics">https://go.dev/doc/diagnostics</a></p><p>Differences: Profiling - Tracing - Debugging - Statistics</p><p>What are hot loops?</p><p>Data structures:</p><p>-&gt; Some have better space / time complexity.
-&gt; Most have tradeoffs, only few are universally useful like arrays / hash tables
-&gt; Some are probalibisitic: i.e. they save you work or space at the expense of accuracy (bloom filters)</p><h1 id="expectation-management">Expectation management</h1><pre class="highlight code">* This is more of a table of contents, than in-depth knowledge.
* Therefore touching a lot of topics. Mostly Go, but also many general techniques.
* TODO: L&#xFC;ckenf&#xFC;ller zwischen How-to-TDD und How-to-unittests.
* You accept software is never perfect and never will be.
* You accept you have to accept and manage mistakes.
* You don't except this workshop to be complete in any way.
* You don't expect that we implement all of the things mentioned.
* You know not to belive everything the internet or Lemurs says.</pre><div class="notes"><p>My expectations:</p><ul><li>You ask immediately when you did not understand something.</li><li>You will have some exercises in between.</li><li>You will not need to understand everything.
In the worst case you get better in buzzword bingo.</li></ul></div></div><div class="step step-level-1" step="1" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="1600" data-y="0" data-z="0"><p>What is complexity?
How does it relate to performance?
Is it cyclomatic complexity? Algorithmic complexity? Design complexity?</p></div><div class="step step-level-1" step="2" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="3200" data-y="0" data-z="0"><p>Questions to ask:</p><ul><li>How fast does this thing need to be?</li><li>Are optimizations worth the risk?</li></ul><p>What people ask:</p><ul><li>How fast could this be?</li></ul><p>or:</p><ul><li>It works, no need for optimization!</li></ul><p>Rule of thumb:</p><p>Do the obvious things right away.
Check if your requirements are met, if not identity the biggest bottleneck(s) (don't optimize right away)
Then tackle this by going from the big to the small.</p></div><div class="step step-level-1" step="3" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="4800" data-y="0" data-z="0"><p>Not included:</p><ul><li>Network performance</li><li>Database performance</li><li>Any of the thousands of specific fields</li><li>Long intro to data structures - pick a book.</li></ul><p>That would be several semesters of material.
The material in this workshop is just one ;)</p></div><div class="step step-level-1" step="4" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="6400" data-y="0" data-z="0"><p>TOC:</p><ul><li>4 Erkenntn&#xFC;sse:</li><li>Jeder der sich daf&#xFC;r interessiert schnelle Programme zu schreiben.</li><li>Rare knowledge, less common knowledge.</li></ul><p>Intro:</p><blockquote><ul><li>Warum Optimierung?</li><li>Contra: Donald Knuth: Premature Optimization is the root of all evil.</li><li>Pro: Moore's Law but software got bad faster then hardware got faster.</li><li>Berufsstolz: Man sollte schon schauen dass die Software m&#xF6;glichst wenig
Resources nutzt. Frontendentwicklung heutzutage ist leider ein gutes Beispiel.</li><li>Electron etc. sind so fett dass es echt peinlich ist.</li></ul></blockquote></div><div class="step step-level-1 chapter-class" step="5" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="8000" data-y="0" data-z="0"><h1 id="cpu">CPU</h1></div><div class="step step-level-1" step="6" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="9600" data-y="0" data-z="0"><ul><li>CPU<blockquote><ul><li>Assembly vs Machine Code<blockquote><p>Assembly: 1:1 human readable interpretation of machine code.
Machine code: machine readable instructions (each instruction has an id)
Assembler: Program that converts assembly to machine code.</p></blockquote></li><li>This slides could be also a talk about "Why interpreted languages suck"<blockquote><p>Most optimizations will not work with python.
As a language it's really disconnected from the HW - every single statement will cause 100s or 1000s of assembly instructions.
Also there are no almost no guarantees how big e.g. arrays or other data structures will be and how they are layout in memory.
You have to rely on your interpreter (and I count Java's JIT as one!) to be fast on modern hardware - most are not and that's why
there's so much C libraries in python, making the whole packaging system a bloody mess.</p></blockquote></li><li>Go assembly<blockquote><p>Assembly</p></blockquote></li><li>von Neumann Architektur (RAM being the bottleneck)</li><li>Many workarounds to fix this: (L1, L2, L3)</li><li>Instruction Sets (RISC/CISC, x86, arm)</li><li>Microarchitecture (Implementation of a certain ISR - Coffee Lake and so on)</li><li>Branch prediction (Heartbleed)</li><li>if(unlikely(x &gt; 100)) { error() }</li><li>Data oriented programming<blockquote><ul><li>employee example</li><li>memcpy</li><li>matrix traversal</li></ul></blockquote></li><li>Instruction Set Extensions (AES, SSE etc.) (not usable in Go by now, except for automatism)</li><li>Flamegraphs</li><li>Linux process scheduler</li></ul></blockquote></li></ul></div><div class="step step-level-1" step="7" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="11200" data-y="0" data-z="0"><h1 id="go-assembler-1">Go Assembler #1</h1><p>TODO: enable line numbers</p><pre class="highlight code go"><span class="kn">package</span><span class="w"> </span><span class="nx">main</span><span class="w">

</span><span class="c1">//go:noinline</span><span class="w">
</span><span class="kd">func</span><span class="w"> </span><span class="nx">add</span><span class="p">(</span><span class="nx">a</span><span class="p">,</span><span class="w"> </span><span class="nx">b</span><span class="w"> </span><span class="kt">int</span><span class="p">)</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="k">return</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nx">b</span><span class="w">
</span><span class="p">}</span><span class="w">

</span><span class="kd">func</span><span class="w"> </span><span class="nx">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nx">add</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">)</span><span class="w">
</span><span class="p">}</span></pre></div><div class="step step-level-1" step="8" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="12800" data-y="0" data-z="0"><h1 id="go-assembler-2">Go Assembler #2</h1><p>Go assembly = assembler for a fantasy CPU</p><pre class="highlight code bash">main.add STEXT nosplit <span class="nv">size</span><span class="o">=</span><span class="m">4</span> <span class="nv">args</span><span class="o">=</span>0x10 <span class="nv">locals</span><span class="o">=</span>0x0 <span class="nv">funcid</span><span class="o">=</span>0x0 <span class="nv">align</span><span class="o">=</span>0x0
      <span class="o">(</span>test.go:4<span class="o">)</span>     TEXT    main.add<span class="o">(</span>SB<span class="o">)</span>, NOSPLIT<span class="p">|</span>ABIInternal, <span class="nv">$0</span>-16
      <span class="o">(</span>test.go:4<span class="o">)</span>     FUNCDATA        <span class="nv">$0</span>, gclocals&#xB7;g2BeySu+wFnoycgXfElmcg<span class="o">==(</span>SB<span class="o">)</span>
      <span class="o">(</span>test.go:4<span class="o">)</span>     FUNCDATA        <span class="nv">$1</span>, gclocals&#xB7;g2BeySu+wFnoycgXfElmcg<span class="o">==(</span>SB<span class="o">)</span>
      <span class="o">(</span>test.go:4<span class="o">)</span>     FUNCDATA        <span class="nv">$5</span>, main.add.arginfo1<span class="o">(</span>SB<span class="o">)</span>
      <span class="o">(</span>test.go:4<span class="o">)</span>     FUNCDATA        <span class="nv">$6</span>, main.add.argliveinfo<span class="o">(</span>SB<span class="o">)</span>
      <span class="o">(</span>test.go:4<span class="o">)</span>     PCDATA  <span class="nv">$3</span>, <span class="nv">$1</span>
      <span class="o">(</span>test.go:5<span class="o">)</span>     ADDQ    BX, AX
      <span class="o">(</span>test.go:5<span class="o">)</span>     RET
<span class="o">(</span>...<span class="o">)</span></pre><p>Can we just say: To make things faster you have to reduce the number of instructions?</p><p>Sadly no. Modern CPUs are MUCH complexer than machines that sequentially execute instructions.
They take all kind of shortcuts to execute things faster - most of the time.
See also: Megaherz myth (-&gt; higher clock = more cycles per time)</p><p>Effects that may play a role</p><ul><li>Not every instruction takes the same amount of cycles (MOV 1 cycle,</li><li>Pipelining</li><li>Superscalar Execution</li><li>Branch prediction / Cache prefetching</li><li>Out-of-order execution</li><li>Cache misses (fetching from main memory mean</li></ul><p>List of typical cycles per instructions ("latency"): <a href="https://www.agner.org/optimize/instruction_tables.pdf">https://www.agner.org/optimize/instruction_tables.pdf</a></p></div><div class="step step-level-1" step="9" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="14400" data-y="0" data-z="0"><h1 id="pipelining">Pipelining</h1><p><a href="https://de.wikipedia.org/wiki/Pipeline_(Prozessor">https://de.wikipedia.org/wiki/Pipeline_(Prozessor</a>)</p><p>LOAD: Load the instruction from memory, increment instruction counter.
DECODE: Data for the command is loaded.
EXEC: Instruction is executed.
WRITEBACK: Result is written back to a register.</p><ul><li>Every instruction needs to do this</li><li>Modern CPUs can work on many instructions at the same time</li><li>They can be also re-ordered by the CPU!</li><li>This can lead to issues when an instruction depends on results of another instructions! (branches!)</li><li>It can even happen that we do unncessary work! See SPECTRE and MELTDOWN security issues!</li></ul></div><div class="step step-level-1" step="10" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="16000" data-y="0" data-z="0"><h1 id="branch-prediction">Branch prediction</h1><p>... you can give hints to your CPU!</p><pre class="highlight code c"><span class="k">if</span><span class="p">(</span><span class="n">likely</span><span class="p">(</span><span class="n">a</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">1</span><span class="p">))</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="c1">// ...
</span><span class="p">}</span><span class="w">

</span><span class="k">if</span><span class="p">(</span><span class="n">unlikely</span><span class="p">(</span><span class="n">err</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">0</span><span class="p">))</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="c1">// ...
</span><span class="p">}</span></pre><p>No likely() in Go, compiler tries to insert those hints automayically.
Not much of an important optimization nowadays though as CPUs get a lot better:</p><p><a href="https://de.wikipedia.org/wiki/Sprungvorhersage">https://de.wikipedia.org/wiki/Sprungvorhersage</a></p><p>(but can be relevant for very hot paths on cheap ARM cpus)</p></div><div class="step step-level-1" step="11" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="17600" data-y="0" data-z="0"><h1 id="branch-prediction-in-real-life">Branch prediction in real life</h1><pre class="highlight code go"><span class="k">for</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="nx">i</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="nx">i</span><span class="w"> </span><span class="p">&lt;</span><span class="w"> </span><span class="nx">N</span><span class="p">;</span><span class="w"> </span><span class="nx">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="nx">unsorted</span><span class="p">[</span><span class="nx">i</span><span class="p">]</span><span class="w"> </span><span class="p">&lt;</span><span class="w"> </span><span class="nx">X</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="nx">sum</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="nx">unsorted</span><span class="p">[</span><span class="nx">i</span><span class="p">];</span><span class="w">
    </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">

</span><span class="k">for</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="nx">i</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="nx">i</span><span class="w"> </span><span class="p">&lt;</span><span class="w"> </span><span class="nx">N</span><span class="p">;</span><span class="w"> </span><span class="nx">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="nx">sorted</span><span class="p">[</span><span class="nx">i</span><span class="p">]</span><span class="w"> </span><span class="p">&lt;</span><span class="w"> </span><span class="nx">X</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="nx">sum</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="nx">sorted</span><span class="p">[</span><span class="nx">i</span><span class="p">];</span><span class="w">
    </span><span class="p">}</span><span class="w">
</span><span class="p">}</span></pre><div class="notes"><p>Effect is unnotice-able if optimizations are enabled.
Why? Compilers can make the inner branch a branchless statement.</p></div></div><div class="step step-level-1" step="12" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="19200" data-y="0" data-z="0"><h1 id="branchless-programming">Branchless programming</h1><p>... helps to reduce pipelining issues.</p><ul><li>Branchless: <a href="https://dev.to/jobinrjohnson/branchless-programming-does-it-really-matter-20j4">https://dev.to/jobinrjohnson/branchless-programming-does-it-really-matter-20j4</a></li></ul><div class="notes"><p>Probably not relevant in most cases, but can be a life saver in really hot loops.</p></div></div><div class="step step-level-1" step="13" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="20800" data-y="0" data-z="0"><h1 id="reduce-number-of-instructions">Reduce number of instructions</h1><p>memcpy example</p></div><div class="step step-level-1" step="14" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="22400" data-y="0" data-z="0"><h1 id="i-want-to-mov-mov-it">I want to MOV, MOV it</h1><pre class="highlight code">MOV &lt;dst&gt; &lt;src&gt;</pre><pre class="highlight code">MOV &lt;reg&gt; &lt;reg&gt;
MOV &lt;mem&gt; &lt;reg&gt;
MOV &lt;reg&gt; &lt;mem&gt;</pre><p>-&gt; Access to main memory is 125</p><p>Fun fact: MOV alone is Turing complete: <a href="https://github.com/xoreaxeaxeax/movfuscator">https://github.com/xoreaxeaxeax/movfuscator</a></p></div><div class="step step-level-1" step="15" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="24000" data-y="0" data-z="0"><h1 id="types-of-memory">Types of memory</h1><p>Static memory (SRAM) vs Dynamic memory (DRAM)</p><p>SRAM:</p><ul><li>Much much faster</li><li>Expensive as hell</li></ul><p>DRAM:</p><ul><li>Has to be constantly refreshed.</li><li>Needs complex handling of memory controllers</li><li>Very cheap</li></ul></div><div class="step step-level-1" step="16" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="25600" data-y="0" data-z="0"><h1 id="the-von-neumann-bottleneck">The von Neumann Bottleneck</h1><p>von Neumann Architektur:</p><ul><li>Computer Architecture where there is common memory accessible by all cores</li><li>Memory contains Data as well as code instructions</li><li>All data/code goes over a common bus</li><li>Pretty much all computer nowadays are build this way</li></ul><p>Bottleneck: Memory acess is much slower than CPUs can process the data.</p></div><div class="step step-level-1" step="17" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="27200" data-y="0" data-z="0"><h1 id="l1-l2-l3">L1, L2, L3</h1><p>Just add caches!</p><p>(sigh)</p></div><div class="step step-level-1" step="18" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="28800" data-y="0" data-z="0"><h1 id="cache-lines">Cache lines</h1><p>typicall 64 byte
Read an written as one!</p></div><div class="step step-level-1" step="19" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="30400" data-y="0" data-z="0"><h1 id="caches-misses">Caches misses</h1><p>Unsure if you have cache misses? Use the perf stat -p &lt;PID&gt; command!</p><p><a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/monitoring_and_managing_system_status_and_performance/getting-started-with-perf_monitoring-and-managing-system-status-and-performance">https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/monitoring_and_managing_system_status_and_performance/getting-started-with-perf_monitoring-and-managing-system-status-and-performance</a>
<a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/monitoring_and_managing_system_status_and_performance/overview-of-performance-monitoring-options_monitoring-and-managing-system-status-and-performance">https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/monitoring_and_managing_system_status_and_performance/overview-of-performance-monitoring-options_monitoring-and-managing-system-status-and-performance</a></p><p>counter example 1-3</p></div><div class="step step-level-1" step="20" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="32000" data-y="0" data-z="0"><h1 id="detour-perf-command">Detour: perf command</h1><p>System wide profiling</p><pre class="highlight code bash">perf stat -a &lt;command&gt;   <span class="c1"># Like `time` but much better.
</span>perf stat -a -p &lt;PID&gt;    <span class="c1"># Attach to existin process.
</span>perf mem                 <span class="c1"># Detailed report about memory access / misses
</span>perf c2c                 <span class="c1"># Can find false sharing (see next chapter)</span></pre></div><div class="step step-level-1" step="21" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="33600" data-y="0" data-z="0"><h1 id="detour-flame-graphs">Detour: Flame graphs</h1><p>TODO:</p><p>Attach to running program with perf record
Render flamegraph from output</p><p>Perfect to see what time is spend in in what symbol.
Available for:</p><ul><li>CPU</li><li>Memory Allocations (although I like pprof more here)</li><li>Off-CPU (i.e. I/O)</li></ul><p>perf works (almost) always though and can be used to profile complete systems,
for specific programming languages better options might be available though.</p></div><div class="step step-level-1" step="22" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="35200" data-y="0" data-z="0"><h1 id="cache-coherency">Cache coherency</h1><p>In multithreaded programs, a cache gets evicted</p></div><div class="step step-level-1" step="23" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="36800" data-y="0" data-z="0"><h1 id="false-sharing">False sharing</h1><p>Counter4 example.</p><p>Multiple threads use the same memory</p><p>Can be fixed by introducing padding!</p><ul><li>False sharing / True sharing (i.e. when to pad your data structures
<a href="https://alic.dev/blog/false-sharing.html">https://alic.dev/blog/false-sharing.html</a> )</li></ul></div><div class="step step-level-1" step="24" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="38400" data-y="0" data-z="0"><h1 id="true-sharing">True sharing</h1><p>This is when the idea of introducing caches between CPU and memory works out.
Good news: Can be controlled by:</p><ul><li>Limiting struct sizes to 64 byt</li><li>Grouping often accessed data together.
(arrays of data, not array of structs of data)</li><li></li></ul><p>-&gt; employee example</p></div><div class="step step-level-1" step="25" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="40000" data-y="0" data-z="0"><h1 id="data-oriented-programming">Data oriented programming</h1><p>The science of designing programs in a CPU friendly way.</p><div class="notes"><p>Object oriented program is designing the program in a way that is friendly to humans.</p><p>It does by encapsulating data and methods together. By coincidence, this is not exactly
helpful to the machine your program runs on. Why?</p><ul><li>global state (i.e. impure functions) make branch/cache predictions way harder.</li><li>hurts cache locality.</li></ul></div></div><div class="step step-level-1" step="26" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="41600" data-y="0" data-z="0"><h1 id="matrix-traversal">Matrix Traversal</h1><ul><li>Why is column traversal so much slower?</li></ul><p>Good picture source: <a href="https://medium.com/mirum-budapest/introduction-to-data-oriented-programming-85b51b99572d">https://medium.com/mirum-budapest/introduction-to-data-oriented-programming-85b51b99572d</a></p></div><div class="step step-level-1" step="27" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="43200" data-y="0" data-z="0"><h1 id="employees">Employees</h1><ul><li>Why is the variant with two arrays faster?</li><li>What happens if we make the name array longer/shorter?</li></ul><p>Array-of-Structures vs Structures-of-Arrays</p><p><a href="https://www.dataorienteddesign.com/dodmain/">https://www.dataorienteddesign.com/dodmain/</a></p></div><div class="step step-level-1" step="28" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="44800" data-y="0" data-z="0"><h1 id="rough-rules">Rough Rules</h1><ol><li>Only use so much memory as you really need.</li><li>Writes modify the cache. Directly use your data or declare it later.</li><li>Keep your structs small.</li><li>Avoid nesting of data, if possible (value over pointers)</li><li>Avoid jumpin around in your memory a lot.</li></ol></div><div class="step step-level-1" step="29" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="46400" data-y="0" data-z="0"><h1 id="memcpy"><tt>memcpy</tt></h1><ul><li>Why is the single-byte memcpy so much slower?</li><li>What evil trick is the system memcpy doing?</li><li>Can we do even faster?</li></ul><div class="notes"><p>-&gt; Problem: von-Neumann-Bottleneck.
-&gt; CPU can work on data faster than typical RAM can deliver it.
-&gt; Workaround: Caches in the CPU, Prefetching.
-&gt; Actual solution: Data oriented design.
-&gt; Sequential access, tight packing of data, SIMD (and if you're crazy: DMA)
-&gt; Still best way to speed up copies: don't copy.</p></div><div class="notes"><p>Object oriented design tends to fuck this up and many Games (at their core)
do not use OOP. You can use both at the same time though!</p></div></div><div class="step step-level-1" step="30" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="48000" data-y="0" data-z="0"><h1 id="memory">Memory</h1></div><div class="step step-level-1" step="31" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="49600" data-y="0" data-z="0"><h1 id="ram">RAM</h1><ul><li><strong>RAM</strong> = Random Access Memory</li><li>Huge, sequential line of individual memory cells</li><li>Usually can only be addressed in pages</li><li>Memory controller that handles the actual interaction.</li><li>Two major types: Static RAM (SRAM) vs Dynamic RAM (DRAM)</li></ul><div class="notes"><p>SDRAM = Synchronous DRAM
DDR-SDRAM = Double Data Rate SDRAM</p></div></div><div class="step step-level-1" step="32" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="51200" data-y="0" data-z="0"><h1 id="dram-one-bit-please">DRAM - one bit, please</h1><img src="images/dram.png" width="100%" align="center"></img><div class="notes"><p>Dynamic sounds good, doesn't it?</p><ul><li>Very simple and cheap to produce.</li><li>High density (many cells per area)</li><li>Needs to be refreshed constantly (64ns or so)</li></ul></div></div><div class="step step-level-1" step="33" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="52800" data-y="0" data-z="0"><h1 id="sram-one-bit-please">SRAM - one bit, please</h1><img src="images/sram.png" width="100%" align="center"></img><div class="notes"><ul><li>Very fast. 10x speed of DRAM</li><li>No refresh required.</li><li>Low power consumption</li><li>Expensive, not so high density</li></ul></div></div><div class="step step-level-1" step="34" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="54400" data-y="0" data-z="0"><h1 id="why-use-dram-at-all">Why use DRAM at all?</h1><ul><li>Because it's cheap,  and we need tons of it.</li><li>Main memory is all DRAM.</li><li>Caches (L1-L3) are SRAM.</li><li>A lightbulb is maybe OSRAM (Sorry.)</li></ul><div class="notes"><p>So basically...</p><p>again, hardware is at fault
and instead of fixing it with some Pfiffikus
we software devs have to cope with slow main memory.</p></div></div><div class="step step-level-1" step="35" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="56000" data-y="0" data-z="0"><h1 id="numa">NUMA</h1><p>Is the access to all memory offsets equally fast?</p><ul><li>Not if you have more than one CPU!</li><li>Every CPU gets 1/nth of the memory.</li><li>Every CPU can access the completely memory.</li><li>Non-local access is costly.</li></ul><div class="notes"><p>NUMA - non uniform memory access</p><p>Linux is NUMA very well capable and that's why it's such a popular server operating system.
Or one of the reasons at least.</p></div></div><div class="step step-level-1" step="36" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="57600" data-y="0" data-z="0"><h1 id="how-the-heck-does-this-stuff-relate-to-me">How the heck does this stuff relate to me?</h1><p>Not so much on a daily basis, to be fair. But:</p><ul><li>Memory allocations are expensive.</li><li>Strategies to make less/smaller allocations help performance</li><li>Requires sadly an understanding how the OS handles memory.</li></ul></div><div class="step step-level-1" step="37" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="59200" data-y="0" data-z="0"><p>TODO: Maybe use graphics from here: <a href="https://medium.com/eureka-engineering/understanding-allocations-in-go-stack-heap-memory-9a2631b5035d">https://medium.com/eureka-engineering/understanding-allocations-in-go-stack-heap-memory-9a2631b5035d</a></p><h1 id="the-stack-heap-1">The stack &amp; heap #1</h1><pre class="highlight code go"><span class="c1">//go:noinline</span><span class="w">
</span><span class="kd">func</span><span class="w"> </span><span class="nx">f</span><span class="p">()</span><span class="w"> </span><span class="o">*</span><span class="kt">int</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nx">v</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="mi">3</span><span class="w">
    </span><span class="k">return</span><span class="w"> </span><span class="o">&amp;</span><span class="nx">v</span><span class="w">
</span><span class="p">}</span><span class="w">

</span><span class="kd">func</span><span class="w"> </span><span class="nx">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="c1">// Two for the stack:</span><span class="w">
    </span><span class="c1">// a=0xc00009aef8 b=0xc00009aef0</span><span class="w">
    </span><span class="nx">a</span><span class="p">,</span><span class="w"> </span><span class="nx">b</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="mi">23</span><span class="p">,</span><span class="w"> </span><span class="mi">42</span><span class="w">

    </span><span class="c1">// Two for the heap:</span><span class="w">
    </span><span class="c1">// c=0xc0000b2000 d=0xc0000b2008</span><span class="w">
    </span><span class="nx">c</span><span class="p">,</span><span class="w"> </span><span class="nx">d</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">f</span><span class="p">(),</span><span class="w"> </span><span class="nx">f</span><span class="p">()</span><span class="w">
</span><span class="p">}</span></pre></div><div class="step step-level-1" step="38" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="60800" data-y="0" data-z="0"><h1 id="the-stack-heap-2">The stack &amp; heap #2</h1><p><strong>Stack</strong> is...</p><ul><li>...cleaned up automatically on return</li><li>...bound to a function call</li><li>...preferred if possible.</li><li>...can be reasoned about during compile time</li><li>...good for small amounts of data.</li></ul><p><strong>Heap</strong> is...</p><ul><li>...needs to be explicitly requested</li><li>...needs to be explititly cleaned up</li><li>...can be used until freed.</li><li>...should be used when required.</li><li>...usually required for a lot of data.</li></ul></div><div class="step step-level-1" step="39" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="62400" data-y="0" data-z="0"><h1 id="the-stack-heap-3">The stack &amp; heap #3</h1><p>Go is clever and hides this from you via
<strong>escape analysis</strong>:</p><pre class="highlight code go"><span class="kd">func</span><span class="w"> </span><span class="nx">f</span><span class="p">()</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="nx">v</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="mi">3</span><span class="p">;</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="o">&amp;</span><span class="nx">v</span><span class="w"> </span><span class="p">}</span><span class="w">
</span><span class="kd">func</span><span class="w"> </span><span class="nx">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nx">fmt</span><span class="p">.</span><span class="nx">Println</span><span class="p">(</span><span class="nx">f</span><span class="p">())</span><span class="w">
</span><span class="p">}</span></pre><pre class="highlight code bash">$ go build -gcflags<span class="o">=</span><span class="s2">"-m"</span> .
./main.go:3:2: moved to heap: v</pre><p>The more you allocate on the heap, the more pressure you put on the
memory bookkeeping and the garbage collector.</p><p><strong>Performance tip:</strong> Avoid variables escaping to the heap:</p><ul><li></li><li>Avoid using pointers if unnecessary</li><li>Prefer return by value if value is small (&lt; 128 byte) (small copy is faster than GC)</li><li>Don't overreact here though. Don't make your APIs ugly just because you know this little fact. Use this in hot loops. AFTER measurement.</li></ul><div class="notes"><p>Never heard of this stuff, why should I care?</p><p>Difference is important in C
Well, you're lucky enough that your compiler does it for you
Or you're unlucky enough to use python where all hope is forlorn</p></div></div><div class="step step-level-1" step="40" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="64000" data-y="0" data-z="0"><h1 id="what-is-a-stackoverflow">What is a StackOverflow</h1><p>Why using the stack only for small data if you can also use it for somewhat dynamic allocations?</p><p>Because stack size is limited (on linux about 8MB, but don't rely on that)</p><p>How can you hit this limit?</p><ul><li>By recursion - lots of nested stacks.</li><li>By running over the extents of a buffer (in C)</li></ul><p>See example: stackoverflow.</p></div><div class="step step-level-1" step="41" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="65600" data-y="0" data-z="0"><h1 id="gc-pressure-locality-and-memory-management">GC pressure, locality and memory management</h1><p>Prefer this:</p><pre class="highlight code go"><span class="nx">m</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nb">make</span><span class="p">(</span><span class="kd">map</span><span class="p">[</span><span class="kt">string</span><span class="p">]</span><span class="nx">someStruct</span><span class="p">)</span></pre><p>over:</p><pre class="highlight code go"><span class="nx">m</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nb">make</span><span class="p">(</span><span class="kd">map</span><span class="p">[</span><span class="kt">string</span><span class="p">]</span><span class="o">*</span><span class="nx">someStruct</span><span class="p">)</span></pre><ul><li>Way less memory in total</li><li>Data is packed together (good for caching!)</li><li>Less work for the GC and the allocator to do</li><li>Pointers give you more potential to fuck up.</li></ul><pre class="highlight code bash">noptr  <span class="m">577</span>.7 ns/op   <span class="m">336</span> B/op             <span class="m">2</span> allocs/op
ptr    <span class="m">761</span>.4 ns/op   <span class="m">384</span> B/op            <span class="m">10</span> allocs/op

<span class="o">(</span>The <span class="m">10</span> will increase with input! Longer runs will cause more GC <span class="k">for</span> the ptr <span class="k">case</span><span class="o">)</span></pre></div><div class="step step-level-1" step="42" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="67200" data-y="0" data-z="0"><h1 id="virtual-memory">Virtual memory</h1><img src="images/virtual_memory.svg.png"></img><ul><li>The physical memory of a system is splitted up into 4k pages.</li><li>Each process maintains a virtual memory mapping table, mapping
from the virtual range of memory to physical memory.</li><li>Address translation is handled efficiently by the MMU</li></ul><div class="notes"><p>Wait, those addresses I saw earlier... are those the addrs in RAM?
Hopefully not, because otherwise you could somehow find out where the OpenSSH
server lives in memory and steal it's keys. For security reasons it must look
for each process like he's completely alone on the system. What you saw above
are virtual memory addresses and they stay very similar on each run.</p><p>The concept how this achieved is called "virtual memory" and it's probably one of
the more clever things we did in computer science.</p></div></div><div class="step step-level-1" step="43" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="68800" data-y="0" data-z="0"><h1 id="virtual-memory-advantages">Virtual memory advantages</h1><ul><li>Pages can be mapped only once it is needed (CoW)</li><li>Processes can share the same page for shared memory.</li><li>Pages do not need to be mapped to physical memory: Disk, DMA or even network is possible!</li><li>Processes are isolated from each other.</li><li>Processes consume only as much physical ("residual") memory as really needed.</li><li>Programs get easier because they can just assume that the memory is not fragmented.</li><li>Pages can be swaped by the OS without the process even noticing (Swapping)</li></ul></div><div class="step step-level-1" step="44" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="70400" data-y="0" data-z="0"><h1 id="virtual-memory-implementation">Virtual memory implementation</h1><ul><li>Each process has a list of page tables mapping virtual to physical memory ("page table")</li><li>On process start this table is filled with a few default kilobytes of mapped pages
(the first few pages are not mapped, so dereferencing a NULL pointer will always crash)</li><li>When the program first accesses those addresses the CPU will generate a page fault, indicating
that there is no such mapping. The OS receives this and will find a free physical page, map
it and retry execution. If another page fault occurs the OS will kill the process with SIGSEGV.</li></ul></div><div class="step step-level-1" step="45" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="72000" data-y="0" data-z="0"><h1 id="malloc">malloc()</h1><p>What the fuck happens on allocation?</p><ul><li>Virtual memory</li><li>Page faults</li><li>Residual vs shared memory</li><li>sbrk()</li><li>What happes on malloc?</li><li>Swap</li><li>Low memory situations (OOM)</li><li>Stack vs Heap</li><li>Dynamic memory allocation</li><li>Measuring allocations in Go</li></ul></div><div class="step step-level-1" step="46" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="73600" data-y="0" data-z="0"><ul><li>I/O<blockquote><ul><li>Page cache</li><li>Filesystems (sync / flush cache)</li><li>direct I/O</li><li>buffered I/O</li><li>avoiding I/O (rmlint)</li><li>avoid copies (sendfile, hard/sym/reflinks -&gt; git!)</li><li>DMA</li><li>Insane stuff: FIEMAP, fadvise</li><li>strace it!</li><li>eBPF for the really hard cases</li><li>IO scheduler in linux (ionice, how does it work?)</li><li>Cost of a syscall / what are syscalls?</li></ul></blockquote></li></ul><div class="step step-level-2" step="47"><p>Make an example that shows the cost of a syscall:</p><ul><li>create big file:<blockquote><ol><li>read it with many small read()</li><li>read with large buffers</li><li>read it with too large buffers</li></ol></blockquote></li></ul></div><div class="step step-level-2" step="48"><p>IO for a simple key value store:</p><ol><li>simple append only write, get reads only the last value
(terribly slow because get needs to scan the whole db)Can be implemented in two lines of bash.</li><li>Index needed!
Store an in-memory hash table mapping keys to offsets
Store data as log structured, append only file.
When loading the database, the hash table gets reconstructed.
When values get read, we can seek to the right position.This is already a DB on the market: Bitcask</li><li>This log file would grow a lot, making performance not optimal. Split it up
in segments afer certain size! Compact old segment files. i.e. deduplicate
keys or join several segments even. Can be easily done in the background.</li><li>How do we delete stuff? We write tombstones.</li></ol><p>Advantage:</p><ul><li>Nice performance actually</li><li>Very simple design an easy to debug.</li></ul><p>Disadvantages:</p><ul><li>No range queries possible.</li><li>All hash keys must fit in memory.</li></ul><p>LSM (Log structured merge tree): Store keys in sorted order on disk (sorted by key)</p><p>-&gt; Makes range queries possible
-&gt; Not all keys need to fit in memory (memory index can be sparse, because we can use binary search)</p><p>Databases like Postgres use parts of this concept by using a WAL (write ahead log)</p><ul><li>Nebenl&#xE4;ufigkeit<blockquote><ul><li>concurrent vs parallel (python, node.js "parallelism")</li><li>Threads (Lightweight Processes) vs Processes</li><li>Goroutinen (Lightweight Threads)</li><li>Shared state (global state is always shared)</li><li>Mutex</li><li>Semaphore</li><li>Channel</li><li>Condition Variable</li><li>Pool</li><li>Exercise: Barrier (oder "Wait Group")</li><li>Atomic Operations (NOP NOP NOP...)</li><li>Race condition detection (helgrind, go, rust)</li></ul></blockquote></li></ul></div></div><div class="step step-level-1" step="49" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="75200" data-y="0" data-z="0"><h1 id="sources">Sources</h1><ul><li>Drepper: What every programmer should know about memory: <a href="https://people.freebsd.org/~lstewart/articles/cpumemory.pdf">https://people.freebsd.org/~lstewart/articles/cpumemory.pdf</a></li></ul></div></div><script type="text/javascript" src="js/impress.js"></script><script type="text/javascript" src="js/gotoSlide.js"></script><script type="text/javascript" src="js/hovercraft.js"></script></body></html>