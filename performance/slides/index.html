<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml"><head><title>Performance</title><meta charset="UTF-8"></meta><meta name="generator" content="Hovercraft! 1.0 http://regebro.github.com/hovercraft"></meta><link rel="stylesheet" href="css/hovercraft.css" media="all"></link><link rel="stylesheet" href="css/highlight.css" media="all"></link><link rel="stylesheet" href="hovercraft.css" media="screen,projection"></link><script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        showProcessingMessages: false,
        messageStyle: "none",
        TeX : { extensions : ['color.js'] }
      });
    </script></head><body class="impress-not-supported"><div id="impress-help"></div><div id="impress" data-transition-duration="1500"><div class="step step-level-1" step="0" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="0" data-y="0" data-z="0"><p>We came from "It doesn't matter how fast you return  "</p><p>But it does matter how fast you return a correct result.</p><p>Everything correct, but the result never arrive? Ok, great.</p><p><a href="https://go.dev/doc/diagnostics">https://go.dev/doc/diagnostics</a></p><p>Differences: Profiling - Tracing - Debugging - Statistics</p><h1 id="expectation-management">Expectation management</h1><pre class="highlight code">* This is more of a table of contents, than in-depth knowledge.
* Therefore touching a lot of topics. Mostly Go, but also many general techniques.
* TODO: L&#xFC;ckenf&#xFC;ller zwischen How-to-TDD und How-to-unittests.
* You accept software is never perfect and never will be.
* You accept you have to accept and manage mistakes.
* You don't except this workshop to be complete in any way.
* You don't expect that we implement all of the things mentioned.
* You know not to belive everything the internet or Lemurs says.</pre><div class="notes"><p>My expectations:</p><ul><li>You ask immediately when you did not understand something.</li><li>You will have some exercises in between.</li><li>You will not need to understand everything.
In the worst case you get better in buzzword bingo.</li></ul></div></div><div class="step step-level-1" step="1" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="1600" data-y="0" data-z="0"><p>Introduction:</p><ul><li>Moore's law: Number of transistors</li><li>Lemur's law: Software gets shittyness doubles every two years.</li></ul></div><div class="step step-level-1" step="2" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="3200" data-y="0" data-z="0"><p>Questions to ask:</p><ul><li>How fast does this thing need to be?</li></ul><p>What people ask:</p><ul><li>How fast could this be?</li></ul><p>or:</p><ul><li>It works, no need for optimization!</li></ul></div><div class="step step-level-1" step="3" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="4800" data-y="0" data-z="0"><p>Not included:</p><ul><li>Network performance</li><li>Database performance</li><li>Any of the thousands of specific fields</li><li>Long intro to data structures - pick a book.</li></ul><p>That would be several semesters of material.
The material in this workshop is just one ;)</p></div><div class="step step-level-1" step="4" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="6400" data-y="0" data-z="0"><p>TOC:</p><ul><li>4 Erkenntn&#xFC;sse:</li><li>Jeder der sich daf&#xFC;r interessiert schnelle Programme zu schreiben.</li><li>Rare knowledge, less common knowledge.</li></ul><p>Intro:</p><blockquote><ul><li>Warum Optimierung?</li><li>Contra: Donald Knuth: Premature Optimization is the root of all evil.</li><li>Pro: Moore's Law but software got bad faster then hardware got faster.</li><li>Berufsstolz: Man sollte schon schauen dass die Software m&#xF6;glichst wenig
Resources nutzt. Frontendentwicklung heutzutage ist leider ein gutes Beispiel.</li><li>Electron etc. sind so fett dass es echt peinlich ist.</li></ul></blockquote></div><div class="step step-level-1 chapter-class" step="5" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="8000" data-y="0" data-z="0"><h1 id="cpu">CPU</h1></div><div class="step step-level-1" step="6" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="9600" data-y="0" data-z="0"><ul><li>CPU<blockquote><ul><li>Assembly vs Machine Code<blockquote><p>Assembly: 1:1 human readable interpretation of machine code.
Machine code: machine readable instructions (each instruction has an id)
Assembler: Program that converts assembly to machine code.</p></blockquote></li><li>This slides could be also a talk about "Why interpreted languages suck"<blockquote><p>Most optimizations will not work with python.
As a language it's really disconnected from the HW - every single statement will cause 100s or 1000s of assembly instructions.
Also there are no almost no guarantees how big e.g. arrays or other data structures will be and how they are layout in memory.
You have to rely on your interpreter (and I count Java's JIT as one!) to be fast on modern hardware - most are not and that's why
there's so much C libraries in python, making the whole packaging system a bloody mess.</p></blockquote></li><li>Go assembly<blockquote><p>Assembly</p></blockquote></li><li>von Neumann Architektur (RAM being the bottleneck)</li><li>Many workarounds to fix this: (L1, L2, L3)</li><li>Instruction Sets (RISC/CISC, x86, arm)</li><li>Branch prediction (Heartbleed)</li><li>if(unlikely(x &gt; 100)) { error() }</li><li>Data oriented programming<blockquote><ul><li>employee example</li><li>memcpy</li><li>matrix traversal</li></ul></blockquote></li><li>Instruction Set Extensions (AES, SSE etc.)</li><li>Complexity Notation</li><li>Flamegraphs</li></ul></blockquote></li></ul></div><div class="step step-level-1" step="7" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="11200" data-y="0" data-z="0"><h1 id="go-assembler-1">Go Assembler #1</h1><p>TODO: enable line numbers</p><pre class="highlight code go"><span class="kn">package</span><span class="w"> </span><span class="nx">main</span><span class="w">

</span><span class="c1">//go:noinline</span><span class="w">
</span><span class="kd">func</span><span class="w"> </span><span class="nx">add</span><span class="p">(</span><span class="nx">a</span><span class="p">,</span><span class="w"> </span><span class="nx">b</span><span class="w"> </span><span class="kt">int</span><span class="p">)</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="k">return</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nx">b</span><span class="w">
</span><span class="p">}</span><span class="w">

</span><span class="kd">func</span><span class="w"> </span><span class="nx">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nx">add</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">)</span><span class="w">
</span><span class="p">}</span></pre></div><div class="step step-level-1" step="8" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="12800" data-y="0" data-z="0"><h1 id="go-assembler-2">Go Assembler #2</h1><p>Go assembly = assembler for a fantasy CPU</p><pre class="highlight code bash">main.add STEXT nosplit <span class="nv">size</span><span class="o">=</span><span class="m">4</span> <span class="nv">args</span><span class="o">=</span>0x10 <span class="nv">locals</span><span class="o">=</span>0x0 <span class="nv">funcid</span><span class="o">=</span>0x0 <span class="nv">align</span><span class="o">=</span>0x0
      <span class="o">(</span>test.go:4<span class="o">)</span>     TEXT    main.add<span class="o">(</span>SB<span class="o">)</span>, NOSPLIT<span class="p">|</span>ABIInternal, <span class="nv">$0</span>-16
      <span class="o">(</span>test.go:4<span class="o">)</span>     FUNCDATA        <span class="nv">$0</span>, gclocals&#xB7;g2BeySu+wFnoycgXfElmcg<span class="o">==(</span>SB<span class="o">)</span>
      <span class="o">(</span>test.go:4<span class="o">)</span>     FUNCDATA        <span class="nv">$1</span>, gclocals&#xB7;g2BeySu+wFnoycgXfElmcg<span class="o">==(</span>SB<span class="o">)</span>
      <span class="o">(</span>test.go:4<span class="o">)</span>     FUNCDATA        <span class="nv">$5</span>, main.add.arginfo1<span class="o">(</span>SB<span class="o">)</span>
      <span class="o">(</span>test.go:4<span class="o">)</span>     FUNCDATA        <span class="nv">$6</span>, main.add.argliveinfo<span class="o">(</span>SB<span class="o">)</span>
      <span class="o">(</span>test.go:4<span class="o">)</span>     PCDATA  <span class="nv">$3</span>, <span class="nv">$1</span>
      <span class="o">(</span>test.go:5<span class="o">)</span>     ADDQ    BX, AX
      <span class="o">(</span>test.go:5<span class="o">)</span>     RET
<span class="o">(</span>...<span class="o">)</span></pre><p>Can we just say: To make things faster you have to reduce the number of instructions?</p><p>Sadly no. Modern CPUs are MUCH complexer than machines that sequentially execute instructions.
They take all kind of shortcuts to execute things faster - most of the time.
See also: Megaherz myth (-&gt; higher clock = more cycles per time)</p><p>Effects that may play a role</p><ul><li>Not every instruction takes the same amount of cycles (MOV 1 cycle,</li><li>Pipelining</li><li>Superscalar Execution</li><li>Branch prediction / Cache prefetching</li><li>Out-of-order execution</li><li>Cache misses (fetching from main memory mean</li></ul><p>List of typical cycles per instructions ("latency"): <a href="https://www.agner.org/optimize/instruction_tables.pdf">https://www.agner.org/optimize/instruction_tables.pdf</a></p></div><div class="step step-level-1" step="9" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="14400" data-y="0" data-z="0"><h1 id="pipelining">Pipelining</h1><p><a href="https://de.wikipedia.org/wiki/Pipeline_(Prozessor">https://de.wikipedia.org/wiki/Pipeline_(Prozessor</a>)</p><p>LOAD: Load the instruction from memory, increment instruction counter.
DECODE: Data for the command is loaded.
EXEC: Instruction is executed.
WRITEBACK: Result is written back to a register.</p><ul><li>Every instruction needs to do this</li><li>Modern CPUs can work on many instructions at the same time</li><li>They can be also re-ordered by the CPU!</li><li>This can lead to issues when an instruction depends on results of another instructions! (branches!)</li><li>It can even happen that we do unncessary work! See SPECTRE and MELTDOWN security issues!</li></ul></div><div class="step step-level-1" step="10" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="16000" data-y="0" data-z="0"><h1 id="branchless-programming">Branchless programming</h1><p>... helps to reduce pipelining issues.</p></div><div class="step step-level-1" step="11" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="17600" data-y="0" data-z="0"><h1 id="branch-prediction">Branch prediction</h1><p>... you can give hints to your CPU!</p><pre class="highlight code c"><span class="k">if</span><span class="p">(</span><span class="n">likely</span><span class="p">(</span><span class="n">a</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">1</span><span class="p">))</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="c1">// ...
</span><span class="p">}</span><span class="w">

</span><span class="k">if</span><span class="p">(</span><span class="n">unlikely</span><span class="p">(</span><span class="n">err</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">0</span><span class="p">))</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="c1">// ...
</span><span class="p">}</span></pre><p>No likely() in Go, compiler tries to insert those hints automayically.
Not much of an important optimization nowadays though as CPUs get a lot better:</p><p><a href="https://de.wikipedia.org/wiki/Sprungvorhersage">https://de.wikipedia.org/wiki/Sprungvorhersage</a></p><p>(but can be relevant for very hot paths on cheap ARM cpus)</p></div><div class="step step-level-1" step="12" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="19200" data-y="0" data-z="0"><h1 id="reduce-number-of-instructions">Reduce number of instructions</h1><p>memcpy example</p></div><div class="step step-level-1" step="13" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="20800" data-y="0" data-z="0"><h1 id="i-want-to-mov-mov-it">I want to MOV, MOV it</h1><pre class="highlight code">MOV &lt;dst&gt; &lt;src&gt;</pre><pre class="highlight code">MOV &lt;reg&gt; &lt;reg&gt;
MOV &lt;mem&gt; &lt;reg&gt;
MOV &lt;reg&gt; &lt;mem&gt;</pre><p>-&gt; Access to main memory is 125</p><p>Fun fact: MOV alone is Turing complete: <a href="https://github.com/xoreaxeaxeax/movfuscator">https://github.com/xoreaxeaxeax/movfuscator</a></p></div><div class="step step-level-1" step="14" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="22400" data-y="0" data-z="0"><h1 id="types-of-memory">Types of memory</h1><p>Static memory (SRAM) vs Dynamic memory (DRAM)</p><p>SRAM:</p><ul><li>Much much faster</li><li>Expensive as hell</li></ul><p>DRAM:</p><ul><li>Has to be constantly refreshed.</li><li>Needs complex handling of memory controllers</li><li>Very cheap</li></ul></div><div class="step step-level-1" step="15" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="24000" data-y="0" data-z="0"><h1 id="the-von-neumann-bottleneck">The von Neumann Bottleneck</h1><p>von Neumann Architektur:</p><ul><li>Computer Architecture where there is common memory accessible by all cores</li><li>Memory contains Data as well as code instructions</li><li>All data/code goes over a common bus</li><li>Pretty much all computer nowadays are build this way</li></ul><p>Bottleneck: Memory acess is much slower than CPUs can process the data.</p></div><div class="step step-level-1" step="16" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="25600" data-y="0" data-z="0"><h1 id="l1-l2-l3">L1, L2, L3</h1><p>Just add caches!</p><p>(sigh)</p></div><div class="step step-level-1" step="17" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="27200" data-y="0" data-z="0"><h1 id="cache-lines">Cache lines</h1><p>typicall 64 byte
Read an written as one!</p></div><div class="step step-level-1" step="18" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="28800" data-y="0" data-z="0"><h1 id="caches-misses">Caches misses</h1><p>Unsure if you have cache misses? Use the perf stat -p &lt;PID&gt; command!</p><p><a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/monitoring_and_managing_system_status_and_performance/getting-started-with-perf_monitoring-and-managing-system-status-and-performance">https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/monitoring_and_managing_system_status_and_performance/getting-started-with-perf_monitoring-and-managing-system-status-and-performance</a>
<a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/monitoring_and_managing_system_status_and_performance/overview-of-performance-monitoring-options_monitoring-and-managing-system-status-and-performance">https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/monitoring_and_managing_system_status_and_performance/overview-of-performance-monitoring-options_monitoring-and-managing-system-status-and-performance</a></p><p>counter example 1-3</p></div><div class="step step-level-1" step="19" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="30400" data-y="0" data-z="0"><h1 id="detour-perf-command">Detour: perf command</h1><p>System wide profiling</p><pre class="highlight code bash">perf stat -a &lt;command&gt;   <span class="c1"># Like `time` but much better.
</span>perf stat -a -p &lt;PID&gt;    <span class="c1"># Attach to existin process.
</span>perf mem                 <span class="c1"># Detailed report about memory access / misses
</span>perf c2c                 <span class="c1"># Can find false sharing (see next chapter)</span></pre></div><div class="step step-level-1" step="20" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="32000" data-y="0" data-z="0"><h1 id="detour-flame-graphs">Detour: Flame graphs</h1><p>TODO:</p><p>Attach to running program with perf record
Render flamegraph from output</p><p>Perfect to see what time is spend in in what symbol.
Available for:</p><ul><li>CPU</li><li>Memory Allocations (although I like pprof more here)</li><li>Off-CPU (i.e. I/O)</li></ul><p>perf works (almost) always though and can be used to profile complete systems,
for specific programming languages better options might be available though.</p></div><div class="step step-level-1" step="21" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="33600" data-y="0" data-z="0"><h1 id="cache-coherency">Cache coherency</h1><p>In multithreaded programs, a cache gets evicted</p></div><div class="step step-level-1" step="22" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="35200" data-y="0" data-z="0"><h1 id="false-sharing">False sharing</h1><p>Counter4 example.</p><p>Multiple threads use the same memory</p><p>Can be fixed by introducing padding!</p></div><div class="step step-level-1" step="23" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="36800" data-y="0" data-z="0"><h1 id="true-sharing">True sharing</h1><p>This is when the idea of introducing caches between CPU and memory works out.
Good news: Can be controlled by:</p><ul><li>Limiting struct sizes to 64 byt</li><li>Grouping often accessed data together.
(arrays of data, not array of structs of data)</li><li></li></ul><p>-&gt; employee example</p></div><div class="step step-level-1" step="24" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="38400" data-y="0" data-z="0"><h1 id="data-oriented-design">Data oriented design</h1><p>The science of designing programs in a CPU friendly way.</p><div class="notes"><p>Object oriented program is designing the program in a way that is friendly to humans.</p></div></div><div class="step step-level-1" step="25" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="40000" data-y="0" data-z="0"><h1 id="matrix-traversal">Matrix Traversal</h1><ul><li>Why is column traversal so much slower?</li></ul><p>Good picture source: <a href="https://medium.com/mirum-budapest/introduction-to-data-oriented-programming-85b51b99572d">https://medium.com/mirum-budapest/introduction-to-data-oriented-programming-85b51b99572d</a></p></div><div class="step step-level-1" step="26" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="41600" data-y="0" data-z="0"><h1 id="employees">Employees</h1><ul><li>Why is the variant with two arrays faster?</li><li>What happens if we make the name array longer/shorter?</li></ul></div><div class="step step-level-1" step="27" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="43200" data-y="0" data-z="0"><h1 id="memcpy"><tt>memcpy</tt></h1><ul><li>Why is the single-byte memcpy so much slower?</li><li>What evil trick is the system memcpy doing?</li><li>Can we do even faster?</li></ul><div class="notes"><p>-&gt; Problem: von-Neumann-Bottleneck.
-&gt; CPU can work on data faster than typical RAM can deliver it.
-&gt; Workaround: Caches in the CPU, Prefetching.
-&gt; Actual solution: Data oriented design.
-&gt; Sequential access, tight packing of data, SIMD (and if you're crazy: DMA)
-&gt; Still best way to speed up copies: don't copy.</p></div><div class="notes"><p>Object oriented design tends to fuck this up and many Games (at their core)
do not use OOP. You can use both at the same time though!</p></div><ul><li>Nebenl&#xE4;ufigkeit<blockquote><ul><li>concurrent vs parallel (python, node.js "parallelism")</li><li>Threads (Lightweight Processes) vs Processes</li><li>Goroutinen (Lightweight Threads)</li><li>Shared state (global state is always shared)</li><li>Mutex</li><li>Semaphore</li><li>Channel</li><li>Condition Variable</li><li>Pool</li><li>Exercise: Barrier (oder "Wait Group")</li><li>Atomic Operations (NOP NOP NOP...)</li><li>Race condition detection (helgrind, go, rust)</li></ul></blockquote></li></ul><ul><li>Memory<blockquote><ul><li>DRAM vs SRAM (show layout of a single memory cell)
-&gt; software developers have to suffer the resulting problems.
-&gt; Main memory is SRAM, caches are SRAM
-&gt; Spatial locality (loops) and temporal locality (access to same data)
-&gt; prefetch</li><li>Virtual memory
-&gt; Each process thinks he is alone on the system.
-&gt; Benefits: Lazy allocation, swapping, file mapping, CoW, shared memory.
-&gt; Each process has a list of page tables mapping virtual to physical memory.
-&gt; SIGSEGV when accessing virtual memory that is not mapped ("page fault").
-&gt; Page cache
-&gt; Page stealing: seldomly used pages go to swap and only read back on use.</li><li>Page faults</li><li>Residual vs shared memory</li><li>Swap</li><li>Low memory situations (OOM)</li><li>Stack vs Heap</li><li>Dynamic memory allocation</li><li>Measuring allocations in Go</li><li>Heap escape analysis</li><li>False sharing / True sharing (i.e. when to pad your data structures
<a href="https://alic.dev/blog/false-sharing.html">https://alic.dev/blog/false-sharing.html</a> )</li><li>Branchless: <a href="https://dev.to/jobinrjohnson/branchless-programming-does-it-really-matter-20j4">https://dev.to/jobinrjohnson/branchless-programming-does-it-really-matter-20j4</a></li></ul><p>Rules:</p><ol><li>Don't use so much memory.</li><li>Writes modify the cache. Directly use your data or initialize it later.</li><li>Keep your structs small.</li><li>Avoid nesting of data, if possible (value over pointers)</li></ol></blockquote></li></ul><ul><li>I/O<blockquote><ul><li>Page cache</li><li>Filesystems (sync / flush cache)</li><li>direct I/O</li><li>buffered I/O</li><li>avoiding I/O (rmlint)</li><li>avoid copies (sendfile, hard/sym/reflinks -&gt; git!)</li><li>DMA</li><li>Insane stuff: FIEMAP, fadvise</li><li>strace it!</li><li>eBPF for the really hard cases</li></ul></blockquote></li></ul></div><div class="step step-level-1" step="28" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="44800" data-y="0" data-z="0"><h1 id="sources">Sources</h1><ul><li>Drepper: What every programmer should know about memory: <a href="https://people.freebsd.org/~lstewart/articles/cpumemory.pdf">https://people.freebsd.org/~lstewart/articles/cpumemory.pdf</a></li></ul></div></div><script type="text/javascript" src="js/impress.js"></script><script type="text/javascript" src="js/gotoSlide.js"></script><script type="text/javascript" src="js/hovercraft.js"></script></body></html>